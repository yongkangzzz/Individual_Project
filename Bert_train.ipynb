{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-dependence",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "other-innocent",
    "outputId": "086a6858-eb31-4392-8afe-c661e8b3ac19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-montreal",
   "metadata": {
    "id": "mozQaVOhhSYc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch, torchvision\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-police",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtUHJbffpBH_",
    "outputId": "ae42dfe8-4d73-4af8-b196-1421f9c59a5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-bahrain",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kRuJekEiuE7",
    "outputId": "ad2d44b1-5509-4e0b-81bf-7c25d6f6cd22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 28 08:25:33 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-coral",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ut8CHH9vkB9y",
    "outputId": "0a658a93-215c-43de-e3be-bc48b94aad0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
      "Cuda compilation tools, release 11.0, V11.0.221\n",
      "Build cuda_11.0_bu.TC445_37.28845127_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-spyware",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZaz5cTooVqX",
    "outputId": "64936cd7-3b11-4617-9279-b336d5065e53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-reader",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "id": "uIEUX7Mho2QW",
    "outputId": "a846b245-92ef-4df0-d7c9-38a1ea6b8ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
      "\u001b[K     |█████████████                   | 834.1 MB 1.2 MB/s eta 0:16:18tcmalloc: large alloc 1147494400 bytes == 0x561faadf2000 @  0x7f76ea512615 0x561f725cf02c 0x561f726af17a 0x561f725d1e4d 0x561f726c3c0d 0x561f726460d8 0x561f72640c35 0x561f725d373a 0x561f72645f40 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f725d3b99 0x561f72616e79 0x561f725d27b2 0x561f72645e65 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35 0x561f725d373a 0x561f72641b0e 0x561f725d365a 0x561f72641d67 0x561f72640c35\n",
      "\u001b[K     |████████████████▌               | 1055.7 MB 1.2 MB/s eta 0:13:39tcmalloc: large alloc 1434370048 bytes == 0x561fef448000 @  0x7f76ea512615 0x561f725cf02c 0x561f726af17a 0x561f725d1e4d 0x561f726c3c0d 0x561f726460d8 0x561f72640c35 0x561f725d373a 0x561f72645f40 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f725d3b99 0x561f72616e79 0x561f725d27b2 0x561f72645e65 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35 0x561f725d373a 0x561f72641b0e 0x561f725d365a 0x561f72641d67 0x561f72640c35\n",
      "\u001b[K     |█████████████████████           | 1336.2 MB 1.2 MB/s eta 0:09:36tcmalloc: large alloc 1792966656 bytes == 0x561f7427a000 @  0x7f76ea512615 0x561f725cf02c 0x561f726af17a 0x561f725d1e4d 0x561f726c3c0d 0x561f726460d8 0x561f72640c35 0x561f725d373a 0x561f72645f40 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f725d3b99 0x561f72616e79 0x561f725d27b2 0x561f72645e65 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35 0x561f725d373a 0x561f72641b0e 0x561f725d365a 0x561f72641d67 0x561f72640c35\n",
      "\u001b[K     |██████████████████████████▌     | 1691.1 MB 1.2 MB/s eta 0:04:58tcmalloc: large alloc 2241208320 bytes == 0x561fdf062000 @  0x7f76ea512615 0x561f725cf02c 0x561f726af17a 0x561f725d1e4d 0x561f726c3c0d 0x561f726460d8 0x561f72640c35 0x561f725d373a 0x561f72645f40 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f726c4a56 0x561f72641fb3 0x561f725d3b99 0x561f72616e79 0x561f725d27b2 0x561f72645e65 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35 0x561f725d373a 0x561f72641b0e 0x561f725d365a 0x561f72641d67 0x561f72640c35\n",
      "\u001b[K     |████████████████████████████████| 2041.3 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2041348096 bytes == 0x5620649c4000 @  0x7f76ea5111e7 0x561f72604ae7 0x561f725cf02c 0x561f726af17a 0x561f725d1e4d 0x561f726c3c0d 0x561f726460d8 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f725d365a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35\n",
      "tcmalloc: large alloc 2551685120 bytes == 0x5621528c0000 @  0x7f76ea512615 0x561f725cf02c 0x561f726af17a 0x561f725d1e4d 0x561f726c3c0d 0x561f726460d8 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f72641d67 0x561f725d365a 0x561f72641d67 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35 0x561f725d373a 0x561f7264293b 0x561f72640c35 0x561f725d3dd1\n",
      "\u001b[K     |████████████████████████████████| 2041.3 MB 5.1 kB/s \n",
      "\u001b[?25hCollecting torchvision==0.10.0+cu111\n",
      "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (23.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.2 MB 1.2 MB/s \n",
      "\u001b[?25hCollecting torchaudio==0.9.0\n",
      "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 8.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0+cu111) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.0+cu111) (1.19.5)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.9.0+cu102\n",
      "    Uninstalling torch-1.9.0+cu102:\n",
      "      Successfully uninstalled torch-1.9.0+cu102\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.10.0+cu102\n",
      "    Uninstalling torchvision-0.10.0+cu102:\n",
      "      Successfully uninstalled torchvision-0.10.0+cu102\n",
      "Successfully installed torch-1.9.0+cu111 torchaudio-0.9.0 torchvision-0.10.0+cu111\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip3 install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-commander",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsDj1Efvp6AL",
    "outputId": "b5fa6062-d136-48c5-9501-ffba5fc081f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111 True\n",
      "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-boston",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qiGLh-X6iLxJ",
    "outputId": "cd0dd2e4-d415-49d5-9e27-42045cd08af8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-preference",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "magnetic-christmas",
    "outputId": "28192213-3b54-4394-ca57-49386ba2585e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "sys.path.append('/content/drive/My Drive/mycolab/ob')\n",
    "data_pos = pd.read_csv('/content/drive/My Drive/mycolab/ob/positive_news_dataset.csv')\n",
    "data_pos['label'] = 1\n",
    "#data_pos = data_pos[:-3000]\n",
    "data_pos1 = pd.read_csv('/content/drive/My Drive/mycolab/ob/good_news_network_dataset.csv')\n",
    "data_pos1['label'] = 1\n",
    "data_neg = pd.read_csv('/content/drive/My Drive/mycolab/ob/crime_online_dataset.csv')\n",
    "data_neg['label'] = 0\n",
    "#data_neg = data_neg[:-3000]\n",
    "data_set1=pd.read_csv('/content/drive/My Drive/mycolab/ob/trainset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-marking",
   "metadata": {
    "id": "italic-regard"
   },
   "outputs": [],
   "source": [
    "data_1 = pd.concat([data_pos, data_neg,data_pos1], axis=0).reset_index(drop=True)\n",
    "data_1.drop(['sentiment'], inplace=True, axis=1)\n",
    "data = pd.concat([data_1, data_set1], axis=0).reset_index(drop=True)\n",
    "data.drop(['url'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-research",
   "metadata": {
    "id": "ex_GjPzBVFuH"
   },
   "outputs": [],
   "source": [
    "data = data[-1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-awareness",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "literary-credits",
    "outputId": "97a7d5b6-996d-46df-b09f-3db85197dd79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18978</th>\n",
       "      <td>Their office hours are spent on simple wooden ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10102</th>\n",
       "      <td>A Houston accused of murdering his estranged w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18867</th>\n",
       "      <td>The Occupy Wall Street movement, forever angry...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26461</th>\n",
       "      <td>Miami-Dade mayor says two more bodies have bee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28555</th>\n",
       "      <td>Pallister '50 years out of date,' professor sa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "18978  Their office hours are spent on simple wooden ...      1\n",
       "10102  A Houston accused of murdering his estranged w...      0\n",
       "18867  The Occupy Wall Street movement, forever angry...      1\n",
       "26461  Miami-Dade mayor says two more bodies have bee...      0\n",
       "28555  Pallister '50 years out of date,' professor sa...      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-dialogue",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "conventional-reserve",
    "outputId": "7a4772a1-4410-428f-9111-6906e6da9f1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29951, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-specification",
   "metadata": {
    "id": "liable-backup"
   },
   "outputs": [],
   "source": [
    "\n",
    "X = data.text.values\n",
    "y = data.label.values\n",
    "\n",
    "X_train, X_val, y_train, y_val =\\\n",
    "    train_test_split(X, y, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-bernard",
   "metadata": {
    "id": "respected-pipeline"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_pos_test = pd.read_csv('/content/drive/My Drive/mycolab/ob/positive_news_dataset.csv')\n",
    "data_pos_test['label'] = 1\n",
    "data_pos_test = data_pos_test[3100:3600]\n",
    "data_neg_test = pd.read_csv('/content/drive/My Drive/mycolab/ob/crime_online_dataset.csv')\n",
    "data_neg_test['label'] = 0\n",
    "data_neg_test = data_neg_test[3100:3600]\n",
    "data_test = pd.concat([data_pos_test, data_neg_test], axis=0).reset_index(drop=True)\n",
    "data_test.drop(['url'], inplace=True, axis=1)\n",
    "data_test.drop(['sentiment'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-rings",
   "metadata": {
    "id": "mighty-engagement"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-heart",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thousand-moderator",
    "outputId": "d27cfe59-9597-49b7-ebba-a9838312214e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-theorem",
   "metadata": {
    "id": "capital-amendment"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Remove unnecessary symbols\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (str): the processed string.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-century",
   "metadata": {
    "id": "7QOS7ZVCTG2a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/mycolab/ob/testset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-trauma",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-z9YOquTOjL",
    "outputId": "e8424217-8881-42e4-e65d-7727856809f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "for num in range(0,df.shape[0]):\n",
    "  print(num)\n",
    "  df['text'].loc[num]=get_text(df['url'].loc[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-desire",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "collapsed": true,
    "id": "_eENuKw-UF5E",
    "outputId": "e7076623-d968-4462-8659-dff92c757b39"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-eaf1fb7ca5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                          \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          smooth_idf=False)\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX_test_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1131\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfeature_idx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeature_counter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                         \u001b[0mfeature_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_test = df.text.values\n",
    "X_test_preprocessed = np.array([text_preprocessing(text) for text in X_test])\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
    "X_test_tfidf = tf_idf.transform(X_test_preprocessed)\n",
    "probs = nb_model.predict_proba(X_test_tfidf)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-pursuit",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxYQagEFZ7Ab",
    "outputId": "9e80076a-a531-40f2-947e-fe5c4b330a91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = nb_model.predict_proba(X_test_tfidf)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-client",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Qy4mw7aWw5w",
    "outputId": "47c4d836-7d1e-4c0f-dadf-7cd32bea87ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:,-1]\n",
    "y_true = np.zeros(df.shape[0])\n",
    "for num in range(0,df.shape[0]):\n",
    "  if probs[:,-1][num]>0.5:\n",
    "    y_true[num] = 1\n",
    "y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-pattern",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qux1cMq0VtCg",
    "outputId": "d334641c-5d1d-4e19-e099-e0add0f469fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[116, 134, 0, 250]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "y_test1  = np.zeros(250)\n",
    "y_test2 = np.ones(250)\n",
    "y_test = np.concatenate((y_test1,y_test2),axis=0)\n",
    "y_test.shape\n",
    "accuracy = accuracy_score(y_test,y_true)\n",
    "print(accuracy)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_true).ravel()\n",
    "[tn, fp, fn, tp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-courage",
   "metadata": {
    "id": "3HuqGmVROidh"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Preprocess text\n",
    "X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
    "X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-triumph",
   "metadata": {
    "id": "7Ij-sfvHO2F_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-breath",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "WF8ANXQlPLQ0",
    "outputId": "f41e2e02-fd7f-4ce3-fac5-c6087d881198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
    "                 for i in np.arange(1, 10, 0.1)],\n",
    "                index=np.arange(1, 10, 0.1))\n",
    "\n",
    "best_alpha = np.round(res.idxmax(), 2)\n",
    "print('Best alpha: ', best_alpha)\n",
    "\n",
    "plt.plot(res)\n",
    "plt.title('AUC vs. Alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('AUC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-hometown",
   "metadata": {
    "id": "pFOwDwKpPpxr"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "def evaluate_roc(probs, y_true):\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-spencer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "jnyXSNnVRmI3",
    "outputId": "341cfe76-d2b0-46ca-ef03-bb96a2489723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-57e2b11465c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "\n",
    "X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         smooth_idf=False)\n",
    "X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
    "X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-thickness",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZV4mxhxMPYA3",
    "outputId": "ff484dbd-b839-4a65-c338-b4b7562b0e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9894\n",
      "Accuracy: 94.49%\n"
     ]
    }
   ],
   "source": [
    "# Compute predicted probabilities\n",
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "probs = nb_model.predict_proba(X_val_tfidf)\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-coach",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cognitive-failure",
    "outputId": "292ad7e4-c609-40e0-adb9-af58d36309bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  What’s the secret to happiness? This course at Yale University sets out to answer the question – and it has proved extremely popularA class titled Psychology and the Good Life at Yale University in the US is so popular that one in four undergraduate students have enrolled.Laurie Santos, the psychology professor who teaches on the course, says it is the most students ever to enrol in a class in the history of Yale, which was founded in 1701.Homework assignments include showing more gratitude, performing acts of kindness and improving social connections. The course also includes practical advice such as choosing satisfying careers, and separating satisfying pursuits from hollow ones.\n",
      "Processed:  What’s the secret to happiness? This course at Yale University sets out to answer the question – and it has proved extremely popularA class titled Psychology and the Good Life at Yale University in the US is so popular that one in four undergraduate students have enrolled.Laurie Santos, the psychology professor who teaches on the course, says it is the most students ever to enrol in a class in the history of Yale, which was founded in 1701.Homework assignments include showing more gratitude, performing acts of kindness and improving social connections. The course also includes practical advice such as choosing satisfying careers, and separating satisfying pursuits from hollow ones.\n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0\n",
    "print('Original: ', X[0])\n",
    "print('Processed: ', text_preprocessing(X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-causing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "04f4de2271fd47a1bfbef882924cb09d",
      "86f40d734baf4d2097d493c31afc4c6e",
      "a7b7cdc5ce7847fdbc81cbe57fb75efc",
      "c38aa949f9ea40c1ba532b06f13ca401",
      "3bfac18b3a744b0d95e8bd24e303d609",
      "22281583fc674b018f767d46b4582130",
      "0dc8d41829af449a84c863790a9138e8",
      "6e51866900044b86b428cb93a78a23b8",
      "d1605c98d37b49999616920f61838101",
      "48203a3e1aa94e9c85b1cc5fbdebcbb6",
      "801d2d0f63e1494780413fcff2bbaa10",
      "c12f62d86241469ea82d143da666d733",
      "4b47dce8dbdc4c7ea56eef5e1e2f4d06",
      "c93175a16210440a931b856ff68dc51a",
      "9462a3acbf234a85b651bcce3f669c62",
      "d6fbae1e353f4b24a4c1bcf63122181d",
      "34cc322beda14d00b62f764b268c7b4c",
      "0fb18f03eaf04078981fb88ffda9da37",
      "b51ae0d687bb48939aef24b812f4b62a",
      "63b67e6a9da748de8cc569c64a499671",
      "d8b9149cec4e4931bb290b2b897920c8",
      "329f0886bba04fc98558a3d0e2dc8e71",
      "5f1388ce45fd4eff923e31322a7fba1d",
      "53faf34f78654183a26b0ca589d6d069",
      "bc1532258ee841e49d727dcc54d9238a",
      "f78421c421bd44259fa12010ac4ff953",
      "7104fbc5b5774014aa1dd84feab22638",
      "516aa1b02a2245a69db142612d93ba7b",
      "1ee466477722426b872be7b952d88a8a",
      "70b619307cb349b5986e307357451ddb",
      "cc89e783b5554db6a51860c90f1ea38d",
      "d277198e13444a4bae71c326830a19af",
      "c56297ad0e364960a297360d6d338cec",
      "6308194b878246e2b65c2fbc0aac664f",
      "7584bde860ec488fa83307c9f765f83e",
      "8bd765d26eda4afbb2eb9f73f007e2cf",
      "dbe30fc67e794c9d818ca110e30ce1ac",
      "f3902364d737418286bd1479a2df3db5",
      "598b03ee0c5e414fb49f4a2a4e599b24",
      "4e261293d240433a87440a4c0e1ec734",
      "3b80fec785c34bbc833229e7efe6598b",
      "6e2e45ec85364197af3654bd77b1be09",
      "98bc13262e684633b49df2b58d3887f5",
      "a6b95151011a44b89017d928efe8b66c"
     ]
    },
    "id": "comparative-secret",
    "outputId": "4e2763e2-aa83-4a22-8e9e-03b30748f36a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f4de2271fd47a1bfbef882924cb09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12f62d86241469ea82d143da666d733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1388ce45fd4eff923e31322a7fba1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6308194b878246e2b65c2fbc0aac664f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Pre-processing for BERT model\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add [CLS] and [SEP]\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True,\n",
    "            truncation=True# Return attention mask\n",
    "            )\n",
    "        \n",
    "\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-remains",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "chinese-surveillance",
    "outputId": "c14550a8-cf27-4055-faab-d37058f6d2ca"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-696ca18b8636>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Encode our concatenated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoded_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Find the maximum length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-696ca18b8636>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Encode our concatenated data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mencoded_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_texts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Find the maximum length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2146\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m         )\n\u001b[1;32m   2150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2474\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2475\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2476\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2477\u001b[0m         )\n\u001b[1;32m   2478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m             )\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     (\n\u001b[1;32m    355\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                     )\n\u001b[1;32m    358\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                     \u001b[0msplit_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0moutput_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0moutput_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Concatenate train data and test data\n",
    "all_texts = data.text.values\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_texts = [tokenizer.encode(sent, add_special_tokens=True,max_length=512,truncation=True) for sent in all_texts]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_texts])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-publisher",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bacterial-moderator",
    "outputId": "b22fba6f-7e51-4e59-dc99-96001f658b2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7656\n"
     ]
    }
   ],
   "source": [
    "num=0\n",
    "for sent in encoded_texts:\n",
    "    if len(sent)==512:\n",
    "        num=num+1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-decrease",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "anonymous-utilization",
    "outputId": "4e94c2c4-73b5-4000-aa23-d64999df1c7a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  What’s the secret to happiness? This course at Yale University sets out to answer the question – and it has proved extremely popularA class titled Psychology and the Good Life at Yale University in the US is so popular that one in four undergraduate students have enrolled.Laurie Santos, the psychology professor who teaches on the course, says it is the most students ever to enrol in a class in the history of Yale, which was founded in 1701.Homework assignments include showing more gratitude, performing acts of kindness and improving social connections. The course also includes practical advice such as choosing satisfying careers, and separating satisfying pursuits from hollow ones.\n",
      "Token IDs:  512\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 512\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', len(token_ids))\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-agenda",
   "metadata": {
    "id": "ESxD_-8AwCpF"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-scientist",
   "metadata": {
    "id": "generic-bulgarian"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-pathology",
   "metadata": {
    "id": "exotic-acoustic"
   },
   "outputs": [],
   "source": [
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 2\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-basics",
   "metadata": {
    "id": "compliant-petersburg"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=2,lr=5e-5):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=lr,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-basement",
   "metadata": {
    "id": "higher-donor"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-junior",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5952245abb5044e0b94a1813f7af970a",
      "ace51fa2c9da4dd6a11dea90f0c61e62",
      "cfb6507faa9f450f8c4f211ce43888bc",
      "ae25edd05dd6492eb018bb55f02e2170",
      "5193976517824e749c400949e1b480bc",
      "d17bd1c1dcdb4cd484d3e1bab9e8db1a",
      "aff6365cd709403d9feab4241b141df8",
      "74d9e5c572e94919b3eba34bfc6d75dd",
      "4a41d8de7cd5420d86196bb70268191c",
      "95149713eaa14c108adf09e2215b45a5",
      "eeb1beb6813a4d328b83f2b1a378799e",
      "e7bffe0221e74f6194900b3bc170f467",
      "b354656726b2476b9d569c07929fa4ad",
      "65d2b6a945254353abadf02edcb097b4",
      "060122daf3d5479d931e05f458a5092f",
      "baedf19d86804066becd596c730047a8"
     ]
    },
    "id": "round-praise",
    "outputId": "fa50ef2d-fa2a-4257-f809-c0d5a3778c87"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5952245abb5044e0b94a1813f7af970a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a41d8de7cd5420d86196bb70268191c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.403066   |     -      |     -     |   15.98  \n",
      "   1    |   40    |   0.105622   |     -      |     -     |   15.12  \n",
      "   1    |   60    |   0.035611   |     -      |     -     |   15.50  \n",
      "   1    |   80    |   0.018572   |     -      |     -     |   16.07  \n",
      "   1    |   100   |   0.137186   |     -      |     -     |   16.58  \n",
      "   1    |   120   |   0.038355   |     -      |     -     |   17.13  \n",
      "   1    |   140   |   0.003380   |     -      |     -     |   18.11  \n",
      "   1    |   160   |   0.012644   |     -      |     -     |   18.12  \n",
      "   1    |   180   |   0.001866   |     -      |     -     |   17.41  \n",
      "   1    |   200   |   0.102330   |     -      |     -     |   17.22  \n",
      "   1    |   220   |   0.040823   |     -      |     -     |   17.30  \n",
      "   1    |   240   |   0.066972   |     -      |     -     |   17.66  \n",
      "   1    |   260   |   0.043639   |     -      |     -     |   17.70  \n",
      "   1    |   280   |   0.068194   |     -      |     -     |   17.39  \n",
      "   1    |   300   |   0.043478   |     -      |     -     |   17.23  \n",
      "   1    |   320   |   0.042068   |     -      |     -     |   17.44  \n",
      "   1    |   340   |   0.157195   |     -      |     -     |   17.61  \n",
      "   1    |   360   |   0.057509   |     -      |     -     |   17.56  \n",
      "   1    |   380   |   0.022245   |     -      |     -     |   17.62  \n",
      "   1    |   400   |   0.045208   |     -      |     -     |   17.60  \n",
      "   1    |   420   |   0.048668   |     -      |     -     |   17.72  \n",
      "   1    |   440   |   0.001047   |     -      |     -     |   17.47  \n",
      "   1    |   460   |   0.077647   |     -      |     -     |   17.60  \n",
      "   1    |   480   |   0.059862   |     -      |     -     |   17.54  \n",
      "   1    |   500   |   0.094844   |     -      |     -     |   17.56  \n",
      "   1    |   520   |   0.016389   |     -      |     -     |   17.57  \n",
      "   1    |   540   |   0.039609   |     -      |     -     |   17.47  \n",
      "   1    |   560   |   0.077359   |     -      |     -     |   17.51  \n",
      "   1    |   580   |   0.008886   |     -      |     -     |   17.57  \n",
      "   1    |   600   |   0.000651   |     -      |     -     |   17.44  \n",
      "   1    |   620   |   0.032118   |     -      |     -     |   17.60  \n",
      "   1    |   640   |   0.004828   |     -      |     -     |   17.59  \n",
      "   1    |   660   |   0.040281   |     -      |     -     |   17.60  \n",
      "   1    |   680   |   0.000429   |     -      |     -     |   17.51  \n",
      "   1    |   700   |   0.047732   |     -      |     -     |   17.41  \n",
      "   1    |   720   |   0.001860   |     -      |     -     |   17.45  \n",
      "   1    |   740   |   0.125597   |     -      |     -     |   17.38  \n",
      "   1    |   760   |   0.176777   |     -      |     -     |   17.48  \n",
      "   1    |   780   |   0.001486   |     -      |     -     |   17.42  \n",
      "   1    |   800   |   0.000534   |     -      |     -     |   17.45  \n",
      "   1    |   820   |   0.044446   |     -      |     -     |   17.35  \n",
      "   1    |   840   |   0.000455   |     -      |     -     |   17.36  \n",
      "   1    |   860   |   0.094513   |     -      |     -     |   17.50  \n",
      "   1    |   880   |   0.019159   |     -      |     -     |   17.49  \n",
      "   1    |   900   |   0.000693   |     -      |     -     |   17.52  \n",
      "   1    |   920   |   0.000486   |     -      |     -     |   17.53  \n",
      "   1    |   940   |   0.000416   |     -      |     -     |   17.50  \n",
      "   1    |   960   |   0.001455   |     -      |     -     |   17.52  \n",
      "   1    |   980   |   0.041616   |     -      |     -     |   17.46  \n",
      "   1    |  1000   |   0.075719   |     -      |     -     |   17.53  \n",
      "   1    |  1020   |   0.025292   |     -      |     -     |   17.58  \n",
      "   1    |  1040   |   0.009471   |     -      |     -     |   17.58  \n",
      "   1    |  1060   |   0.025254   |     -      |     -     |   17.50  \n",
      "   1    |  1080   |   0.111393   |     -      |     -     |   17.57  \n",
      "   1    |  1100   |   0.001325   |     -      |     -     |   17.58  \n",
      "   1    |  1120   |   0.069806   |     -      |     -     |   17.59  \n",
      "   1    |  1140   |   0.000504   |     -      |     -     |   17.61  \n",
      "   1    |  1160   |   0.046176   |     -      |     -     |   17.58  \n",
      "   1    |  1180   |   0.000328   |     -      |     -     |   17.68  \n",
      "   1    |  1200   |   0.000282   |     -      |     -     |   17.63  \n",
      "   1    |  1220   |   0.049542   |     -      |     -     |   17.51  \n",
      "   1    |  1240   |   0.002047   |     -      |     -     |   17.68  \n",
      "   1    |  1260   |   0.000463   |     -      |     -     |   17.59  \n",
      "   1    |  1280   |   0.039495   |     -      |     -     |   17.49  \n",
      "   1    |  1300   |   0.000472   |     -      |     -     |   17.62  \n",
      "   1    |  1320   |   0.082129   |     -      |     -     |   17.58  \n",
      "   1    |  1340   |   0.000394   |     -      |     -     |   17.54  \n",
      "   1    |  1360   |   0.000363   |     -      |     -     |   17.44  \n",
      "   1    |  1380   |   0.054442   |     -      |     -     |   17.59  \n",
      "   1    |  1400   |   0.000531   |     -      |     -     |   17.50  \n",
      "   1    |  1420   |   0.000519   |     -      |     -     |   17.56  \n",
      "   1    |  1440   |   0.041945   |     -      |     -     |   17.61  \n",
      "   1    |  1460   |   0.154709   |     -      |     -     |   17.67  \n",
      "   1    |  1480   |   0.022136   |     -      |     -     |   17.61  \n",
      "   1    |  1500   |   0.002933   |     -      |     -     |   17.61  \n",
      "   1    |  1520   |   0.000375   |     -      |     -     |   17.51  \n",
      "   1    |  1540   |   0.000263   |     -      |     -     |   17.57  \n",
      "   1    |  1560   |   0.032064   |     -      |     -     |   17.54  \n",
      "   1    |  1580   |   0.000237   |     -      |     -     |   17.54  \n",
      "   1    |  1600   |   0.028241   |     -      |     -     |   17.61  \n",
      "   1    |  1620   |   0.000287   |     -      |     -     |   17.58  \n",
      "   1    |  1640   |   0.000231   |     -      |     -     |   17.56  \n",
      "   1    |  1660   |   0.000197   |     -      |     -     |   17.60  \n",
      "   1    |  1680   |   0.000203   |     -      |     -     |   17.60  \n",
      "   1    |  1700   |   0.000184   |     -      |     -     |   17.63  \n",
      "   1    |  1720   |   0.050682   |     -      |     -     |   17.58  \n",
      "   1    |  1740   |   0.046529   |     -      |     -     |   17.64  \n",
      "   1    |  1760   |   0.002352   |     -      |     -     |   17.64  \n",
      "   1    |  1780   |   0.000267   |     -      |     -     |   17.56  \n",
      "   1    |  1800   |   0.000181   |     -      |     -     |   17.51  \n",
      "   1    |  1820   |   0.000175   |     -      |     -     |   17.56  \n",
      "   1    |  1840   |   0.052894   |     -      |     -     |   17.41  \n",
      "   1    |  1860   |   0.000214   |     -      |     -     |   17.48  \n",
      "   1    |  1880   |   0.000406   |     -      |     -     |   17.42  \n",
      "   1    |  1900   |   0.005600   |     -      |     -     |   17.45  \n",
      "   1    |  1920   |   0.003344   |     -      |     -     |   17.52  \n",
      "   1    |  1940   |   0.045379   |     -      |     -     |   17.49  \n",
      "   1    |  1960   |   0.000493   |     -      |     -     |   17.58  \n",
      "   1    |  1980   |   0.053944   |     -      |     -     |   17.48  \n",
      "   1    |  2000   |   0.063444   |     -      |     -     |   17.52  \n",
      "   1    |  2020   |   0.000425   |     -      |     -     |   17.67  \n",
      "   1    |  2040   |   0.048367   |     -      |     -     |   17.49  \n",
      "   1    |  2060   |   0.019567   |     -      |     -     |   17.44  \n",
      "   1    |  2080   |   0.051697   |     -      |     -     |   17.36  \n",
      "   1    |  2100   |   0.000256   |     -      |     -     |   17.48  \n",
      "   1    |  2120   |   0.048241   |     -      |     -     |   17.63  \n",
      "   1    |  2140   |   0.045565   |     -      |     -     |   17.57  \n",
      "   1    |  2160   |   0.046193   |     -      |     -     |   17.54  \n",
      "   1    |  2180   |   0.044707   |     -      |     -     |   17.65  \n",
      "   1    |  2200   |   0.111560   |     -      |     -     |   17.57  \n",
      "   1    |  2220   |   0.002552   |     -      |     -     |   17.46  \n",
      "   1    |  2240   |   0.000314   |     -      |     -     |   17.54  \n",
      "   1    |  2260   |   0.000216   |     -      |     -     |   17.50  \n",
      "   1    |  2280   |   0.058003   |     -      |     -     |   17.56  \n",
      "   1    |  2300   |   0.001158   |     -      |     -     |   17.47  \n",
      "   1    |  2320   |   0.012754   |     -      |     -     |   17.69  \n",
      "   1    |  2340   |   0.000184   |     -      |     -     |   17.62  \n",
      "   1    |  2360   |   0.000156   |     -      |     -     |   17.58  \n",
      "   1    |  2380   |   0.053720   |     -      |     -     |   17.49  \n",
      "   1    |  2400   |   0.000334   |     -      |     -     |   17.61  \n",
      "   1    |  2420   |   0.000381   |     -      |     -     |   17.64  \n",
      "   1    |  2440   |   0.018647   |     -      |     -     |   17.56  \n",
      "   1    |  2460   |   0.000176   |     -      |     -     |   17.65  \n",
      "   1    |  2480   |   0.046693   |     -      |     -     |   17.66  \n",
      "   1    |  2500   |   0.074515   |     -      |     -     |   17.65  \n",
      "   1    |  2520   |   0.000424   |     -      |     -     |   17.58  \n",
      "   1    |  2540   |   0.000645   |     -      |     -     |   17.68  \n",
      "   1    |  2560   |   0.000194   |     -      |     -     |   17.54  \n",
      "   1    |  2580   |   0.035392   |     -      |     -     |   17.55  \n",
      "   1    |  2600   |   0.018448   |     -      |     -     |   17.49  \n",
      "   1    |  2620   |   0.000180   |     -      |     -     |   17.46  \n",
      "   1    |  2640   |   0.037701   |     -      |     -     |   17.48  \n",
      "   1    |  2641   |   0.000129   |     -      |     -     |   0.14   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.034150   |  0.017173  |   99.66   |  2401.18 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.000135   |     -      |     -     |   18.39  \n",
      "   2    |   40    |   0.000147   |     -      |     -     |   17.50  \n",
      "   2    |   60    |   0.000134   |     -      |     -     |   17.53  \n",
      "   2    |   80    |   0.000497   |     -      |     -     |   17.54  \n",
      "   2    |   100   |   0.035330   |     -      |     -     |   17.65  \n",
      "   2    |   120   |   0.000727   |     -      |     -     |   17.49  \n",
      "   2    |   140   |   0.000120   |     -      |     -     |   17.60  \n",
      "   2    |   160   |   0.000129   |     -      |     -     |   17.68  \n",
      "   2    |   180   |   0.000116   |     -      |     -     |   17.50  \n",
      "   2    |   200   |   0.042151   |     -      |     -     |   17.47  \n",
      "   2    |   220   |   0.000117   |     -      |     -     |   17.60  \n",
      "   2    |   240   |   0.000119   |     -      |     -     |   17.46  \n",
      "   2    |   260   |   0.000105   |     -      |     -     |   17.50  \n",
      "   2    |   280   |   0.000100   |     -      |     -     |   17.53  \n",
      "   2    |   300   |   0.000104   |     -      |     -     |   17.47  \n",
      "   2    |   320   |   0.000099   |     -      |     -     |   17.40  \n",
      "   2    |   340   |   0.000093   |     -      |     -     |   17.45  \n",
      "   2    |   360   |   0.000090   |     -      |     -     |   17.59  \n",
      "   2    |   380   |   0.057487   |     -      |     -     |   17.60  \n",
      "   2    |   400   |   0.000855   |     -      |     -     |   17.49  \n",
      "   2    |   420   |   0.000165   |     -      |     -     |   17.53  \n",
      "   2    |   440   |   0.000133   |     -      |     -     |   17.59  \n",
      "   2    |   460   |   0.000112   |     -      |     -     |   17.39  \n",
      "   2    |   480   |   0.000118   |     -      |     -     |   17.57  \n",
      "   2    |   500   |   0.000118   |     -      |     -     |   17.54  \n",
      "   2    |   520   |   0.000100   |     -      |     -     |   17.45  \n",
      "   2    |   540   |   0.000096   |     -      |     -     |   17.41  \n",
      "   2    |   560   |   0.000281   |     -      |     -     |   17.48  \n",
      "   2    |   580   |   0.000100   |     -      |     -     |   17.43  \n",
      "   2    |   600   |   0.000091   |     -      |     -     |   17.54  \n",
      "   2    |   620   |   0.000147   |     -      |     -     |   17.59  \n",
      "   2    |   640   |   0.000080   |     -      |     -     |   17.56  \n",
      "   2    |   660   |   0.043401   |     -      |     -     |   17.50  \n",
      "   2    |   680   |   0.000091   |     -      |     -     |   17.53  \n",
      "   2    |   700   |   0.000109   |     -      |     -     |   17.62  \n",
      "   2    |   720   |   0.000089   |     -      |     -     |   17.58  \n",
      "   2    |   740   |   0.036312   |     -      |     -     |   17.62  \n",
      "   2    |   760   |   0.000095   |     -      |     -     |   17.62  \n",
      "   2    |   780   |   0.000082   |     -      |     -     |   17.65  \n",
      "   2    |   800   |   0.000083   |     -      |     -     |   17.54  \n",
      "   2    |   820   |   0.000149   |     -      |     -     |   17.59  \n",
      "   2    |   840   |   0.011750   |     -      |     -     |   17.64  \n",
      "   2    |   860   |   0.000081   |     -      |     -     |   17.62  \n",
      "   2    |   880   |   0.000075   |     -      |     -     |   17.67  \n",
      "   2    |   900   |   0.000072   |     -      |     -     |   17.62  \n",
      "   2    |   920   |   0.000073   |     -      |     -     |   17.50  \n",
      "   2    |   940   |   0.004327   |     -      |     -     |   17.62  \n",
      "   2    |   960   |   0.036829   |     -      |     -     |   17.60  \n",
      "   2    |   980   |   0.000079   |     -      |     -     |   17.51  \n",
      "   2    |  1000   |   0.000073   |     -      |     -     |   17.51  \n",
      "   2    |  1020   |   0.000076   |     -      |     -     |   17.53  \n",
      "   2    |  1040   |   0.000067   |     -      |     -     |   17.52  \n",
      "   2    |  1060   |   0.000072   |     -      |     -     |   17.56  \n",
      "   2    |  1080   |   0.027449   |     -      |     -     |   17.57  \n",
      "   2    |  1100   |   0.000069   |     -      |     -     |   17.46  \n",
      "   2    |  1120   |   0.000125   |     -      |     -     |   17.48  \n",
      "   2    |  1140   |   0.000065   |     -      |     -     |   17.60  \n",
      "   2    |  1160   |   0.000069   |     -      |     -     |   17.60  \n",
      "   2    |  1180   |   0.000064   |     -      |     -     |   17.58  \n",
      "   2    |  1200   |   0.000064   |     -      |     -     |   17.61  \n",
      "   2    |  1220   |   0.000062   |     -      |     -     |   17.51  \n",
      "   2    |  1240   |   0.000067   |     -      |     -     |   17.48  \n",
      "   2    |  1260   |   0.000081   |     -      |     -     |   17.45  \n",
      "   2    |  1280   |   0.000061   |     -      |     -     |   17.51  \n",
      "   2    |  1300   |   0.000057   |     -      |     -     |   17.53  \n",
      "   2    |  1320   |   0.000058   |     -      |     -     |   17.40  \n",
      "   2    |  1340   |   0.000057   |     -      |     -     |   17.49  \n",
      "   2    |  1360   |   0.000118   |     -      |     -     |   17.53  \n",
      "   2    |  1380   |   0.000056   |     -      |     -     |   17.51  \n",
      "   2    |  1400   |   0.000056   |     -      |     -     |   17.50  \n",
      "   2    |  1420   |   0.000054   |     -      |     -     |   17.55  \n",
      "   2    |  1440   |   0.000054   |     -      |     -     |   17.57  \n",
      "   2    |  1460   |   0.000061   |     -      |     -     |   17.58  \n",
      "   2    |  1480   |   0.000054   |     -      |     -     |   17.64  \n",
      "   2    |  1500   |   0.000053   |     -      |     -     |   17.49  \n",
      "   2    |  1520   |   0.000052   |     -      |     -     |   17.54  \n",
      "   2    |  1540   |   0.000051   |     -      |     -     |   17.63  \n",
      "   2    |  1560   |   0.000058   |     -      |     -     |   17.59  \n",
      "   2    |  1580   |   0.000050   |     -      |     -     |   17.61  \n",
      "   2    |  1600   |   0.000055   |     -      |     -     |   17.60  \n",
      "   2    |  1620   |   0.000051   |     -      |     -     |   17.65  \n",
      "   2    |  1640   |   0.000049   |     -      |     -     |   17.65  \n",
      "   2    |  1660   |   0.000052   |     -      |     -     |   17.64  \n",
      "   2    |  1680   |   0.000049   |     -      |     -     |   17.60  \n",
      "   2    |  1700   |   0.000051   |     -      |     -     |   17.54  \n",
      "   2    |  1720   |   0.000068   |     -      |     -     |   17.46  \n",
      "   2    |  1740   |   0.000048   |     -      |     -     |   17.44  \n",
      "   2    |  1760   |   0.000050   |     -      |     -     |   17.45  \n",
      "   2    |  1780   |   0.000057   |     -      |     -     |   17.53  \n",
      "   2    |  1800   |   0.000050   |     -      |     -     |   17.52  \n",
      "   2    |  1820   |   0.000047   |     -      |     -     |   17.52  \n",
      "   2    |  1840   |   0.000047   |     -      |     -     |   17.40  \n",
      "   2    |  1860   |   0.000046   |     -      |     -     |   17.47  \n",
      "   2    |  1880   |   0.000046   |     -      |     -     |   17.48  \n",
      "   2    |  1900   |   0.000046   |     -      |     -     |   17.48  \n",
      "   2    |  1920   |   0.000047   |     -      |     -     |   17.36  \n",
      "   2    |  1940   |   0.000045   |     -      |     -     |   17.46  \n",
      "   2    |  1960   |   0.000044   |     -      |     -     |   17.41  \n",
      "   2    |  1980   |   0.000058   |     -      |     -     |   17.45  \n",
      "   2    |  2000   |   0.000045   |     -      |     -     |   17.40  \n",
      "   2    |  2020   |   0.000045   |     -      |     -     |   17.47  \n",
      "   2    |  2040   |   0.000044   |     -      |     -     |   17.45  \n",
      "   2    |  2060   |   0.057215   |     -      |     -     |   17.55  \n",
      "   2    |  2080   |   0.000046   |     -      |     -     |   17.49  \n",
      "   2    |  2100   |   0.000056   |     -      |     -     |   17.58  \n",
      "   2    |  2120   |   0.000047   |     -      |     -     |   17.55  \n",
      "   2    |  2140   |   0.022761   |     -      |     -     |   17.47  \n",
      "   2    |  2160   |   0.000049   |     -      |     -     |   17.58  \n",
      "   2    |  2180   |   0.000048   |     -      |     -     |   17.54  \n",
      "   2    |  2200   |   0.000105   |     -      |     -     |   17.54  \n",
      "   2    |  2220   |   0.000048   |     -      |     -     |   17.48  \n",
      "   2    |  2240   |   0.000056   |     -      |     -     |   17.52  \n",
      "   2    |  2260   |   0.000047   |     -      |     -     |   17.51  \n",
      "   2    |  2280   |   0.052014   |     -      |     -     |   17.64  \n",
      "   2    |  2300   |   0.000051   |     -      |     -     |   17.54  \n",
      "   2    |  2320   |   0.000047   |     -      |     -     |   17.53  \n",
      "   2    |  2340   |   0.000047   |     -      |     -     |   17.58  \n",
      "   2    |  2360   |   0.000048   |     -      |     -     |   17.63  \n",
      "   2    |  2380   |   0.000048   |     -      |     -     |   17.54  \n",
      "   2    |  2400   |   0.000048   |     -      |     -     |   17.68  \n",
      "   2    |  2420   |   0.000048   |     -      |     -     |   17.64  \n",
      "   2    |  2440   |   0.000047   |     -      |     -     |   17.64  \n",
      "   2    |  2460   |   0.000047   |     -      |     -     |   17.56  \n",
      "   2    |  2480   |   0.000053   |     -      |     -     |   17.54  \n",
      "   2    |  2500   |   0.000046   |     -      |     -     |   17.51  \n",
      "   2    |  2520   |   0.000049   |     -      |     -     |   17.46  \n",
      "   2    |  2540   |   0.000046   |     -      |     -     |   17.45  \n",
      "   2    |  2560   |   0.000047   |     -      |     -     |   17.40  \n",
      "   2    |  2580   |   0.000056   |     -      |     -     |   17.41  \n",
      "   2    |  2600   |   0.000046   |     -      |     -     |   17.63  \n",
      "   2    |  2620   |   0.000048   |     -      |     -     |   17.47  \n",
      "   2    |  2640   |   0.000055   |     -      |     -     |   17.47  \n",
      "   2    |  2641   |   0.000037   |     -      |     -     |   0.14   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.003314   |  0.008306  |   99.91   |  2408.89 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2,lr=5e-5)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-wound",
   "metadata": {
    "id": "engaged-processor"
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model1.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-animation",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3eba9ffef5944435ac316c8e6418e3df",
      "b2e42814904948728288aa00a86798fd",
      "7c414b0c29fd4eebb00af32b6cb4a926",
      "4be9ab392a8b45fc8517b1f0464ad65f",
      "e2c1489e27ee4c75b01abd9999f1ffd6",
      "4df09bd241f5466dabe0bb8c58ccf682",
      "00b4ac654b194d9787bbff041e293a4f",
      "eb561b4163bc41b08778fee1cdaef0e0",
      "a4ddd7b790664701845bf3816a5934a1",
      "38b6a4e2ce384612a0eaa4e9f6f81a98",
      "b0302be9e7ae43b7a1ee1afc826afdb3",
      "4e8fb7e4364a488d8b37300a8bb1c553",
      "6c063963d9644c6e8ccf7b978bda62c5",
      "7b7ee9b2f8ad4de1968323e1899cd2a2",
      "00c3911fd2bb49f192e2f28d14f515b2",
      "89b742442f51489d8b72f267b82fd6cb"
     ]
    },
    "id": "vJETwAykJK3J",
    "outputId": "4a12ef8e-d8c3-43e4-fc65-f3db73670e5a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eba9ffef5944435ac316c8e6418e3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ddd7b790664701845bf3816a5934a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.403109   |     -      |     -     |   16.72  \n",
      "   1    |   40    |   0.105499   |     -      |     -     |   14.62  \n",
      "   1    |   60    |   0.035844   |     -      |     -     |   14.80  \n",
      "   1    |   80    |   0.018020   |     -      |     -     |   15.33  \n",
      "   1    |   100   |   0.138348   |     -      |     -     |   15.57  \n",
      "   1    |   120   |   0.038239   |     -      |     -     |   16.11  \n",
      "   1    |   140   |   0.003223   |     -      |     -     |   16.64  \n",
      "   1    |   160   |   0.107043   |     -      |     -     |   16.21  \n",
      "   1    |   180   |   0.004141   |     -      |     -     |   15.91  \n",
      "   1    |   200   |   0.085096   |     -      |     -     |   15.85  \n",
      "   1    |   220   |   0.002214   |     -      |     -     |   15.86  \n",
      "   1    |   240   |   0.104193   |     -      |     -     |   16.00  \n",
      "   1    |   260   |   0.052908   |     -      |     -     |   16.15  \n",
      "   1    |   280   |   0.116087   |     -      |     -     |   16.08  \n",
      "   1    |   300   |   0.039316   |     -      |     -     |   15.98  \n",
      "   1    |   320   |   0.033817   |     -      |     -     |   16.04  \n",
      "   1    |   340   |   0.080003   |     -      |     -     |   16.07  \n",
      "   1    |   360   |   0.034176   |     -      |     -     |   16.06  \n",
      "   1    |   380   |   0.019372   |     -      |     -     |   16.08  \n",
      "   1    |   400   |   0.027848   |     -      |     -     |   16.05  \n",
      "   1    |   420   |   0.003444   |     -      |     -     |   16.17  \n",
      "   1    |   440   |   0.038240   |     -      |     -     |   15.99  \n",
      "   1    |   460   |   0.047766   |     -      |     -     |   16.09  \n",
      "   1    |   480   |   0.128566   |     -      |     -     |   16.07  \n",
      "   1    |   500   |   0.147145   |     -      |     -     |   16.06  \n",
      "   1    |   520   |   0.017438   |     -      |     -     |   16.05  \n",
      "   1    |   540   |   0.042778   |     -      |     -     |   15.98  \n",
      "   1    |   560   |   0.045674   |     -      |     -     |   16.00  \n",
      "   1    |   580   |   0.042987   |     -      |     -     |   16.07  \n",
      "   1    |   600   |   0.015338   |     -      |     -     |   15.97  \n",
      "   1    |   620   |   0.090510   |     -      |     -     |   16.09  \n",
      "   1    |   640   |   0.001293   |     -      |     -     |   16.09  \n",
      "   1    |   660   |   0.000791   |     -      |     -     |   16.15  \n",
      "   1    |   680   |   0.039463   |     -      |     -     |   16.12  \n",
      "   1    |   700   |   0.108173   |     -      |     -     |   16.03  \n",
      "   1    |   720   |   0.000873   |     -      |     -     |   16.04  \n",
      "   1    |   740   |   0.046016   |     -      |     -     |   16.01  \n",
      "   1    |   760   |   0.177747   |     -      |     -     |   16.09  \n",
      "   1    |   780   |   0.046397   |     -      |     -     |   16.01  \n",
      "   1    |   800   |   0.068984   |     -      |     -     |   16.03  \n",
      "   1    |   820   |   0.000883   |     -      |     -     |   15.97  \n",
      "   1    |   840   |   0.062281   |     -      |     -     |   15.98  \n",
      "   1    |   860   |   0.374813   |     -      |     -     |   16.10  \n",
      "   1    |   880   |   0.085589   |     -      |     -     |   16.06  \n",
      "   1    |   900   |   0.078178   |     -      |     -     |   16.07  \n",
      "   1    |   920   |   0.089569   |     -      |     -     |   16.06  \n",
      "   1    |   940   |   0.000928   |     -      |     -     |   16.06  \n",
      "   1    |   960   |   0.012922   |     -      |     -     |   16.08  \n",
      "   1    |   980   |   0.000648   |     -      |     -     |   16.02  \n",
      "   1    |  1000   |   0.020828   |     -      |     -     |   16.07  \n",
      "   1    |  1020   |   0.040457   |     -      |     -     |   16.12  \n",
      "   1    |  1040   |   0.055295   |     -      |     -     |   16.12  \n",
      "   1    |  1060   |   0.043528   |     -      |     -     |   16.08  \n",
      "   1    |  1080   |   0.080577   |     -      |     -     |   16.12  \n",
      "   1    |  1100   |   0.002206   |     -      |     -     |   16.14  \n",
      "   1    |  1120   |   0.059468   |     -      |     -     |   16.10  \n",
      "   1    |  1140   |   0.074079   |     -      |     -     |   16.17  \n",
      "   1    |  1160   |   0.000409   |     -      |     -     |   16.10  \n",
      "   1    |  1180   |   0.000462   |     -      |     -     |   16.17  \n",
      "   1    |  1200   |   0.000315   |     -      |     -     |   16.11  \n",
      "   1    |  1220   |   0.044426   |     -      |     -     |   16.01  \n",
      "   1    |  1240   |   0.013482   |     -      |     -     |   16.16  \n",
      "   1    |  1260   |   0.001683   |     -      |     -     |   16.07  \n",
      "   1    |  1280   |   0.017481   |     -      |     -     |   15.99  \n",
      "   1    |  1300   |   0.000320   |     -      |     -     |   16.08  \n",
      "   1    |  1320   |   0.042009   |     -      |     -     |   16.07  \n",
      "   1    |  1340   |   0.004496   |     -      |     -     |   16.03  \n",
      "   1    |  1360   |   0.000294   |     -      |     -     |   15.96  \n",
      "   1    |  1380   |   0.020582   |     -      |     -     |   16.06  \n",
      "   1    |  1400   |   0.043204   |     -      |     -     |   15.99  \n",
      "   1    |  1420   |   0.069037   |     -      |     -     |   16.05  \n",
      "   1    |  1440   |   0.003770   |     -      |     -     |   16.08  \n",
      "   1    |  1460   |   0.043232   |     -      |     -     |   16.12  \n",
      "   1    |  1480   |   0.049231   |     -      |     -     |   16.06  \n",
      "   1    |  1500   |   0.003426   |     -      |     -     |   16.05  \n",
      "   1    |  1520   |   0.051361   |     -      |     -     |   16.00  \n",
      "   1    |  1540   |   0.000252   |     -      |     -     |   16.04  \n",
      "   1    |  1560   |   0.000227   |     -      |     -     |   16.04  \n",
      "   1    |  1580   |   0.000218   |     -      |     -     |   16.01  \n",
      "   1    |  1600   |   0.000204   |     -      |     -     |   16.10  \n",
      "   1    |  1620   |   0.043544   |     -      |     -     |   16.09  \n",
      "   1    |  1640   |   0.000222   |     -      |     -     |   16.05  \n",
      "   1    |  1660   |   0.000234   |     -      |     -     |   16.10  \n",
      "   1    |  1680   |   0.001019   |     -      |     -     |   16.11  \n",
      "   1    |  1700   |   0.000432   |     -      |     -     |   16.15  \n",
      "   1    |  1720   |   0.036243   |     -      |     -     |   16.06  \n",
      "   1    |  1740   |   0.053054   |     -      |     -     |   16.12  \n",
      "   1    |  1760   |   0.010033   |     -      |     -     |   16.12  \n",
      "   1    |  1780   |   0.000156   |     -      |     -     |   16.02  \n",
      "   1    |  1800   |   0.000148   |     -      |     -     |   16.01  \n",
      "   1    |  1820   |   0.000145   |     -      |     -     |   16.01  \n",
      "   1    |  1840   |   0.013556   |     -      |     -     |   15.99  \n",
      "   1    |  1860   |   0.020729   |     -      |     -     |   16.09  \n",
      "   1    |  1880   |   0.050148   |     -      |     -     |   16.01  \n",
      "   1    |  1900   |   0.000234   |     -      |     -     |   16.01  \n",
      "   1    |  1920   |   0.090661   |     -      |     -     |   16.06  \n",
      "   1    |  1940   |   0.042934   |     -      |     -     |   16.03  \n",
      "   1    |  1960   |   0.000306   |     -      |     -     |   16.12  \n",
      "   1    |  1980   |   0.000335   |     -      |     -     |   16.02  \n",
      "   1    |  2000   |   0.049936   |     -      |     -     |   16.04  \n",
      "   1    |  2020   |   0.000434   |     -      |     -     |   16.15  \n",
      "   1    |  2040   |   0.000199   |     -      |     -     |   16.08  \n",
      "   1    |  2060   |   0.000225   |     -      |     -     |   16.10  \n",
      "   1    |  2080   |   0.000807   |     -      |     -     |   16.04  \n",
      "   1    |  2100   |   0.000140   |     -      |     -     |   16.08  \n",
      "   1    |  2120   |   0.056798   |     -      |     -     |   16.12  \n",
      "   1    |  2140   |   0.079494   |     -      |     -     |   16.05  \n",
      "   1    |  2160   |   0.052929   |     -      |     -     |   16.04  \n",
      "   1    |  2180   |   0.039477   |     -      |     -     |   16.17  \n",
      "   1    |  2200   |   0.051502   |     -      |     -     |   16.14  \n",
      "   1    |  2220   |   0.000757   |     -      |     -     |   16.09  \n",
      "   1    |  2240   |   0.000601   |     -      |     -     |   16.15  \n",
      "   1    |  2260   |   0.000371   |     -      |     -     |   16.07  \n",
      "   1    |  2280   |   0.030112   |     -      |     -     |   16.10  \n",
      "   1    |  2300   |   0.000251   |     -      |     -     |   16.01  \n",
      "   1    |  2320   |   0.000479   |     -      |     -     |   16.17  \n",
      "   1    |  2340   |   0.000626   |     -      |     -     |   16.12  \n",
      "   1    |  2360   |   0.016114   |     -      |     -     |   16.08  \n",
      "   1    |  2380   |   0.106372   |     -      |     -     |   15.97  \n",
      "   1    |  2400   |   0.002294   |     -      |     -     |   16.11  \n",
      "   1    |  2420   |   0.003772   |     -      |     -     |   16.12  \n",
      "   1    |  2440   |   0.000161   |     -      |     -     |   16.04  \n",
      "   1    |  2460   |   0.000164   |     -      |     -     |   16.10  \n",
      "   1    |  2480   |   0.010526   |     -      |     -     |   16.12  \n",
      "   1    |  2500   |   0.104500   |     -      |     -     |   16.11  \n",
      "   1    |  2520   |   0.000551   |     -      |     -     |   16.09  \n",
      "   1    |  2540   |   0.000290   |     -      |     -     |   16.18  \n",
      "   1    |  2560   |   0.037301   |     -      |     -     |   16.07  \n",
      "   1    |  2580   |   0.020657   |     -      |     -     |   16.07  \n",
      "   1    |  2600   |   0.042772   |     -      |     -     |   16.05  \n",
      "   1    |  2620   |   0.028503   |     -      |     -     |   16.03  \n",
      "   1    |  2640   |   0.011234   |     -      |     -     |   16.06  \n",
      "   1    |  2641   |   0.000102   |     -      |     -     |   0.14   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.039252   |  0.009991  |   99.83   |  2203.19 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.000117   |     -      |     -     |   16.92  \n",
      "   2    |   40    |   0.000116   |     -      |     -     |   16.06  \n",
      "   2    |   60    |   0.000119   |     -      |     -     |   16.07  \n",
      "   2    |   80    |   0.000154   |     -      |     -     |   16.06  \n",
      "   2    |   100   |   0.000100   |     -      |     -     |   16.11  \n",
      "   2    |   120   |   0.029999   |     -      |     -     |   16.01  \n",
      "   2    |   140   |   0.033216   |     -      |     -     |   16.09  \n",
      "   2    |   160   |   0.042022   |     -      |     -     |   16.20  \n",
      "   2    |   180   |   0.033030   |     -      |     -     |   16.06  \n",
      "   2    |   200   |   0.000191   |     -      |     -     |   16.04  \n",
      "   2    |   220   |   0.000168   |     -      |     -     |   16.15  \n",
      "   2    |   240   |   0.000108   |     -      |     -     |   16.00  \n",
      "   2    |   260   |   0.000091   |     -      |     -     |   16.07  \n",
      "   2    |   280   |   0.000083   |     -      |     -     |   16.08  \n",
      "   2    |   300   |   0.000081   |     -      |     -     |   16.02  \n",
      "   2    |   320   |   0.027834   |     -      |     -     |   15.99  \n",
      "   2    |   340   |   0.000079   |     -      |     -     |   15.99  \n",
      "   2    |   360   |   0.000117   |     -      |     -     |   16.08  \n",
      "   2    |   380   |   0.000075   |     -      |     -     |   16.07  \n",
      "   2    |   400   |   0.000072   |     -      |     -     |   15.98  \n",
      "   2    |   420   |   0.000071   |     -      |     -     |   16.02  \n",
      "   2    |   440   |   0.000071   |     -      |     -     |   16.09  \n",
      "   2    |   460   |   0.060568   |     -      |     -     |   15.97  \n",
      "   2    |   480   |   0.000262   |     -      |     -     |   16.07  \n",
      "   2    |   500   |   0.000224   |     -      |     -     |   16.10  \n",
      "   2    |   520   |   0.000128   |     -      |     -     |   16.03  \n",
      "   2    |   540   |   0.000115   |     -      |     -     |   16.03  \n",
      "   2    |   560   |   0.000102   |     -      |     -     |   16.04  \n",
      "   2    |   580   |   0.000129   |     -      |     -     |   16.03  \n",
      "   2    |   600   |   0.000279   |     -      |     -     |   16.10  \n",
      "   2    |   620   |   0.000094   |     -      |     -     |   16.14  \n",
      "   2    |   640   |   0.000088   |     -      |     -     |   16.10  \n",
      "   2    |   660   |   0.034878   |     -      |     -     |   16.08  \n",
      "   2    |   680   |   0.000086   |     -      |     -     |   16.08  \n",
      "   2    |   700   |   0.004845   |     -      |     -     |   16.14  \n",
      "   2    |   720   |   0.000081   |     -      |     -     |   16.11  \n",
      "   2    |   740   |   0.003878   |     -      |     -     |   16.17  \n",
      "   2    |   760   |   0.000193   |     -      |     -     |   16.13  \n",
      "   2    |   780   |   0.000082   |     -      |     -     |   16.14  \n",
      "   2    |   800   |   0.000073   |     -      |     -     |   16.06  \n",
      "   2    |   820   |   0.000071   |     -      |     -     |   16.09  \n",
      "   2    |   840   |   0.056360   |     -      |     -     |   16.16  \n",
      "   2    |   860   |   0.000070   |     -      |     -     |   16.11  \n",
      "   2    |   880   |   0.000077   |     -      |     -     |   16.11  \n",
      "   2    |   900   |   0.000066   |     -      |     -     |   16.10  \n",
      "   2    |   920   |   0.000161   |     -      |     -     |   16.00  \n",
      "   2    |   940   |   0.055293   |     -      |     -     |   16.11  \n",
      "   2    |   960   |   0.000094   |     -      |     -     |   16.09  \n",
      "   2    |   980   |   0.000615   |     -      |     -     |   16.00  \n",
      "   2    |  1000   |   0.047155   |     -      |     -     |   16.02  \n",
      "   2    |  1020   |   0.000171   |     -      |     -     |   16.02  \n",
      "   2    |  1040   |   0.000082   |     -      |     -     |   16.01  \n",
      "   2    |  1060   |   0.000102   |     -      |     -     |   16.03  \n",
      "   2    |  1080   |   0.000114   |     -      |     -     |   16.07  \n",
      "   2    |  1100   |   0.000078   |     -      |     -     |   16.01  \n",
      "   2    |  1120   |   0.000067   |     -      |     -     |   15.99  \n",
      "   2    |  1140   |   0.000063   |     -      |     -     |   16.07  \n",
      "   2    |  1160   |   0.000062   |     -      |     -     |   16.05  \n",
      "   2    |  1180   |   0.000060   |     -      |     -     |   16.08  \n",
      "   2    |  1200   |   0.000062   |     -      |     -     |   16.10  \n",
      "   2    |  1220   |   0.000068   |     -      |     -     |   16.05  \n",
      "   2    |  1240   |   0.000056   |     -      |     -     |   16.02  \n",
      "   2    |  1260   |   0.000056   |     -      |     -     |   16.03  \n",
      "   2    |  1280   |   0.000211   |     -      |     -     |   16.09  \n",
      "   2    |  1300   |   0.000554   |     -      |     -     |   16.10  \n",
      "   2    |  1320   |   0.000052   |     -      |     -     |   16.02  \n",
      "   2    |  1340   |   0.000051   |     -      |     -     |   16.05  \n",
      "   2    |  1360   |   0.000054   |     -      |     -     |   16.04  \n",
      "   2    |  1380   |   0.000051   |     -      |     -     |   16.02  \n",
      "   2    |  1400   |   0.000049   |     -      |     -     |   15.96  \n",
      "   2    |  1420   |   0.000048   |     -      |     -     |   16.00  \n",
      "   2    |  1440   |   0.000047   |     -      |     -     |   16.01  \n",
      "   2    |  1460   |   0.000047   |     -      |     -     |   16.04  \n",
      "   2    |  1480   |   0.000046   |     -      |     -     |   16.07  \n",
      "   2    |  1500   |   0.000045   |     -      |     -     |   15.96  \n",
      "   2    |  1520   |   0.000045   |     -      |     -     |   16.02  \n",
      "   2    |  1540   |   0.000044   |     -      |     -     |   16.09  \n",
      "   2    |  1560   |   0.023400   |     -      |     -     |   16.05  \n",
      "   2    |  1580   |   0.000046   |     -      |     -     |   16.05  \n",
      "   2    |  1600   |   0.000851   |     -      |     -     |   16.05  \n",
      "   2    |  1620   |   0.000059   |     -      |     -     |   16.08  \n",
      "   2    |  1640   |   0.059469   |     -      |     -     |   16.09  \n",
      "   2    |  1660   |   0.052335   |     -      |     -     |   16.06  \n",
      "   2    |  1680   |   0.000080   |     -      |     -     |   16.09  \n",
      "   2    |  1700   |   0.000054   |     -      |     -     |   16.03  \n",
      "   2    |  1720   |   0.000082   |     -      |     -     |   16.00  \n",
      "   2    |  1740   |   0.000046   |     -      |     -     |   16.01  \n",
      "   2    |  1760   |   0.000046   |     -      |     -     |   16.01  \n",
      "   2    |  1780   |   0.000045   |     -      |     -     |   16.06  \n",
      "   2    |  1800   |   0.000042   |     -      |     -     |   16.09  \n",
      "   2    |  1820   |   0.000086   |     -      |     -     |   16.08  \n",
      "   2    |  1840   |   0.000050   |     -      |     -     |   15.97  \n",
      "   2    |  1860   |   0.000042   |     -      |     -     |   16.03  \n",
      "   2    |  1880   |   0.000040   |     -      |     -     |   16.02  \n",
      "   2    |  1900   |   0.055295   |     -      |     -     |   16.04  \n",
      "   2    |  1920   |   0.000149   |     -      |     -     |   15.97  \n",
      "   2    |  1940   |   0.000130   |     -      |     -     |   16.08  \n",
      "   2    |  1960   |   0.000055   |     -      |     -     |   16.00  \n",
      "   2    |  1980   |   0.000070   |     -      |     -     |   16.05  \n",
      "   2    |  2000   |   0.000047   |     -      |     -     |   16.02  \n",
      "   2    |  2020   |   0.000047   |     -      |     -     |   16.08  \n",
      "   2    |  2040   |   0.000043   |     -      |     -     |   16.07  \n",
      "   2    |  2060   |   0.026644   |     -      |     -     |   16.10  \n",
      "   2    |  2080   |   0.023073   |     -      |     -     |   16.06  \n",
      "   2    |  2100   |   0.000040   |     -      |     -     |   16.10  \n",
      "   2    |  2120   |   0.000040   |     -      |     -     |   16.07  \n",
      "   2    |  2140   |   0.000067   |     -      |     -     |   16.03  \n",
      "   2    |  2160   |   0.000036   |     -      |     -     |   16.07  \n",
      "   2    |  2180   |   0.000235   |     -      |     -     |   16.07  \n",
      "   2    |  2200   |   0.000044   |     -      |     -     |   16.07  \n",
      "   2    |  2220   |   0.000056   |     -      |     -     |   16.02  \n",
      "   2    |  2240   |   0.000695   |     -      |     -     |   16.05  \n",
      "   2    |  2260   |   0.000036   |     -      |     -     |   16.04  \n",
      "   2    |  2280   |   0.000035   |     -      |     -     |   16.16  \n",
      "   2    |  2300   |   0.000033   |     -      |     -     |   16.04  \n",
      "   2    |  2320   |   0.000032   |     -      |     -     |   16.05  \n",
      "   2    |  2340   |   0.000037   |     -      |     -     |   16.07  \n",
      "   2    |  2360   |   0.055222   |     -      |     -     |   16.13  \n",
      "   2    |  2380   |   0.000040   |     -      |     -     |   16.06  \n",
      "   2    |  2400   |   0.000042   |     -      |     -     |   16.14  \n",
      "   2    |  2420   |   0.000051   |     -      |     -     |   16.08  \n",
      "   2    |  2440   |   0.036940   |     -      |     -     |   16.14  \n",
      "   2    |  2460   |   0.000036   |     -      |     -     |   16.09  \n",
      "   2    |  2480   |   0.000040   |     -      |     -     |   16.10  \n",
      "   2    |  2500   |   0.000031   |     -      |     -     |   16.06  \n",
      "   2    |  2520   |   0.000034   |     -      |     -     |   16.04  \n",
      "   2    |  2540   |   0.029444   |     -      |     -     |   16.01  \n",
      "   2    |  2560   |   0.000040   |     -      |     -     |   16.02  \n",
      "   2    |  2580   |   0.000059   |     -      |     -     |   16.00  \n",
      "   2    |  2600   |   0.000037   |     -      |     -     |   16.15  \n",
      "   2    |  2620   |   0.000034   |     -      |     -     |   16.07  \n",
      "   2    |  2640   |   0.042271   |     -      |     -     |   16.07  \n",
      "   2    |  2641   |   0.000026   |     -      |     -     |   0.14   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.006392   |  0.018878  |   99.70   |  2206.07 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.000877   |     -      |     -     |   16.91  \n",
      "   3    |   40    |   0.000048   |     -      |     -     |   16.13  \n",
      "   3    |   60    |   0.000031   |     -      |     -     |   16.05  \n",
      "   3    |   80    |   0.000033   |     -      |     -     |   16.08  \n",
      "   3    |   100   |   0.000029   |     -      |     -     |   16.16  \n",
      "   3    |   120   |   0.000927   |     -      |     -     |   16.07  \n",
      "   3    |   140   |   0.036745   |     -      |     -     |   16.08  \n",
      "   3    |   160   |   0.000031   |     -      |     -     |   16.10  \n",
      "   3    |   180   |   0.000051   |     -      |     -     |   16.07  \n",
      "   3    |   200   |   0.000056   |     -      |     -     |   16.10  \n",
      "   3    |   220   |   0.000051   |     -      |     -     |   16.04  \n",
      "   3    |   240   |   0.000029   |     -      |     -     |   16.02  \n",
      "   3    |   260   |   0.000030   |     -      |     -     |   16.03  \n",
      "   3    |   280   |   0.000067   |     -      |     -     |   16.06  \n",
      "   3    |   300   |   0.000026   |     -      |     -     |   16.05  \n",
      "   3    |   320   |   0.000027   |     -      |     -     |   15.96  \n",
      "   3    |   340   |   0.000027   |     -      |     -     |   16.11  \n",
      "   3    |   360   |   0.000026   |     -      |     -     |   16.02  \n",
      "   3    |   380   |   0.000031   |     -      |     -     |   16.07  \n",
      "   3    |   400   |   0.000025   |     -      |     -     |   16.02  \n",
      "   3    |   420   |   0.000026   |     -      |     -     |   16.09  \n",
      "   3    |   440   |   0.000026   |     -      |     -     |   16.03  \n",
      "   3    |   460   |   0.000035   |     -      |     -     |   16.04  \n",
      "   3    |   480   |   0.036081   |     -      |     -     |   16.08  \n",
      "   3    |   500   |   0.000024   |     -      |     -     |   16.07  \n",
      "   3    |   520   |   0.000096   |     -      |     -     |   16.13  \n",
      "   3    |   540   |   0.000024   |     -      |     -     |   16.05  \n",
      "   3    |   560   |   0.000024   |     -      |     -     |   15.96  \n",
      "   3    |   580   |   0.000024   |     -      |     -     |   16.06  \n",
      "   3    |   600   |   0.000023   |     -      |     -     |   16.06  \n",
      "   3    |   620   |   0.000023   |     -      |     -     |   16.11  \n",
      "   3    |   640   |   0.000024   |     -      |     -     |   15.99  \n",
      "   3    |   660   |   0.000022   |     -      |     -     |   16.06  \n",
      "   3    |   680   |   0.000022   |     -      |     -     |   16.06  \n",
      "   3    |   700   |   0.000025   |     -      |     -     |   16.10  \n",
      "   3    |   720   |   0.000022   |     -      |     -     |   16.12  \n",
      "   3    |   740   |   0.000022   |     -      |     -     |   16.05  \n",
      "   3    |   760   |   0.000022   |     -      |     -     |   16.10  \n",
      "   3    |   780   |   0.000021   |     -      |     -     |   16.02  \n",
      "   3    |   800   |   0.000021   |     -      |     -     |   16.02  \n",
      "   3    |   820   |   0.000022   |     -      |     -     |   16.11  \n",
      "   3    |   840   |   0.000022   |     -      |     -     |   16.06  \n",
      "   3    |   860   |   0.000021   |     -      |     -     |   16.00  \n",
      "   3    |   880   |   0.000029   |     -      |     -     |   16.03  \n",
      "   3    |   900   |   0.000021   |     -      |     -     |   15.99  \n",
      "   3    |   920   |   0.000026   |     -      |     -     |   16.01  \n",
      "   3    |   940   |   0.000020   |     -      |     -     |   16.08  \n",
      "   3    |   960   |   0.000020   |     -      |     -     |   16.01  \n",
      "   3    |   980   |   0.000027   |     -      |     -     |   16.00  \n",
      "   3    |  1000   |   0.000020   |     -      |     -     |   16.04  \n",
      "   3    |  1020   |   0.054526   |     -      |     -     |   16.00  \n",
      "   3    |  1040   |   0.000191   |     -      |     -     |   16.06  \n",
      "   3    |  1060   |   0.000020   |     -      |     -     |   16.08  \n",
      "   3    |  1080   |   0.000020   |     -      |     -     |   16.06  \n",
      "   3    |  1100   |   0.000020   |     -      |     -     |   16.15  \n",
      "   3    |  1120   |   0.000022   |     -      |     -     |   16.17  \n",
      "   3    |  1140   |   0.000025   |     -      |     -     |   16.12  \n",
      "   3    |  1160   |   0.000020   |     -      |     -     |   16.03  \n",
      "   3    |  1180   |   0.000019   |     -      |     -     |   16.05  \n",
      "   3    |  1200   |   0.000021   |     -      |     -     |   16.11  \n",
      "   3    |  1220   |   0.000019   |     -      |     -     |   16.11  \n",
      "   3    |  1240   |   0.000019   |     -      |     -     |   16.11  \n",
      "   3    |  1260   |   0.000018   |     -      |     -     |   16.06  \n",
      "   3    |  1280   |   0.000018   |     -      |     -     |   16.02  \n",
      "   3    |  1300   |   0.000018   |     -      |     -     |   16.02  \n",
      "   3    |  1320   |   0.000018   |     -      |     -     |   16.04  \n",
      "   3    |  1340   |   0.000018   |     -      |     -     |   16.07  \n",
      "   3    |  1360   |   0.000018   |     -      |     -     |   16.07  \n",
      "   3    |  1380   |   0.000019   |     -      |     -     |   16.07  \n",
      "   3    |  1400   |   0.000018   |     -      |     -     |   16.09  \n",
      "   3    |  1420   |   0.000018   |     -      |     -     |   16.03  \n",
      "   3    |  1440   |   0.000018   |     -      |     -     |   16.09  \n",
      "   3    |  1460   |   0.000765   |     -      |     -     |   16.03  \n",
      "   3    |  1480   |   0.000310   |     -      |     -     |   16.03  \n",
      "   3    |  1500   |   0.000017   |     -      |     -     |   16.11  \n",
      "   3    |  1520   |   0.000017   |     -      |     -     |   16.06  \n",
      "   3    |  1540   |   0.020537   |     -      |     -     |   16.05  \n",
      "   3    |  1560   |   0.000017   |     -      |     -     |   16.13  \n",
      "   3    |  1580   |   0.000016   |     -      |     -     |   16.08  \n",
      "   3    |  1600   |   0.000016   |     -      |     -     |   16.10  \n",
      "   3    |  1620   |   0.000017   |     -      |     -     |   16.14  \n",
      "   3    |  1640   |   0.000016   |     -      |     -     |   16.10  \n",
      "   3    |  1660   |   0.000027   |     -      |     -     |   16.03  \n",
      "   3    |  1680   |   0.000016   |     -      |     -     |   16.02  \n",
      "   3    |  1700   |   0.000022   |     -      |     -     |   16.06  \n",
      "   3    |  1720   |   0.000016   |     -      |     -     |   16.07  \n",
      "   3    |  1740   |   0.000016   |     -      |     -     |   16.11  \n",
      "   3    |  1760   |   0.000015   |     -      |     -     |   16.08  \n",
      "   3    |  1780   |   0.000016   |     -      |     -     |   16.09  \n",
      "   3    |  1800   |   0.000022   |     -      |     -     |   16.01  \n",
      "   3    |  1820   |   0.032396   |     -      |     -     |   16.02  \n",
      "   3    |  1840   |   0.000016   |     -      |     -     |   16.06  \n",
      "   3    |  1860   |   0.000016   |     -      |     -     |   16.04  \n",
      "   3    |  1880   |   0.000015   |     -      |     -     |   15.97  \n",
      "   3    |  1900   |   0.000015   |     -      |     -     |   16.02  \n",
      "   3    |  1920   |   0.000016   |     -      |     -     |   15.99  \n",
      "   3    |  1940   |   0.000015   |     -      |     -     |   16.06  \n",
      "   3    |  1960   |   0.000015   |     -      |     -     |   16.01  \n",
      "   3    |  1980   |   0.000015   |     -      |     -     |   16.02  \n",
      "   3    |  2000   |   0.000039   |     -      |     -     |   16.01  \n",
      "   3    |  2020   |   0.000016   |     -      |     -     |   16.07  \n",
      "   3    |  2040   |   0.000016   |     -      |     -     |   15.98  \n",
      "   3    |  2060   |   0.000015   |     -      |     -     |   15.99  \n",
      "   3    |  2080   |   0.000018   |     -      |     -     |   15.95  \n",
      "   3    |  2100   |   0.000019   |     -      |     -     |   16.01  \n",
      "   3    |  2120   |   0.000019   |     -      |     -     |   16.02  \n",
      "   3    |  2140   |   0.000016   |     -      |     -     |   16.06  \n",
      "   3    |  2160   |   0.000018   |     -      |     -     |   16.04  \n",
      "   3    |  2180   |   0.000016   |     -      |     -     |   16.06  \n",
      "   3    |  2200   |   0.000884   |     -      |     -     |   16.09  \n",
      "   3    |  2220   |   0.000016   |     -      |     -     |   16.03  \n",
      "   3    |  2240   |   0.000015   |     -      |     -     |   16.13  \n",
      "   3    |  2260   |   0.000016   |     -      |     -     |   16.03  \n",
      "   3    |  2280   |   0.000015   |     -      |     -     |   16.01  \n",
      "   3    |  2300   |   0.000015   |     -      |     -     |   16.08  \n",
      "   3    |  2320   |   0.000014   |     -      |     -     |   15.99  \n",
      "   3    |  2340   |   0.000014   |     -      |     -     |   16.02  \n",
      "   3    |  2360   |   0.000015   |     -      |     -     |   15.95  \n",
      "   3    |  2380   |   0.000015   |     -      |     -     |   16.04  \n",
      "   3    |  2400   |   0.000014   |     -      |     -     |   16.03  \n",
      "   3    |  2420   |   0.000017   |     -      |     -     |   16.05  \n",
      "   3    |  2440   |   0.000014   |     -      |     -     |   16.04  \n",
      "   3    |  2460   |   0.000014   |     -      |     -     |   15.91  \n",
      "   3    |  2480   |   0.000014   |     -      |     -     |   16.04  \n",
      "   3    |  2500   |   0.034583   |     -      |     -     |   15.97  \n",
      "   3    |  2520   |   0.000014   |     -      |     -     |   16.05  \n",
      "   3    |  2540   |   0.000015   |     -      |     -     |   15.98  \n",
      "   3    |  2560   |   0.000014   |     -      |     -     |   16.01  \n",
      "   3    |  2580   |   0.000014   |     -      |     -     |   16.02  \n",
      "   3    |  2600   |   0.000014   |     -      |     -     |   16.08  \n",
      "   3    |  2620   |   0.000014   |     -      |     -     |   15.99  \n",
      "   3    |  2640   |   0.000014   |     -      |     -     |   16.02  \n",
      "   3    |  2641   |   0.000012   |     -      |     -     |   0.14   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.001677   |  0.014445  |   99.87   |  2204.71 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5d95ffaa380f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mycolab/ob/lr_model2.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=3,lr=5e-5)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model2.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-injection",
   "metadata": {
    "id": "FNCFH0dBHDyy"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model2.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-injury",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cedb2ca08fa342ec94e7acb6ecf58ed2",
      "6bc9a7b3f20b4be58e7a3030c01dd1d8",
      "8f7c3875657d4f0ea7c7d8abe9389b68",
      "91e38294851140e78b7328b4dac71e82",
      "f83ca4fade6040359a6da1d000dad5f7",
      "7fd76663043c412f8fd92b9bebb60539",
      "fe387e812f8542bba4bd7eba212b69eb",
      "a04e456f444a40afb9d911b71ee67f30",
      "fc3b4a35f0b54e6997085bbd3c73bd50",
      "361d85ba71bd443385856476c55d9ea7",
      "f99e44b4d9004190878b98da41f6a308",
      "f039da7554684ed9afe3c70370adc785",
      "61062e55b02f41f88edf3407c18d270e",
      "fddf144da84d41fb8c332ec0b2357bd8",
      "a2afd530a5ea42e593e173f9301b0fdb",
      "3977a980d05b4eac9a232d3a1bfa315b"
     ]
    },
    "id": "4rsRM7CbJSCi",
    "outputId": "5099970d-4bd3-4cb5-8e9a-af4176606a05"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cedb2ca08fa342ec94e7acb6ecf58ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc3b4a35f0b54e6997085bbd3c73bd50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.341009   |     -      |     -     |   10.23  \n",
      "   1    |   40    |   0.035067   |     -      |     -     |   9.47   \n",
      "   1    |   60    |   0.076399   |     -      |     -     |   9.49   \n",
      "   1    |   80    |   0.050432   |     -      |     -     |   9.51   \n",
      "   1    |   100   |   0.050829   |     -      |     -     |   9.50   \n",
      "   1    |   120   |   0.055280   |     -      |     -     |   9.50   \n",
      "   1    |   140   |   0.083510   |     -      |     -     |   9.52   \n",
      "   1    |   160   |   0.056794   |     -      |     -     |   9.50   \n",
      "   1    |   180   |   0.043393   |     -      |     -     |   9.52   \n",
      "   1    |   200   |   0.024454   |     -      |     -     |   9.52   \n",
      "   1    |   220   |   0.017199   |     -      |     -     |   9.51   \n",
      "   1    |   240   |   0.075596   |     -      |     -     |   9.53   \n",
      "   1    |   260   |   0.022581   |     -      |     -     |   9.52   \n",
      "   1    |   280   |   0.013869   |     -      |     -     |   9.52   \n",
      "   1    |   300   |   0.069306   |     -      |     -     |   9.52   \n",
      "   1    |   320   |   0.048524   |     -      |     -     |   9.50   \n",
      "   1    |   340   |   0.001724   |     -      |     -     |   9.49   \n",
      "   1    |   360   |   0.004512   |     -      |     -     |   9.50   \n",
      "   1    |   380   |   0.103004   |     -      |     -     |   9.54   \n",
      "   1    |   400   |   0.023270   |     -      |     -     |   9.52   \n",
      "   1    |   420   |   0.017941   |     -      |     -     |   9.50   \n",
      "   1    |   440   |   0.047292   |     -      |     -     |   9.52   \n",
      "   1    |   460   |   0.005470   |     -      |     -     |   9.51   \n",
      "   1    |   480   |   0.040530   |     -      |     -     |   9.50   \n",
      "   1    |   500   |   0.041569   |     -      |     -     |   9.50   \n",
      "   1    |   520   |   0.010234   |     -      |     -     |   9.50   \n",
      "   1    |   540   |   0.022761   |     -      |     -     |   9.53   \n",
      "   1    |   560   |   0.001595   |     -      |     -     |   9.52   \n",
      "   1    |   580   |   0.015244   |     -      |     -     |   9.50   \n",
      "   1    |   600   |   0.000580   |     -      |     -     |   9.53   \n",
      "   1    |   620   |   0.023682   |     -      |     -     |   9.50   \n",
      "   1    |   640   |   0.000883   |     -      |     -     |   9.49   \n",
      "   1    |   660   |   0.032401   |     -      |     -     |   9.53   \n",
      "   1    |   680   |   0.016203   |     -      |     -     |   9.53   \n",
      "   1    |   700   |   0.023429   |     -      |     -     |   9.53   \n",
      "   1    |   720   |   0.000636   |     -      |     -     |   9.53   \n",
      "   1    |   740   |   0.094633   |     -      |     -     |   9.53   \n",
      "   1    |   760   |   0.019297   |     -      |     -     |   9.51   \n",
      "   1    |   780   |   0.000470   |     -      |     -     |   9.50   \n",
      "   1    |   800   |   0.000428   |     -      |     -     |   9.51   \n",
      "   1    |   820   |   0.000392   |     -      |     -     |   9.51   \n",
      "   1    |   840   |   0.000562   |     -      |     -     |   9.52   \n",
      "   1    |   860   |   0.026962   |     -      |     -     |   9.50   \n",
      "   1    |   880   |   0.002908   |     -      |     -     |   9.52   \n",
      "   1    |   900   |   0.001086   |     -      |     -     |   9.51   \n",
      "   1    |   920   |   0.000588   |     -      |     -     |   9.50   \n",
      "   1    |   940   |   0.000302   |     -      |     -     |   9.51   \n",
      "   1    |   960   |   0.022351   |     -      |     -     |   9.51   \n",
      "   1    |   980   |   0.004561   |     -      |     -     |   9.51   \n",
      "   1    |  1000   |   0.039306   |     -      |     -     |   9.50   \n",
      "   1    |  1020   |   0.000506   |     -      |     -     |   9.52   \n",
      "   1    |  1040   |   0.043786   |     -      |     -     |   9.51   \n",
      "   1    |  1060   |   0.024422   |     -      |     -     |   9.50   \n",
      "   1    |  1080   |   0.028317   |     -      |     -     |   9.51   \n",
      "   1    |  1100   |   0.048858   |     -      |     -     |   9.51   \n",
      "   1    |  1120   |   0.001244   |     -      |     -     |   9.50   \n",
      "   1    |  1140   |   0.014428   |     -      |     -     |   9.51   \n",
      "   1    |  1160   |   0.000582   |     -      |     -     |   9.53   \n",
      "   1    |  1180   |   0.020234   |     -      |     -     |   9.54   \n",
      "   1    |  1200   |   0.024679   |     -      |     -     |   9.50   \n",
      "   1    |  1220   |   0.014339   |     -      |     -     |   9.50   \n",
      "   1    |  1240   |   0.000573   |     -      |     -     |   9.51   \n",
      "   1    |  1260   |   0.023334   |     -      |     -     |   9.52   \n",
      "   1    |  1280   |   0.000413   |     -      |     -     |   9.51   \n",
      "   1    |  1300   |   0.000500   |     -      |     -     |   9.53   \n",
      "   1    |  1320   |   0.024596   |     -      |     -     |   9.31   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.030264   |  0.003125  |   99.96   |  651.29  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.000255   |     -      |     -     |   9.98   \n",
      "   2    |   40    |   0.000272   |     -      |     -     |   9.49   \n",
      "   2    |   60    |   0.000234   |     -      |     -     |   9.51   \n",
      "   2    |   80    |   0.000251   |     -      |     -     |   9.54   \n",
      "   2    |   100   |   0.010786   |     -      |     -     |   9.51   \n",
      "   2    |   120   |   0.025559   |     -      |     -     |   9.52   \n",
      "   2    |   140   |   0.000241   |     -      |     -     |   9.51   \n",
      "   2    |   160   |   0.000219   |     -      |     -     |   9.50   \n",
      "   2    |   180   |   0.000354   |     -      |     -     |   9.52   \n",
      "   2    |   200   |   0.000186   |     -      |     -     |   9.50   \n",
      "   2    |   220   |   0.022048   |     -      |     -     |   9.55   \n",
      "   2    |   240   |   0.000327   |     -      |     -     |   9.51   \n",
      "   2    |   260   |   0.000213   |     -      |     -     |   9.51   \n",
      "   2    |   280   |   0.000184   |     -      |     -     |   9.51   \n",
      "   2    |   300   |   0.000190   |     -      |     -     |   9.52   \n",
      "   2    |   320   |   0.000182   |     -      |     -     |   9.50   \n",
      "   2    |   340   |   0.000862   |     -      |     -     |   9.50   \n",
      "   2    |   360   |   0.024946   |     -      |     -     |   9.52   \n",
      "   2    |   380   |   0.000158   |     -      |     -     |   9.51   \n",
      "   2    |   400   |   0.019521   |     -      |     -     |   9.51   \n",
      "   2    |   420   |   0.004384   |     -      |     -     |   9.53   \n",
      "   2    |   440   |   0.000141   |     -      |     -     |   9.52   \n",
      "   2    |   460   |   0.000138   |     -      |     -     |   9.50   \n",
      "   2    |   480   |   0.016930   |     -      |     -     |   9.52   \n",
      "   2    |   500   |   0.028415   |     -      |     -     |   9.51   \n",
      "   2    |   520   |   0.000778   |     -      |     -     |   9.50   \n",
      "   2    |   540   |   0.064840   |     -      |     -     |   9.53   \n",
      "   2    |   560   |   0.030460   |     -      |     -     |   9.53   \n",
      "   2    |   580   |   0.000177   |     -      |     -     |   9.51   \n",
      "   2    |   600   |   0.003371   |     -      |     -     |   9.52   \n",
      "   2    |   620   |   0.000141   |     -      |     -     |   9.50   \n",
      "   2    |   640   |   0.000136   |     -      |     -     |   9.51   \n",
      "   2    |   660   |   0.000150   |     -      |     -     |   9.50   \n",
      "   2    |   680   |   0.000129   |     -      |     -     |   9.49   \n",
      "   2    |   700   |   0.000116   |     -      |     -     |   9.53   \n",
      "   2    |   720   |   0.000168   |     -      |     -     |   9.52   \n",
      "   2    |   740   |   0.000105   |     -      |     -     |   9.50   \n",
      "   2    |   760   |   0.000099   |     -      |     -     |   9.51   \n",
      "   2    |   780   |   0.027623   |     -      |     -     |   9.51   \n",
      "   2    |   800   |   0.000146   |     -      |     -     |   9.50   \n",
      "   2    |   820   |   0.000197   |     -      |     -     |   9.53   \n",
      "   2    |   840   |   0.000140   |     -      |     -     |   9.51   \n",
      "   2    |   860   |   0.000252   |     -      |     -     |   9.51   \n",
      "   2    |   880   |   0.000096   |     -      |     -     |   9.49   \n",
      "   2    |   900   |   0.000093   |     -      |     -     |   9.52   \n",
      "   2    |   920   |   0.000125   |     -      |     -     |   9.54   \n",
      "   2    |   940   |   0.000091   |     -      |     -     |   9.50   \n",
      "   2    |   960   |   0.007471   |     -      |     -     |   9.49   \n",
      "   2    |   980   |   0.000094   |     -      |     -     |   9.49   \n",
      "   2    |  1000   |   0.000084   |     -      |     -     |   9.48   \n",
      "   2    |  1020   |   0.000204   |     -      |     -     |   9.52   \n",
      "   2    |  1040   |   0.000413   |     -      |     -     |   9.52   \n",
      "   2    |  1060   |   0.000086   |     -      |     -     |   9.53   \n",
      "   2    |  1080   |   0.000076   |     -      |     -     |   9.51   \n",
      "   2    |  1100   |   0.014107   |     -      |     -     |   9.49   \n",
      "   2    |  1120   |   0.000078   |     -      |     -     |   9.51   \n",
      "   2    |  1140   |   0.000076   |     -      |     -     |   9.50   \n",
      "   2    |  1160   |   0.000095   |     -      |     -     |   9.50   \n",
      "   2    |  1180   |   0.000598   |     -      |     -     |   9.52   \n",
      "   2    |  1200   |   0.000069   |     -      |     -     |   9.54   \n",
      "   2    |  1220   |   0.023957   |     -      |     -     |   9.53   \n",
      "   2    |  1240   |   0.000245   |     -      |     -     |   9.50   \n",
      "   2    |  1260   |   0.000079   |     -      |     -     |   9.49   \n",
      "   2    |  1280   |   0.000072   |     -      |     -     |   9.50   \n",
      "   2    |  1300   |   0.000067   |     -      |     -     |   9.49   \n",
      "   2    |  1320   |   0.029350   |     -      |     -     |   9.32   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.005506   |  0.007309  |   99.87   |  651.00  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.000336   |     -      |     -     |   9.99   \n",
      "   3    |   40    |   0.000230   |     -      |     -     |   9.50   \n",
      "   3    |   60    |   0.000133   |     -      |     -     |   9.51   \n",
      "   3    |   80    |   0.000106   |     -      |     -     |   9.50   \n",
      "   3    |   100   |   0.000092   |     -      |     -     |   9.51   \n",
      "   3    |   120   |   0.001067   |     -      |     -     |   9.50   \n",
      "   3    |   140   |   0.009097   |     -      |     -     |   9.50   \n",
      "   3    |   160   |   0.000096   |     -      |     -     |   9.50   \n",
      "   3    |   180   |   0.000073   |     -      |     -     |   9.48   \n",
      "   3    |   200   |   0.000070   |     -      |     -     |   9.50   \n",
      "   3    |   220   |   0.000068   |     -      |     -     |   9.53   \n",
      "   3    |   240   |   0.000068   |     -      |     -     |   9.52   \n",
      "   3    |   260   |   0.000064   |     -      |     -     |   9.53   \n",
      "   3    |   280   |   0.000063   |     -      |     -     |   9.48   \n",
      "   3    |   300   |   0.023342   |     -      |     -     |   9.51   \n",
      "   3    |   320   |   0.000090   |     -      |     -     |   9.51   \n",
      "   3    |   340   |   0.000077   |     -      |     -     |   9.50   \n",
      "   3    |   360   |   0.000105   |     -      |     -     |   9.52   \n",
      "   3    |   380   |   0.000070   |     -      |     -     |   9.52   \n",
      "   3    |   400   |   0.000079   |     -      |     -     |   9.50   \n",
      "   3    |   420   |   0.012116   |     -      |     -     |   9.49   \n",
      "   3    |   440   |   0.000069   |     -      |     -     |   9.51   \n",
      "   3    |   460   |   0.014365   |     -      |     -     |   9.52   \n",
      "   3    |   480   |   0.028631   |     -      |     -     |   9.49   \n",
      "   3    |   500   |   0.000475   |     -      |     -     |   9.51   \n",
      "   3    |   520   |   0.000064   |     -      |     -     |   9.53   \n",
      "   3    |   540   |   0.000058   |     -      |     -     |   9.51   \n",
      "   3    |   560   |   0.014386   |     -      |     -     |   9.51   \n",
      "   3    |   580   |   0.000092   |     -      |     -     |   9.52   \n",
      "   3    |   600   |   0.000136   |     -      |     -     |   9.52   \n",
      "   3    |   620   |   0.000061   |     -      |     -     |   9.51   \n",
      "   3    |   640   |   0.002195   |     -      |     -     |   9.50   \n",
      "   3    |   660   |   0.000053   |     -      |     -     |   9.50   \n",
      "   3    |   680   |   0.006803   |     -      |     -     |   9.51   \n",
      "   3    |   700   |   0.028737   |     -      |     -     |   9.50   \n",
      "   3    |   720   |   0.000081   |     -      |     -     |   9.52   \n",
      "   3    |   740   |   0.014529   |     -      |     -     |   9.51   \n",
      "   3    |   760   |   0.000082   |     -      |     -     |   9.51   \n",
      "   3    |   780   |   0.002367   |     -      |     -     |   9.50   \n",
      "   3    |   800   |   0.000063   |     -      |     -     |   9.50   \n",
      "   3    |   820   |   0.000191   |     -      |     -     |   9.51   \n",
      "   3    |   840   |   0.000072   |     -      |     -     |   9.51   \n",
      "   3    |   860   |   0.000062   |     -      |     -     |   9.52   \n",
      "   3    |   880   |   0.000052   |     -      |     -     |   9.52   \n",
      "   3    |   900   |   0.000052   |     -      |     -     |   9.50   \n",
      "   3    |   920   |   0.000051   |     -      |     -     |   9.52   \n",
      "   3    |   940   |   0.000047   |     -      |     -     |   9.52   \n",
      "   3    |   960   |   0.000048   |     -      |     -     |   9.51   \n",
      "   3    |   980   |   0.000088   |     -      |     -     |   9.51   \n",
      "   3    |  1000   |   0.000046   |     -      |     -     |   9.49   \n",
      "   3    |  1020   |   0.000381   |     -      |     -     |   9.51   \n",
      "   3    |  1040   |   0.000046   |     -      |     -     |   9.50   \n",
      "   3    |  1060   |   0.020399   |     -      |     -     |   9.52   \n",
      "   3    |  1080   |   0.000043   |     -      |     -     |   9.52   \n",
      "   3    |  1100   |   0.000335   |     -      |     -     |   9.53   \n",
      "   3    |  1120   |   0.000050   |     -      |     -     |   9.51   \n",
      "   3    |  1140   |   0.000041   |     -      |     -     |   9.50   \n",
      "   3    |  1160   |   0.003411   |     -      |     -     |   9.49   \n",
      "   3    |  1180   |   0.000041   |     -      |     -     |   9.51   \n",
      "   3    |  1200   |   0.000040   |     -      |     -     |   9.53   \n",
      "   3    |  1220   |   0.000041   |     -      |     -     |   9.52   \n",
      "   3    |  1240   |   0.000044   |     -      |     -     |   9.51   \n",
      "   3    |  1260   |   0.000039   |     -      |     -     |   9.49   \n",
      "   3    |  1280   |   0.000042   |     -      |     -     |   9.50   \n",
      "   3    |  1300   |   0.000038   |     -      |     -     |   9.51   \n",
      "   3    |  1320   |   0.020753   |     -      |     -     |   9.30   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.003136   |  0.006514  |   99.91   |  650.79  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   4    |   20    |   0.000039   |     -      |     -     |   9.98   \n",
      "   4    |   40    |   0.000037   |     -      |     -     |   9.49   \n",
      "   4    |   60    |   0.000037   |     -      |     -     |   9.50   \n",
      "   4    |   80    |   0.000037   |     -      |     -     |   9.50   \n",
      "   4    |   100   |   0.000038   |     -      |     -     |   9.51   \n",
      "   4    |   120   |   0.000040   |     -      |     -     |   9.53   \n",
      "   4    |   140   |   0.000038   |     -      |     -     |   9.53   \n",
      "   4    |   160   |   0.000036   |     -      |     -     |   9.52   \n",
      "   4    |   180   |   0.000036   |     -      |     -     |   9.51   \n",
      "   4    |   200   |   0.022118   |     -      |     -     |   9.54   \n",
      "   4    |   220   |   0.000036   |     -      |     -     |   9.51   \n",
      "   4    |   240   |   0.000042   |     -      |     -     |   9.49   \n",
      "   4    |   260   |   0.000035   |     -      |     -     |   9.49   \n",
      "   4    |   280   |   0.000034   |     -      |     -     |   9.52   \n",
      "   4    |   300   |   0.000036   |     -      |     -     |   9.49   \n",
      "   4    |   320   |   0.000036   |     -      |     -     |   9.54   \n",
      "   4    |   340   |   0.000040   |     -      |     -     |   9.51   \n",
      "   4    |   360   |   0.000033   |     -      |     -     |   9.49   \n",
      "   4    |   380   |   0.000034   |     -      |     -     |   9.50   \n",
      "   4    |   400   |   0.000033   |     -      |     -     |   9.49   \n",
      "   4    |   420   |   0.000032   |     -      |     -     |   9.53   \n",
      "   4    |   440   |   0.000034   |     -      |     -     |   9.51   \n",
      "   4    |   460   |   0.000034   |     -      |     -     |   9.51   \n",
      "   4    |   480   |   0.000032   |     -      |     -     |   9.53   \n",
      "   4    |   500   |   0.000035   |     -      |     -     |   9.51   \n",
      "   4    |   520   |   0.000031   |     -      |     -     |   9.53   \n",
      "   4    |   540   |   0.000031   |     -      |     -     |   9.51   \n",
      "   4    |   560   |   0.000032   |     -      |     -     |   9.52   \n",
      "   4    |   580   |   0.000034   |     -      |     -     |   9.52   \n",
      "   4    |   600   |   0.000031   |     -      |     -     |   9.51   \n",
      "   4    |   620   |   0.000035   |     -      |     -     |   9.53   \n",
      "   4    |   640   |   0.000035   |     -      |     -     |   9.49   \n",
      "   4    |   660   |   0.000030   |     -      |     -     |   9.53   \n",
      "   4    |   680   |   0.000031   |     -      |     -     |   9.51   \n",
      "   4    |   700   |   0.000034   |     -      |     -     |   9.51   \n",
      "   4    |   720   |   0.000030   |     -      |     -     |   9.52   \n",
      "   4    |   740   |   0.000030   |     -      |     -     |   9.52   \n",
      "   4    |   760   |   0.000029   |     -      |     -     |   9.51   \n",
      "   4    |   780   |   0.000031   |     -      |     -     |   9.49   \n",
      "   4    |   800   |   0.000029   |     -      |     -     |   9.50   \n",
      "   4    |   820   |   0.000030   |     -      |     -     |   9.50   \n",
      "   4    |   840   |   0.000029   |     -      |     -     |   9.50   \n",
      "   4    |   860   |   0.000029   |     -      |     -     |   9.50   \n",
      "   4    |   880   |   0.000030   |     -      |     -     |   9.51   \n",
      "   4    |   900   |   0.000029   |     -      |     -     |   9.53   \n",
      "   4    |   920   |   0.000029   |     -      |     -     |   9.51   \n",
      "   4    |   940   |   0.000031   |     -      |     -     |   9.52   \n",
      "   4    |   960   |   0.001210   |     -      |     -     |   9.52   \n",
      "   4    |   980   |   0.000028   |     -      |     -     |   9.51   \n",
      "   4    |  1000   |   0.000031   |     -      |     -     |   9.50   \n",
      "   4    |  1020   |   0.000029   |     -      |     -     |   9.50   \n",
      "   4    |  1040   |   0.000028   |     -      |     -     |   9.50   \n",
      "   4    |  1060   |   0.000028   |     -      |     -     |   9.50   \n",
      "   4    |  1080   |   0.000028   |     -      |     -     |   9.53   \n",
      "   4    |  1100   |   0.000028   |     -      |     -     |   9.51   \n",
      "   4    |  1120   |   0.000029   |     -      |     -     |   9.49   \n",
      "   4    |  1140   |   0.000028   |     -      |     -     |   9.50   \n",
      "   4    |  1160   |   0.000028   |     -      |     -     |   9.49   \n",
      "   4    |  1180   |   0.000028   |     -      |     -     |   9.50   \n",
      "   4    |  1200   |   0.000028   |     -      |     -     |   9.50   \n",
      "   4    |  1220   |   0.000028   |     -      |     -     |   9.50   \n",
      "   4    |  1240   |   0.000028   |     -      |     -     |   9.51   \n",
      "   4    |  1260   |   0.000028   |     -      |     -     |   9.50   \n",
      "   4    |  1280   |   0.000027   |     -      |     -     |   9.49   \n",
      "   4    |  1300   |   0.000028   |     -      |     -     |   9.49   \n",
      "   4    |  1320   |   0.000028   |     -      |     -     |   9.30   \n",
      "----------------------------------------------------------------------\n",
      "   4    |    -    |   0.000384   |  0.004229  |   99.96   |  650.83  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=4,lr=5e-5)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=4, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model3.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-windsor",
   "metadata": {
    "id": "8Ox1qAJKHANS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-saturday",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "136062b9e6f04ec9abedf0495c72a3a7",
      "3cc433fa676c4deb8bfb15186f22cbd9",
      "f7bd845f37cb459ab037e84ce02f70c6",
      "12e101c88ad144ed8641f600fa1cc44e",
      "1610044f589b4a9ebbd414bf93947ad3",
      "4a4464861b8b43d5a09047fb96a91db1",
      "5aee5418609e4123a19b445e52fff584",
      "5b817e62ba744c4ea1dbeaa264a960e6",
      "01643d76f2b54f9b92b34152644e8d82",
      "72d20c857c7a499ea8642da5b379cb1a",
      "6ae80cace6bb4d4b98d00090ca8fccd8",
      "37a5b813ad914bc8b5ba4bc585aac713",
      "3275ef25a7a14c47817aff65a570a602",
      "99eef8c0e40341ac95c3f9558c726b0a",
      "f6223d67da1b4840b9733b648ebc55d9",
      "0136b39bb5b64199be6df9af593d42a9"
     ]
    },
    "id": "cbTkbS38JUlv",
    "outputId": "646cde56-eb39-47d6-a55e-5e2b630ed344"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136062b9e6f04ec9abedf0495c72a3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01643d76f2b54f9b92b34152644e8d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.676478   |     -      |     -     |   17.96  \n",
      "   1    |   40    |   0.640667   |     -      |     -     |   16.98  \n",
      "   1    |   60    |   0.576343   |     -      |     -     |   16.96  \n",
      "   1    |   80    |   0.501619   |     -      |     -     |   16.97  \n",
      "   1    |   100   |   0.406502   |     -      |     -     |   16.96  \n",
      "   1    |   120   |   0.321839   |     -      |     -     |   16.96  \n",
      "   1    |   140   |   0.256456   |     -      |     -     |   16.97  \n",
      "   1    |   160   |   0.182960   |     -      |     -     |   16.95  \n",
      "   1    |   180   |   0.159547   |     -      |     -     |   16.96  \n",
      "   1    |   200   |   0.131660   |     -      |     -     |   16.97  \n",
      "   1    |   220   |   0.098434   |     -      |     -     |   16.98  \n",
      "   1    |   240   |   0.080929   |     -      |     -     |   16.94  \n",
      "   1    |   260   |   0.073035   |     -      |     -     |   16.94  \n",
      "   1    |   280   |   0.057160   |     -      |     -     |   16.94  \n",
      "   1    |   300   |   0.051737   |     -      |     -     |   16.93  \n",
      "   1    |   320   |   0.050256   |     -      |     -     |   16.94  \n",
      "   1    |   340   |   0.040888   |     -      |     -     |   16.91  \n",
      "   1    |   360   |   0.042574   |     -      |     -     |   16.95  \n",
      "   1    |   380   |   0.080956   |     -      |     -     |   16.96  \n",
      "   1    |   400   |   0.037596   |     -      |     -     |   16.94  \n",
      "   1    |   420   |   0.052023   |     -      |     -     |   16.92  \n",
      "   1    |   440   |   0.089492   |     -      |     -     |   16.94  \n",
      "   1    |   460   |   0.037915   |     -      |     -     |   16.95  \n",
      "   1    |   480   |   0.043749   |     -      |     -     |   16.94  \n",
      "   1    |   500   |   0.035938   |     -      |     -     |   16.94  \n",
      "   1    |   520   |   0.033681   |     -      |     -     |   16.94  \n",
      "   1    |   540   |   0.046544   |     -      |     -     |   16.95  \n",
      "   1    |   560   |   0.062035   |     -      |     -     |   16.94  \n",
      "   1    |   580   |   0.013821   |     -      |     -     |   16.94  \n",
      "   1    |   600   |   0.015893   |     -      |     -     |   16.94  \n",
      "   1    |   620   |   0.070710   |     -      |     -     |   16.94  \n",
      "   1    |   640   |   0.036985   |     -      |     -     |   16.96  \n",
      "   1    |   660   |   0.019404   |     -      |     -     |   16.94  \n",
      "   1    |   680   |   0.040432   |     -      |     -     |   16.94  \n",
      "   1    |   700   |   0.022991   |     -      |     -     |   16.95  \n",
      "   1    |   720   |   0.036883   |     -      |     -     |   16.94  \n",
      "   1    |   740   |   0.049523   |     -      |     -     |   16.95  \n",
      "   1    |   760   |   0.025564   |     -      |     -     |   16.96  \n",
      "   1    |   780   |   0.021003   |     -      |     -     |   16.93  \n",
      "   1    |   800   |   0.012777   |     -      |     -     |   16.92  \n",
      "   1    |   820   |   0.010698   |     -      |     -     |   16.93  \n",
      "   1    |   840   |   0.017646   |     -      |     -     |   16.94  \n",
      "   1    |   860   |   0.062426   |     -      |     -     |   16.94  \n",
      "   1    |   880   |   0.012641   |     -      |     -     |   16.95  \n",
      "   1    |   900   |   0.015046   |     -      |     -     |   16.97  \n",
      "   1    |   920   |   0.021391   |     -      |     -     |   16.93  \n",
      "   1    |   940   |   0.031325   |     -      |     -     |   16.93  \n",
      "   1    |   960   |   0.034192   |     -      |     -     |   16.95  \n",
      "   1    |   980   |   0.008855   |     -      |     -     |   16.92  \n",
      "   1    |  1000   |   0.051900   |     -      |     -     |   16.94  \n",
      "   1    |  1020   |   0.007852   |     -      |     -     |   16.92  \n",
      "   1    |  1040   |   0.036772   |     -      |     -     |   16.95  \n",
      "   1    |  1060   |   0.030007   |     -      |     -     |   16.93  \n",
      "   1    |  1080   |   0.024614   |     -      |     -     |   16.95  \n",
      "   1    |  1100   |   0.060504   |     -      |     -     |   16.95  \n",
      "   1    |  1120   |   0.054130   |     -      |     -     |   16.95  \n",
      "   1    |  1140   |   0.038309   |     -      |     -     |   16.95  \n",
      "   1    |  1160   |   0.036674   |     -      |     -     |   16.94  \n",
      "   1    |  1180   |   0.013649   |     -      |     -     |   16.93  \n",
      "   1    |  1200   |   0.023535   |     -      |     -     |   16.95  \n",
      "   1    |  1220   |   0.023706   |     -      |     -     |   16.95  \n",
      "   1    |  1240   |   0.030218   |     -      |     -     |   16.94  \n",
      "   1    |  1260   |   0.029796   |     -      |     -     |   16.95  \n",
      "   1    |  1280   |   0.030004   |     -      |     -     |   16.92  \n",
      "   1    |  1300   |   0.023959   |     -      |     -     |   16.93  \n",
      "   1    |  1320   |   0.023692   |     -      |     -     |   16.59  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.091179   |  0.024536  |   99.32   |  1160.90 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.019207   |     -      |     -     |   17.79  \n",
      "   2    |   40    |   0.035445   |     -      |     -     |   16.96  \n",
      "   2    |   60    |   0.013132   |     -      |     -     |   16.95  \n",
      "   2    |   80    |   0.025100   |     -      |     -     |   16.93  \n",
      "   2    |   100   |   0.022560   |     -      |     -     |   16.95  \n",
      "   2    |   120   |   0.021428   |     -      |     -     |   16.95  \n",
      "   2    |   140   |   0.004965   |     -      |     -     |   16.92  \n",
      "   2    |   160   |   0.044986   |     -      |     -     |   16.95  \n",
      "   2    |   180   |   0.024635   |     -      |     -     |   16.93  \n",
      "   2    |   200   |   0.008017   |     -      |     -     |   16.93  \n",
      "   2    |   220   |   0.022804   |     -      |     -     |   16.94  \n",
      "   2    |   240   |   0.007730   |     -      |     -     |   16.94  \n",
      "   2    |   260   |   0.012070   |     -      |     -     |   16.92  \n",
      "   2    |   280   |   0.025320   |     -      |     -     |   16.94  \n",
      "   2    |   300   |   0.005732   |     -      |     -     |   16.93  \n",
      "   2    |   320   |   0.018494   |     -      |     -     |   16.93  \n",
      "   2    |   340   |   0.051532   |     -      |     -     |   16.93  \n",
      "   2    |   360   |   0.023256   |     -      |     -     |   16.96  \n",
      "   2    |   380   |   0.018242   |     -      |     -     |   16.95  \n",
      "   2    |   400   |   0.004774   |     -      |     -     |   16.93  \n",
      "   2    |   420   |   0.040869   |     -      |     -     |   16.95  \n",
      "   2    |   440   |   0.005900   |     -      |     -     |   16.95  \n",
      "   2    |   460   |   0.018984   |     -      |     -     |   16.97  \n",
      "   2    |   480   |   0.045955   |     -      |     -     |   16.94  \n",
      "   2    |   500   |   0.015419   |     -      |     -     |   16.96  \n",
      "   2    |   520   |   0.010937   |     -      |     -     |   16.94  \n",
      "   2    |   540   |   0.043316   |     -      |     -     |   16.94  \n",
      "   2    |   560   |   0.007545   |     -      |     -     |   16.95  \n",
      "   2    |   580   |   0.029475   |     -      |     -     |   16.95  \n",
      "   2    |   600   |   0.027933   |     -      |     -     |   16.95  \n",
      "   2    |   620   |   0.022506   |     -      |     -     |   16.96  \n",
      "   2    |   640   |   0.020648   |     -      |     -     |   16.95  \n",
      "   2    |   660   |   0.017171   |     -      |     -     |   16.94  \n",
      "   2    |   680   |   0.004942   |     -      |     -     |   16.96  \n",
      "   2    |   700   |   0.030263   |     -      |     -     |   16.95  \n",
      "   2    |   720   |   0.007461   |     -      |     -     |   16.96  \n",
      "   2    |   740   |   0.012329   |     -      |     -     |   16.94  \n",
      "   2    |   760   |   0.004151   |     -      |     -     |   16.93  \n",
      "   2    |   780   |   0.046775   |     -      |     -     |   16.96  \n",
      "   2    |   800   |   0.041993   |     -      |     -     |   16.97  \n",
      "   2    |   820   |   0.006102   |     -      |     -     |   16.97  \n",
      "   2    |   840   |   0.079818   |     -      |     -     |   16.96  \n",
      "   2    |   860   |   0.067651   |     -      |     -     |   16.96  \n",
      "   2    |   880   |   0.005347   |     -      |     -     |   16.95  \n",
      "   2    |   900   |   0.009817   |     -      |     -     |   16.94  \n",
      "   2    |   920   |   0.023310   |     -      |     -     |   16.95  \n",
      "   2    |   940   |   0.007089   |     -      |     -     |   16.96  \n",
      "   2    |   960   |   0.033189   |     -      |     -     |   16.94  \n",
      "   2    |   980   |   0.020004   |     -      |     -     |   16.96  \n",
      "   2    |  1000   |   0.004572   |     -      |     -     |   16.96  \n",
      "   2    |  1020   |   0.024983   |     -      |     -     |   16.96  \n",
      "   2    |  1040   |   0.030779   |     -      |     -     |   16.96  \n",
      "   2    |  1060   |   0.011643   |     -      |     -     |   16.95  \n",
      "   2    |  1080   |   0.010985   |     -      |     -     |   16.94  \n",
      "   2    |  1100   |   0.020468   |     -      |     -     |   16.95  \n",
      "   2    |  1120   |   0.020945   |     -      |     -     |   16.95  \n",
      "   2    |  1140   |   0.022366   |     -      |     -     |   16.95  \n",
      "   2    |  1160   |   0.023177   |     -      |     -     |   16.96  \n",
      "   2    |  1180   |   0.017919   |     -      |     -     |   16.95  \n",
      "   2    |  1200   |   0.012355   |     -      |     -     |   16.96  \n",
      "   2    |  1220   |   0.007004   |     -      |     -     |   16.96  \n",
      "   2    |  1240   |   0.034226   |     -      |     -     |   16.96  \n",
      "   2    |  1260   |   0.004566   |     -      |     -     |   16.95  \n",
      "   2    |  1280   |   0.004387   |     -      |     -     |   16.95  \n",
      "   2    |  1300   |   0.006943   |     -      |     -     |   16.95  \n",
      "   2    |  1320   |   0.021865   |     -      |     -     |   16.59  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.021143   |  0.019530  |   99.53   |  1161.02 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2,lr=2e-6)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model4.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-registrar",
   "metadata": {
    "id": "zT7b-qTDJX3K"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=3,lr=5e-6)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model5.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-referral",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c467340bb5324356a4cd0a6d45ece484",
      "db7968309df34569aade64de63d38da0",
      "bb65a6dbf9f0432f9264271e36488d22",
      "8085a8c47c5942698edf39a9786550bb",
      "9ea46bd09f3547b296981b6376ee33f7",
      "70d4273195ac43928313cc0dd73f067b",
      "646e396f91c344d0ae1a33d1d0bf049e",
      "002938520c414ee285395437d57f8a2b",
      "4f1c25a0a98543439dfc97674668e0dd",
      "9616d8d1706943d19c82fd9cdd804cbd",
      "e3ede741e3e34f819817bdcf2ef04699",
      "01182e29779940599e17c5caee682d39",
      "3e96078563f64baab04244454f476913",
      "1c5030f346e34c789ab3e8dd6d992cab",
      "45b0cd3ac6e34bf0b9848a37f1992058",
      "5d4acabbfa5b43ddaa2cc44cb24faae1"
     ]
    },
    "id": "5moQ5iphJagW",
    "outputId": "ef46ff50-2155-493f-8343-33f18fef4453"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c467340bb5324356a4cd0a6d45ece484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1c25a0a98543439dfc97674668e0dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.670454   |     -      |     -     |   17.88  \n",
      "   1    |   40    |   0.602665   |     -      |     -     |   16.92  \n",
      "   1    |   60    |   0.491426   |     -      |     -     |   16.92  \n",
      "   1    |   80    |   0.362424   |     -      |     -     |   16.91  \n",
      "   1    |   100   |   0.250362   |     -      |     -     |   16.94  \n",
      "   1    |   120   |   0.178239   |     -      |     -     |   16.92  \n",
      "   1    |   140   |   0.147064   |     -      |     -     |   16.91  \n",
      "   1    |   160   |   0.094126   |     -      |     -     |   16.92  \n",
      "   1    |   180   |   0.084845   |     -      |     -     |   16.92  \n",
      "   1    |   200   |   0.082623   |     -      |     -     |   16.94  \n",
      "   1    |   220   |   0.053554   |     -      |     -     |   16.92  \n",
      "   1    |   240   |   0.052630   |     -      |     -     |   16.90  \n",
      "   1    |   260   |   0.050711   |     -      |     -     |   16.92  \n",
      "   1    |   280   |   0.038635   |     -      |     -     |   16.90  \n",
      "   1    |   300   |   0.034691   |     -      |     -     |   16.94  \n",
      "   1    |   320   |   0.037177   |     -      |     -     |   16.89  \n",
      "   1    |   340   |   0.029669   |     -      |     -     |   16.92  \n",
      "   1    |   360   |   0.030382   |     -      |     -     |   16.91  \n",
      "   1    |   380   |   0.070432   |     -      |     -     |   16.93  \n",
      "   1    |   400   |   0.027622   |     -      |     -     |   16.89  \n",
      "   1    |   420   |   0.046058   |     -      |     -     |   16.90  \n",
      "   1    |   440   |   0.089055   |     -      |     -     |   16.92  \n",
      "   1    |   460   |   0.028626   |     -      |     -     |   16.90  \n",
      "   1    |   480   |   0.029732   |     -      |     -     |   16.92  \n",
      "   1    |   500   |   0.022575   |     -      |     -     |   16.90  \n",
      "   1    |   520   |   0.026375   |     -      |     -     |   16.91  \n",
      "   1    |   540   |   0.041641   |     -      |     -     |   16.89  \n",
      "   1    |   560   |   0.054432   |     -      |     -     |   16.90  \n",
      "   1    |   580   |   0.008320   |     -      |     -     |   16.90  \n",
      "   1    |   600   |   0.008957   |     -      |     -     |   16.89  \n",
      "   1    |   620   |   0.067693   |     -      |     -     |   16.91  \n",
      "   1    |   640   |   0.028181   |     -      |     -     |   16.92  \n",
      "   1    |   660   |   0.019219   |     -      |     -     |   16.91  \n",
      "   1    |   680   |   0.035033   |     -      |     -     |   16.91  \n",
      "   1    |   700   |   0.024113   |     -      |     -     |   16.91  \n",
      "   1    |   720   |   0.028507   |     -      |     -     |   16.92  \n",
      "   1    |   740   |   0.045511   |     -      |     -     |   16.93  \n",
      "   1    |   760   |   0.021099   |     -      |     -     |   16.90  \n",
      "   1    |   780   |   0.015740   |     -      |     -     |   16.90  \n",
      "   1    |   800   |   0.007863   |     -      |     -     |   16.89  \n",
      "   1    |   820   |   0.005855   |     -      |     -     |   16.90  \n",
      "   1    |   840   |   0.010874   |     -      |     -     |   16.90  \n",
      "   1    |   860   |   0.059388   |     -      |     -     |   16.91  \n",
      "   1    |   880   |   0.008979   |     -      |     -     |   16.90  \n",
      "   1    |   900   |   0.012000   |     -      |     -     |   16.91  \n",
      "   1    |   920   |   0.017533   |     -      |     -     |   16.93  \n",
      "   1    |   940   |   0.023412   |     -      |     -     |   16.90  \n",
      "   1    |   960   |   0.026122   |     -      |     -     |   16.94  \n",
      "   1    |   980   |   0.004824   |     -      |     -     |   16.89  \n",
      "   1    |  1000   |   0.044644   |     -      |     -     |   16.90  \n",
      "   1    |  1020   |   0.005036   |     -      |     -     |   16.90  \n",
      "   1    |  1040   |   0.030725   |     -      |     -     |   16.90  \n",
      "   1    |  1060   |   0.032022   |     -      |     -     |   16.91  \n",
      "   1    |  1080   |   0.021948   |     -      |     -     |   16.91  \n",
      "   1    |  1100   |   0.053911   |     -      |     -     |   16.93  \n",
      "   1    |  1120   |   0.054371   |     -      |     -     |   16.94  \n",
      "   1    |  1140   |   0.032612   |     -      |     -     |   16.92  \n",
      "   1    |  1160   |   0.028919   |     -      |     -     |   16.92  \n",
      "   1    |  1180   |   0.008885   |     -      |     -     |   16.88  \n",
      "   1    |  1200   |   0.022012   |     -      |     -     |   16.92  \n",
      "   1    |  1220   |   0.022819   |     -      |     -     |   16.89  \n",
      "   1    |  1240   |   0.035135   |     -      |     -     |   16.92  \n",
      "   1    |  1260   |   0.024643   |     -      |     -     |   16.90  \n",
      "   1    |  1280   |   0.025932   |     -      |     -     |   16.87  \n",
      "   1    |  1300   |   0.022388   |     -      |     -     |   16.92  \n",
      "   1    |  1320   |   0.018464   |     -      |     -     |   16.55  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.071543   |  0.019574  |   99.57   |  1158.28 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.011073   |     -      |     -     |   17.77  \n",
      "   2    |   40    |   0.026436   |     -      |     -     |   16.89  \n",
      "   2    |   60    |   0.010743   |     -      |     -     |   16.93  \n",
      "   2    |   80    |   0.023158   |     -      |     -     |   16.90  \n",
      "   2    |   100   |   0.017281   |     -      |     -     |   16.94  \n",
      "   2    |   120   |   0.019985   |     -      |     -     |   16.93  \n",
      "   2    |   140   |   0.003321   |     -      |     -     |   16.90  \n",
      "   2    |   160   |   0.031629   |     -      |     -     |   16.91  \n",
      "   2    |   180   |   0.030732   |     -      |     -     |   16.94  \n",
      "   2    |   200   |   0.006364   |     -      |     -     |   16.91  \n",
      "   2    |   220   |   0.019780   |     -      |     -     |   16.91  \n",
      "   2    |   240   |   0.004305   |     -      |     -     |   16.90  \n",
      "   2    |   260   |   0.005709   |     -      |     -     |   16.92  \n",
      "   2    |   280   |   0.025401   |     -      |     -     |   16.90  \n",
      "   2    |   300   |   0.003229   |     -      |     -     |   16.90  \n",
      "   2    |   320   |   0.011002   |     -      |     -     |   16.92  \n",
      "   2    |   340   |   0.049962   |     -      |     -     |   16.92  \n",
      "   2    |   360   |   0.022816   |     -      |     -     |   16.91  \n",
      "   2    |   380   |   0.006925   |     -      |     -     |   16.93  \n",
      "   2    |   400   |   0.003589   |     -      |     -     |   16.91  \n",
      "   2    |   420   |   0.026776   |     -      |     -     |   16.93  \n",
      "   2    |   440   |   0.003166   |     -      |     -     |   16.89  \n",
      "   2    |   460   |   0.005894   |     -      |     -     |   16.91  \n",
      "   2    |   480   |   0.038977   |     -      |     -     |   16.92  \n",
      "   2    |   500   |   0.013932   |     -      |     -     |   16.89  \n",
      "   2    |   520   |   0.003696   |     -      |     -     |   16.90  \n",
      "   2    |   540   |   0.043428   |     -      |     -     |   16.91  \n",
      "   2    |   560   |   0.005213   |     -      |     -     |   16.90  \n",
      "   2    |   580   |   0.025392   |     -      |     -     |   16.89  \n",
      "   2    |   600   |   0.014885   |     -      |     -     |   16.92  \n",
      "   2    |   620   |   0.013544   |     -      |     -     |   16.91  \n",
      "   2    |   640   |   0.008630   |     -      |     -     |   16.91  \n",
      "   2    |   660   |   0.006170   |     -      |     -     |   16.92  \n",
      "   2    |   680   |   0.002976   |     -      |     -     |   16.91  \n",
      "   2    |   700   |   0.022620   |     -      |     -     |   16.92  \n",
      "   2    |   720   |   0.003225   |     -      |     -     |   16.92  \n",
      "   2    |   740   |   0.002909   |     -      |     -     |   16.92  \n",
      "   2    |   760   |   0.002645   |     -      |     -     |   16.91  \n",
      "   2    |   780   |   0.039896   |     -      |     -     |   16.94  \n",
      "   2    |   800   |   0.038306   |     -      |     -     |   16.93  \n",
      "   2    |   820   |   0.005257   |     -      |     -     |   16.93  \n",
      "   2    |   840   |   0.062773   |     -      |     -     |   16.92  \n",
      "   2    |   860   |   0.054666   |     -      |     -     |   16.92  \n",
      "   2    |   880   |   0.003002   |     -      |     -     |   16.92  \n",
      "   2    |   900   |   0.005167   |     -      |     -     |   16.91  \n",
      "   2    |   920   |   0.012760   |     -      |     -     |   16.91  \n",
      "   2    |   940   |   0.003068   |     -      |     -     |   16.90  \n",
      "   2    |   960   |   0.019692   |     -      |     -     |   16.91  \n",
      "   2    |   980   |   0.017703   |     -      |     -     |   16.93  \n",
      "   2    |  1000   |   0.002764   |     -      |     -     |   16.91  \n",
      "   2    |  1020   |   0.020150   |     -      |     -     |   16.92  \n",
      "   2    |  1040   |   0.026394   |     -      |     -     |   16.93  \n",
      "   2    |  1060   |   0.006523   |     -      |     -     |   16.90  \n",
      "   2    |  1080   |   0.008187   |     -      |     -     |   16.92  \n",
      "   2    |  1100   |   0.019561   |     -      |     -     |   16.93  \n",
      "   2    |  1120   |   0.014263   |     -      |     -     |   16.92  \n",
      "   2    |  1140   |   0.020254   |     -      |     -     |   16.92  \n",
      "   2    |  1160   |   0.022703   |     -      |     -     |   16.92  \n",
      "   2    |  1180   |   0.010139   |     -      |     -     |   16.91  \n",
      "   2    |  1200   |   0.004273   |     -      |     -     |   16.91  \n",
      "   2    |  1220   |   0.005188   |     -      |     -     |   16.91  \n",
      "   2    |  1240   |   0.020560   |     -      |     -     |   16.92  \n",
      "   2    |  1260   |   0.002575   |     -      |     -     |   16.92  \n",
      "   2    |  1280   |   0.002625   |     -      |     -     |   16.91  \n",
      "   2    |  1300   |   0.003020   |     -      |     -     |   16.93  \n",
      "   2    |  1320   |   0.017402   |     -      |     -     |   16.55  \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.015851   |  0.014321  |   99.57   |  1158.44 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8066af684a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mycolab/ob/lr_model6.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2,lr=3e-6)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model6.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-timothy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "id": "rs9GidKpoqZi",
    "outputId": "f8e06b89-463f-4e8f-8801-d563619df361"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a45fe7b185be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mycolab/ob/lr_model.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-rocket",
   "metadata": {
    "id": "LpR-G3GwJera"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=3,lr=3e-6)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model7.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-prompt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "id": "civil-apollo",
    "outputId": "0a886fc0-bbcd-46a3-b65d-50fefc90d01c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d1ff856b5428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lr_model.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lr_model.pickle'"
     ]
    }
   ],
   "source": [
    "with open('lr_model.pickle', 'rb') as fp:\n",
    "    bert_model = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-upper",
   "metadata": {
    "id": "accessory-berlin"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-pension",
   "metadata": {
    "id": "duplicate-guatemala"
   },
   "outputs": [],
   "source": [
    "data_test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-gazette",
   "metadata": {
    "id": "alert-vanilla"
   },
   "outputs": [],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "print('Tokenizing data...')\n",
    "test_inputs, test_masks = preprocessing_for_bert(data_test.text)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-oliver",
   "metadata": {
    "id": "governing-active"
   },
   "outputs": [],
   "source": [
    "data_pos_test = pd.read_csv('dataset/positive_news_dataset.csv')\n",
    "data_pos_test['label'] = 1\n",
    "data_pos_test = data_pos_test[3100:3600]\n",
    "data_neg_test = pd.read_csv('dataset/crime_online_dataset.csv')\n",
    "data_neg_test['label'] = 0\n",
    "data_neg_test = data_neg_test[3100:3600]\n",
    "data_test = pd.concat([data_pos_test, data_neg_test], axis=0).reset_index(drop=True)\n",
    "data_test.drop(['url'], inplace=True, axis=1)\n",
    "data_test.drop(['sentiment'], inplace=True, axis=1)\n",
    "X_test = data_test.text.values\n",
    "y_test = data_test.label.values\n",
    "test_inputs, test_masks = preprocessing_for_bert(data_test.text)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-gazette",
   "metadata": {
    "id": "changed-hands"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-format",
   "metadata": {
    "id": "strategic-performance"
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-pricing",
   "metadata": {
    "id": "native-blair"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-postage",
   "metadata": {
    "id": "reliable-provision"
   },
   "outputs": [],
   "source": [
    "data_pos_test = pd.read_csv('dataset/good_news_network_dataset.csv')\n",
    "data_pos_test['label'] = 1\n",
    "data_pos_test = data_pos_test[1000:1100]\n",
    "data_neg_test = pd.read_csv('dataset/crime_online_dataset.csv')\n",
    "data_neg_test['label'] = 0\n",
    "data_neg_test = data_neg_test[5000:5100]\n",
    "data_test = pd.concat([data_pos_test, data_neg_test], axis=0).reset_index(drop=True)\n",
    "data_test.drop(['url'], inplace=True, axis=1)\n",
    "data_test.drop(['sentiment'], inplace=True, axis=1)\n",
    "data_test = data_test[0:100]\n",
    "X_test = data_test.text.values\n",
    "y_test = data_test.label.values\n",
    "test_inputs, test_masks = preprocessing_for_bert(data_test.text)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-buddy",
   "metadata": {
    "id": "chubby-reservoir"
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-torture",
   "metadata": {
    "id": "powered-sperm"
   },
   "outputs": [],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-stress",
   "metadata": {
    "id": "oriental-payday"
   },
   "outputs": [],
   "source": [
    "data_pos_test = pd.read_csv('dataset/good_news_network_dataset.csv')\n",
    "data_pos_test['label'] = 1\n",
    "data_pos_test = data_pos_test[900:1000]\n",
    "data_neg_test = pd.read_csv('dataset/crime_online_dataset.csv')\n",
    "data_neg_test['label'] = 0\n",
    "data_neg_test = data_neg_test[6000:6100]\n",
    "data_test = pd.concat([data_pos_test, data_neg_test], axis=0).reset_index(drop=True)\n",
    "data_test.drop(['url'], inplace=True, axis=1)\n",
    "data_test.drop(['sentiment'], inplace=True, axis=1)\n",
    "data_test = data_test[0:100]\n",
    "X_test = data_test.text.values\n",
    "y_test = data_test.label.values\n",
    "test_inputs, test_masks = preprocessing_for_bert(data_test.text)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-kazakhstan",
   "metadata": {
    "id": "rental-ethernet"
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-journalist",
   "metadata": {
    "id": "silver-cosmetic"
   },
   "outputs": [],
   "source": [
    "preds = probs[:, 1]\n",
    "y_pred = np.where(preds >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-degree",
   "metadata": {
    "id": "fixed-expense"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-european",
   "metadata": {
    "id": "secret-drink"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-morning",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "damaged-mexican",
    "outputId": "e2a7d503-3204-4afe-be47-b44b4281ceaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |█▌                              | 10 kB 31.2 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 20 kB 23.8 MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 30 kB 17.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▏                         | 40 kB 15.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▊                        | 51 kB 7.9 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 61 kB 7.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 71 kB 8.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▍                   | 81 kB 9.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 92 kB 9.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 102 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 112 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 122 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 133 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▊          | 143 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 153 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 163 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 174 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 184 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▌  | 194 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 204 kB 7.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 211 kB 7.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2)\n",
      "Collecting tldextract>=2.0.1\n",
      "  Downloading tldextract-3.1.1-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 6.6 MB/s \n",
      "\u001b[?25hCollecting jieba3k>=0.35.1\n",
      "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 23.9 MB/s \n",
      "\u001b[?25hCollecting cssselect>=0.9.2\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 10.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (5.4.1)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.10.0->newspaper3k) (2.10)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
      "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
      "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13552 sha256=27c5282de63140c0bed0417583231abd67e3e55021eef8b017488ab9492955b2\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa\n",
      "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3356 sha256=3c559c940a5a0b598dc2c5cb1c17fa5479bcd8bcd7864e0fdba335cb2585d203\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346\n",
      "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398405 sha256=2d8caae95e8b79663e6f81b73cee404de32bd1f7eafeabd42117076543581850\n",
      "  Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=ac7f81bf4e7fe7e92acfea26468514f281f4b9137327361eba6760bd8ad6ae16\n",
      "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
      "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
      "Installing collected packages: sgmllib3k, requests-file, tldextract, tinysegmenter, jieba3k, feedparser, feedfinder2, cssselect, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.8 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-mission",
   "metadata": {
    "id": "guided-serve"
   },
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, val_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-eagle",
   "metadata": {
    "id": "electric-bosnia"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from newspaper import Config\n",
    "from newspaper import Article\n",
    "from newspaper.utils import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-experiment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "3OnDbU1kUfSQ",
    "outputId": "24371350-1da1-46f8-933c-fea05dec15e9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"WARNING: This story contains details some readers may find distressing.\\n\\nPrime Minister Justin Trudeau and Saskatchewan Premier Scott Moe are set to sign an agreement that will see Cowessess First Nation retake jurisdiction of child welfare, according to Chief Cadmus Delorme.\\n\\nThe agreement will be signed at the First Nation in a ceremony on Tuesday, Delorme said.\\n\\nCowessess has not had decision-making power over children in care since it was stripped of it in 1951, according to a letter distributed by Delorme on Monday.\\n\\nMore than 80 per cent of children in care in Saskatchewan are Indigenous, according to a 2018 children's advocate report.\\n\\nThat began to change with 2019's passage of An Act Respecting First Nations, Inuit and Métis Children, Youth and Families, aimed at reducing the number of youth in care and allowing communities to create their own child-welfare systems.\\n\\nCowessess did that in 2020, when it asserted its inherent rights over its children and families.\\n\\nDelorme's letter says the federal and provincial governments have agreed to help fund the program.\\n\\nDetails, including on the nature of the funding, were not immediately available.\\n\\nThe event will be held at the First Nation's Pow Wow Arbour at 2 p.m. CST on Tuesday. Attendance will be limited to 150 people.\\n\\nThe visit by Trudeau will also mark the first time the prime minister has visited the First Nation since Cowessess announced preliminary findings of 751 unmarked graves at the site of the former Marieval Residential School last month.\\n\\nTrudeau's daily schedule confirmed he and Moe would be at the First Nation for an announcement.\\n\\nSupport is available for anyone affected by their experience at residential schools, and those who are triggered by the latest reports.\\n\\nA national Indian Residential School Crisis Line has been set up to provide support for survivors and those affected. People can access emotional and crisis referral services by calling the 24-hour national crisis line: 1-866-925-4419.\""
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "# 目标新闻网址\n",
    "url = 'https://www.cbc.ca/news/canada/saskatchewan/child-welfare-cowessess-trudeau-moe-1.6090835'\n",
    "news = Article(url)\n",
    "news.download()        # 加载网页\n",
    "news.parse()           # 解析网页\n",
    "text = news.text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-bench",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "d8XzAS5bU7fQ",
    "outputId": "a5d58e80-b81e-4812-8fd0-6ddaef8ad358"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "url = 'https://abcnews.go.com/International/wireStory/uk-joint-airdrop-drill-affirms-uks-support-jordan-78518143'\n",
    "article = requests.get(url)\n",
    "soup = bs(article.content, \"html.parser\")\n",
    "text = ''\n",
    "for EachPart in soup.select('div[class*=\"RichTextContainer\"]'):\n",
    "  for p in EachPart.find_all(\"p\"):\n",
    "    text = text+' '+p.text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-bolivia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fEsVsLOLCf4v",
    "outputId": "d54ac2e4-c6ea-450e-b798-fa63810b03e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "drive.mount('/content/drive')\n",
    "sys.path.append('/content/drive/My Drive/mycolab/ob')\n",
    "with open('/content/drive/My Drive/mycolab/ob/lr_model1.pickle', 'rb') as fp:\n",
    "  bert_model = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-dressing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "2CqQTN_CAv7U",
    "outputId": "db692984-f4ca-4f1d-e1e1-38ae38ddc0c8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' For much of the past two centuries, it was illegal to be gay in a vast swathe of the world - thanks to colonial Britain. Till today, colonial-era laws that ban homosexuality continue to exist in former British territories including parts of Africa and Oceania. But it is in Asia where they have had a significantly widespread impact. This is the region where, before India legalised homosexual sex in 2018, at least one billion people lived with anti-LGBTQ legislation. It can be traced back to one particular law first conceptualised in India, and one man\\'s mission to \"modernise\" the colony. Currently, it is illegal to be gay in around 69 countries, nearly two-thirds of which were under some form of British control at one point of time. This is no coincidence, according to Enze Han and Joseph O\\'Mahoney, who wrote the book British Colonialism and the Criminalization of Homosexuality. Dr Han told the BBC that British rulers introduced such laws because of a \"Victorian, Christian puritanical concept of sex\".  \"They wanted to protect innocent British soldiers from the \\'exotic, mystical Orient\\' - there was this very orientalised view of Asia and the Middle East that they were overly erotic.\" \"They thought if there were no regulations, the soldiers would be easily led astray.\" While there were several criminal codes used across British colonies around the world, in Asia one particular set of laws was used prominently - the Indian Penal Code (IPC) drawn up by British historian Lord Thomas Babington Macaulay, which came into force in 1862. It contained section 377, which stated that \"whoever voluntarily has carnal intercourse against the order of nature with any man, woman or animal\" would be punished with imprisonment or fines.  Lord Macaulay, who modelled the section on Britain\\'s 16th Century Buggery Act, believed the IPC was a \"blessing\" for India as it would \"modernise\" its society, according to Dr Han and Dr O\\'Mahoney\\'s book.  The British went on to use the IPC as the basis for criminal law codes in many other territories they controlled.  Till today, 377 continues to exist in various forms in several former colonies in Asia such as Pakistan, Singapore, Bangladesh, Malaysia, Brunei, Myanmar and Sri Lanka.  Penalties range from two to 20 years in prison. In places with Muslim-majority populations which also have sharia law, LGBT persons can also face more severe punishment such as flogging. Activists say these laws have left a damaging legacy on these countries, some of which have long had flexible attitudes towards LGBTQ people.  Transgenderism, intersex identity and the third gender, for example, have traditionally been a part of South Asian culture with the hijra or eunuch communities. In India, where for centuries LGBTQ relationships were featured in literature, myths and Hindu temple art, present-day attitudes now largely skew conservative. \"It\\'s in our traditions. But now we are getting so embarrassed about [LGBTQ relations]. Clearly the change happened because of certain influences,\" says Anjali Gopalan, executive director of Naz Foundation India, a non-governmental organisation which offers counselling services for the LGBTQ community. One common argument governments have made for keeping the law is that it continues to reflect the conservative stance of their societies. Some, like India, have even ironically argued that it keeps out \"Western influence\". But activists point out that this perpetuates discrimination and goes against some countries\\' constitutions which promise equal rights to all citizens. This has a \"de-humanising effect\" on an LGBTQ person, and can seriously impact their access to education and career opportunities as well as increase their risk of poverty and physical violence, said Jessica Stern, executive director of LGBTQ rights group OutRight International. \"If you\\'re a walking criminal, you\\'re living with a burden every day. Whether you internalise it or not, it affects you and everyone who loves you,\" she told the BBC. The Covid pandemic has exacerbated these problems, she added. One recent example her group found was in Sri Lanka, where the police were tasked to distribute emergency rations while the country was under curfew - but some in the LGBTQ community were too afraid to come forward due to the country\\'s anti-sodomy law.  \"People said they have to risk arrest or risk going hungry… it\\'s a stark life or death choice they have to make,\" said Ms Stern. Some governments, like Singapore, have tried to tread the middle ground by publicly promising never to enforce the law. But the LGBTQ community in the city-state say this is unfair as they live knowing the government could change its mind at any time. Olivia and Irene Chiong left Singapore five years ago for the US, where they got married and are both legally recognised as the mothers of their two daughters - something that would not be possible back home.  The lack of rights is one reason they find it difficult to return, as well as the refusal among some Singaporeans including government ministers to acknowledge that there is discrimination. \"I think for me the biggest frustration comes from the fact that Singaporeans think everything\\'s okay - that as long as gay people keep quiet... keep themselves in the closet, it\\'s fine!\" said Olivia. \"There are many rainbow families in Singapore…You can\\'t just keep sweeping things under the carpet. \"The only reason why Singapore is holding so tightly to (377) is because it gives them the illusion of control,\" she said.  There has been progress - most notably, of course, with the Indian Supreme Court\\'s decision in 2018 to repeal 377, following years of legal challenges mounted by determined activists.  It was a historic decision and a major step forward for LGBT rights in India. But three years on, there is still a very long way to go in changing cultural attitudes, activists say. \"The most common thing we still see in counselling is families wanting their gay sons to get married (to a woman),\" said Ms Gopalan.  \"Everything is linked to the family in India, and marriage is a very big part of our lives. So the first issue is acceptance from the family and then by extension, society.\" Activists say more protection is needed, such as anti-discrimination laws. Earlier this month, a court in Chennai ordered officials to draw up plans for reforms to respect LGBTQ rights. Still, India\\'s repeal of 377 has helped to lessen the stigma - and inspired other countries.  In Singapore and Kenya, activists have used the repeal in legal arguments against their own colonial anti-homosexuality laws. Two centuries after it was used by the British as a legal blueprint, India once again is seen as an example to follow - this time to strike down that very law that was exported across Asia. \"It has emboldened others in Asia, unequivocally... it sent a message to all former colonial outposts,\" said Ms Stern. \"Activists I spoke to have said that if it can happen in India, it can happen here too.\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.bbc.co.uk/news/world-asia-57606847'\n",
    "article = requests.get(url)\n",
    "soup = bs(article.content, \"html.parser\")\n",
    "text = ''\n",
    "for EachPart in soup.select('div[class*=\"RichTextContainer\"]'):\n",
    "  for p in EachPart.find_all(\"p\"):\n",
    "    text = text+' '+p.text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-brief",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CJwu7VKRBTxB",
    "outputId": "03d8c608-53df-4f95-f57c-5f0a22fecc28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9979831"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "data.append(text)\n",
    "a= np.array(data)\n",
    "data_to_test = pd.Series(a)\n",
    "test_inputs, test_masks = preprocessing_for_bert(data_to_test)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=2)\n",
    "prob = bert_predict(bert_model, test_dataloader)\n",
    "prob[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-transaction",
   "metadata": {
    "id": "QS1N_OHjLENx"
   },
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    news = Article(url)\n",
    "    news.download()\n",
    "    news.parse()\n",
    "    title = news.title\n",
    "    if url.startswith('https://www.bbc'):\n",
    "        article = requests.get(url)\n",
    "        soup = bs(article.content, \"html.parser\")\n",
    "        text = ''\n",
    "        for EachPart in soup.select('div[class*=\"RichTextContainer\"]'):\n",
    "            for p in EachPart.find_all(\"p\"):\n",
    "                text = text+' '+p.text\n",
    "        if len(text)==0:\n",
    "            text=news.text\n",
    "    else :\n",
    "        text = news.text\n",
    "    text = title+text\n",
    "    return text\n",
    "\n",
    "def predict(url):\n",
    "  text=get_text(url)\n",
    "  data=[]\n",
    "  data.append(text)\n",
    "  a= np.array(data)\n",
    "  data_to_test = pd.Series(a)\n",
    "  test_inputs, test_masks = preprocessing_for_bert(data_to_test)\n",
    "  test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "  test_sampler = SequentialSampler(test_dataset)\n",
    "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=2)\n",
    "  prob = bert_predict(bert_model, test_dataloader)\n",
    "  return prob[0,1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-assumption",
   "metadata": {
    "id": "sBxza0NImvIl"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"/content/drive/My Drive/mycolab/ob/testset.csv\", \"r\", encoding = \"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    column = [row[2] for row in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-sequence",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jn4nKxEqBlrI",
    "outputId": "d8fab135-b6e0-4e48-d71f-121d38032886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "drive.mount('/content/drive')\n",
    "sys.path.append('/content/drive/My Drive/mycolab/ob')\n",
    "with open('/content/drive/My Drive/mycolab/ob/bert_model1.pickle', 'rb') as fp:\n",
    "  bert_model = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-buying",
   "metadata": {
    "id": "QbirE2H1HsLb"
   },
   "outputs": [],
   "source": [
    "def predicttext(text):\n",
    "  data=[]\n",
    "  data.append(text)\n",
    "  a= np.array(data)\n",
    "  data_to_test = pd.Series(a)\n",
    "  test_inputs, test_masks = preprocessing_for_bert(data_to_test)\n",
    "  test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "  test_sampler = SequentialSampler(test_dataset)\n",
    "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=2)\n",
    "  prob = bert_predict(bert_model, test_dataloader)\n",
    "  return prob[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-novelty",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p07NiKZoyJ_7",
    "outputId": "734a6cdc-c170-4102-cb60-53eee0a00f0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43m流式输出内容被截断，只能显示最后 5000 行内容。\u001b[0m\n",
      "1474\n",
      "1475\n",
      "1476\n",
      "1477\n",
      "1478\n",
      "1479\n",
      "1480\n",
      "1481\n",
      "1482\n",
      "1483\n",
      "1484\n",
      "1485\n",
      "1486\n",
      "1487\n",
      "1488\n",
      "1489\n",
      "1490\n",
      "1491\n",
      "1492\n",
      "1493\n",
      "1494\n",
      "1495\n",
      "1496\n",
      "1497\n",
      "1498\n",
      "1499\n",
      "1500\n",
      "1501\n",
      "1502\n",
      "1503\n",
      "1504\n",
      "1505\n",
      "1506\n",
      "1507\n",
      "1508\n",
      "1509\n",
      "1510\n",
      "1511\n",
      "1512\n",
      "1513\n",
      "1514\n",
      "1515\n",
      "1516\n",
      "1517\n",
      "1518\n",
      "1519\n",
      "1520\n",
      "1521\n",
      "1522\n",
      "1523\n",
      "1524\n",
      "1525\n",
      "1526\n",
      "1527\n",
      "1528\n",
      "1529\n",
      "1530\n",
      "1531\n",
      "1532\n",
      "1533\n",
      "1534\n",
      "1535\n",
      "1536\n",
      "1537\n",
      "1538\n",
      "1539\n",
      "1540\n",
      "1541\n",
      "1542\n",
      "1543\n",
      "1544\n",
      "1545\n",
      "1546\n",
      "1547\n",
      "1548\n",
      "1549\n",
      "1550\n",
      "1551\n",
      "1552\n",
      "1553\n",
      "1554\n",
      "1555\n",
      "1556\n",
      "1557\n",
      "1558\n",
      "1559\n",
      "1560\n",
      "1561\n",
      "1562\n",
      "1563\n",
      "1564\n",
      "1565\n",
      "1566\n",
      "1567\n",
      "1568\n",
      "1569\n",
      "1570\n",
      "1571\n",
      "1572\n",
      "1573\n",
      "1574\n",
      "1575\n",
      "1576\n",
      "1577\n",
      "1578\n",
      "1579\n",
      "1580\n",
      "1581\n",
      "1582\n",
      "1583\n",
      "1584\n",
      "1585\n",
      "1586\n",
      "1587\n",
      "1588\n",
      "1589\n",
      "1590\n",
      "1591\n",
      "1592\n",
      "1593\n",
      "1594\n",
      "1595\n",
      "1596\n",
      "1597\n",
      "1598\n",
      "1599\n",
      "1600\n",
      "1601\n",
      "1602\n",
      "1603\n",
      "1604\n",
      "1605\n",
      "1606\n",
      "1607\n",
      "1608\n",
      "1609\n",
      "1610\n",
      "1611\n",
      "1612\n",
      "1613\n",
      "1614\n",
      "1615\n",
      "1616\n",
      "1617\n",
      "1618\n",
      "1619\n",
      "1620\n",
      "1621\n",
      "1622\n",
      "1623\n",
      "1624\n",
      "1625\n",
      "1626\n",
      "1627\n",
      "1628\n",
      "1629\n",
      "1630\n",
      "1631\n",
      "1632\n",
      "1633\n",
      "1634\n",
      "1635\n",
      "1636\n",
      "1637\n",
      "1638\n",
      "1639\n",
      "1640\n",
      "1641\n",
      "1642\n",
      "1643\n",
      "1644\n",
      "1645\n",
      "1646\n",
      "1647\n",
      "1648\n",
      "1649\n",
      "1650\n",
      "1651\n",
      "1652\n",
      "1653\n",
      "1654\n",
      "1655\n",
      "1656\n",
      "1657\n",
      "1658\n",
      "1659\n",
      "1660\n",
      "1661\n",
      "1662\n",
      "1663\n",
      "1664\n",
      "1665\n",
      "1666\n",
      "1667\n",
      "1668\n",
      "1669\n",
      "1670\n",
      "1671\n",
      "1672\n",
      "1673\n",
      "1674\n",
      "1675\n",
      "1676\n",
      "1677\n",
      "1678\n",
      "1679\n",
      "1680\n",
      "1681\n",
      "1682\n",
      "1683\n",
      "1684\n",
      "1685\n",
      "1686\n",
      "1687\n",
      "1688\n",
      "1689\n",
      "1690\n",
      "1691\n",
      "1692\n",
      "1693\n",
      "1694\n",
      "1695\n",
      "1696\n",
      "1697\n",
      "1698\n",
      "1699\n",
      "1700\n",
      "1701\n",
      "1702\n",
      "1703\n",
      "1704\n",
      "1705\n",
      "1706\n",
      "1707\n",
      "1708\n",
      "1709\n",
      "1710\n",
      "1711\n",
      "1712\n",
      "1713\n",
      "1714\n",
      "1715\n",
      "1716\n",
      "1717\n",
      "1718\n",
      "1719\n",
      "1720\n",
      "1721\n",
      "1722\n",
      "1723\n",
      "1724\n",
      "1725\n",
      "1726\n",
      "1727\n",
      "1728\n",
      "1729\n",
      "1730\n",
      "1731\n",
      "1732\n",
      "1733\n",
      "1734\n",
      "1735\n",
      "1736\n",
      "1737\n",
      "1738\n",
      "1739\n",
      "1740\n",
      "1741\n",
      "1742\n",
      "1743\n",
      "1744\n",
      "1745\n",
      "1746\n",
      "1747\n",
      "1748\n",
      "1749\n",
      "1750\n",
      "1751\n",
      "1752\n",
      "1753\n",
      "1754\n",
      "1755\n",
      "1756\n",
      "1757\n",
      "1758\n",
      "1759\n",
      "1760\n",
      "1761\n",
      "1762\n",
      "1763\n",
      "1764\n",
      "1765\n",
      "1766\n",
      "1767\n",
      "1768\n",
      "1769\n",
      "1770\n",
      "1771\n",
      "1772\n",
      "1773\n",
      "1774\n",
      "1775\n",
      "1776\n",
      "1777\n",
      "1778\n",
      "1779\n",
      "1780\n",
      "1781\n",
      "1782\n",
      "1783\n",
      "1784\n",
      "1785\n",
      "1786\n",
      "1787\n",
      "1788\n",
      "1789\n",
      "1790\n",
      "1791\n",
      "1792\n",
      "1793\n",
      "1794\n",
      "1795\n",
      "1796\n",
      "1797\n",
      "1798\n",
      "1799\n",
      "1800\n",
      "1801\n",
      "1802\n",
      "1803\n",
      "1804\n",
      "1805\n",
      "1806\n",
      "1807\n",
      "1808\n",
      "1809\n",
      "1810\n",
      "1811\n",
      "1812\n",
      "1813\n",
      "1814\n",
      "1815\n",
      "1816\n",
      "1817\n",
      "1818\n",
      "1819\n",
      "1820\n",
      "1821\n",
      "1822\n",
      "1823\n",
      "1824\n",
      "1825\n",
      "1826\n",
      "1827\n",
      "1828\n",
      "1829\n",
      "1830\n",
      "1831\n",
      "1832\n",
      "1833\n",
      "1834\n",
      "1835\n",
      "1836\n",
      "1837\n",
      "1838\n",
      "1839\n",
      "1840\n",
      "1841\n",
      "1842\n",
      "1843\n",
      "1844\n",
      "1845\n",
      "1846\n",
      "1847\n",
      "1848\n",
      "1849\n",
      "1850\n",
      "1851\n",
      "1852\n",
      "1853\n",
      "1854\n",
      "1855\n",
      "1856\n",
      "1857\n",
      "1858\n",
      "1859\n",
      "1860\n",
      "1861\n",
      "1862\n",
      "1863\n",
      "1864\n",
      "1865\n",
      "1866\n",
      "1867\n",
      "1868\n",
      "1869\n",
      "1870\n",
      "1871\n",
      "1872\n",
      "1873\n",
      "1874\n",
      "1875\n",
      "1876\n",
      "1877\n",
      "1878\n",
      "1879\n",
      "1880\n",
      "1881\n",
      "1882\n",
      "1883\n",
      "1884\n",
      "1885\n",
      "1886\n",
      "1887\n",
      "1888\n",
      "1889\n",
      "1890\n",
      "1891\n",
      "1892\n",
      "1893\n",
      "1894\n",
      "1895\n",
      "1896\n",
      "1897\n",
      "1898\n",
      "1899\n",
      "1900\n",
      "1901\n",
      "1902\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1909\n",
      "1910\n",
      "1911\n",
      "1912\n",
      "1913\n",
      "1914\n",
      "1915\n",
      "1916\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1924\n",
      "1925\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n",
      "1985\n",
      "1986\n",
      "1987\n",
      "1988\n",
      "1989\n",
      "1990\n",
      "1991\n",
      "1992\n",
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n",
      "2101\n",
      "2102\n",
      "2103\n",
      "2104\n",
      "2105\n",
      "2106\n",
      "2107\n",
      "2108\n",
      "2109\n",
      "2110\n",
      "2111\n",
      "2112\n",
      "2113\n",
      "2114\n",
      "2115\n",
      "2116\n",
      "2117\n",
      "2118\n",
      "2119\n",
      "2120\n",
      "2121\n",
      "2122\n",
      "2123\n",
      "2124\n",
      "2125\n",
      "2126\n",
      "2127\n",
      "2128\n",
      "2129\n",
      "2130\n",
      "2131\n",
      "2132\n",
      "2133\n",
      "2134\n",
      "2135\n",
      "2136\n",
      "2137\n",
      "2138\n",
      "2139\n",
      "2140\n",
      "2141\n",
      "2142\n",
      "2143\n",
      "2144\n",
      "2145\n",
      "2146\n",
      "2147\n",
      "2148\n",
      "2149\n",
      "2150\n",
      "2151\n",
      "2152\n",
      "2153\n",
      "2154\n",
      "2155\n",
      "2156\n",
      "2157\n",
      "2158\n",
      "2159\n",
      "2160\n",
      "2161\n",
      "2162\n",
      "2163\n",
      "2164\n",
      "2165\n",
      "2166\n",
      "2167\n",
      "2168\n",
      "2169\n",
      "2170\n",
      "2171\n",
      "2172\n",
      "2173\n",
      "2174\n",
      "2175\n",
      "2176\n",
      "2177\n",
      "2178\n",
      "2179\n",
      "2180\n",
      "2181\n",
      "2182\n",
      "2183\n",
      "2184\n",
      "2185\n",
      "2186\n",
      "2187\n",
      "2188\n",
      "2189\n",
      "2190\n",
      "2191\n",
      "2192\n",
      "2193\n",
      "2194\n",
      "2195\n",
      "2196\n",
      "2197\n",
      "2198\n",
      "2199\n",
      "2200\n",
      "2201\n",
      "2202\n",
      "2203\n",
      "2204\n",
      "2205\n",
      "2206\n",
      "2207\n",
      "2208\n",
      "2209\n",
      "2210\n",
      "2211\n",
      "2212\n",
      "2213\n",
      "2214\n",
      "2215\n",
      "2216\n",
      "2217\n",
      "2218\n",
      "2219\n",
      "2220\n",
      "2221\n",
      "2222\n",
      "2223\n",
      "2224\n",
      "2225\n",
      "2226\n",
      "2227\n",
      "2228\n",
      "2229\n",
      "2230\n",
      "2231\n",
      "2232\n",
      "2233\n",
      "2234\n",
      "2235\n",
      "2236\n",
      "2237\n",
      "2238\n",
      "2239\n",
      "2240\n",
      "2241\n",
      "2242\n",
      "2243\n",
      "2244\n",
      "2245\n",
      "2246\n",
      "2247\n",
      "2248\n",
      "2249\n",
      "2250\n",
      "2251\n",
      "2252\n",
      "2253\n",
      "2254\n",
      "2255\n",
      "2256\n",
      "2257\n",
      "2258\n",
      "2259\n",
      "2260\n",
      "2261\n",
      "2262\n",
      "2263\n",
      "2264\n",
      "2265\n",
      "2266\n",
      "2267\n",
      "2268\n",
      "2269\n",
      "2270\n",
      "2271\n",
      "2272\n",
      "2273\n",
      "2274\n",
      "2275\n",
      "2276\n",
      "2277\n",
      "2278\n",
      "2279\n",
      "2280\n",
      "2281\n",
      "2282\n",
      "2283\n",
      "2284\n",
      "2285\n",
      "2286\n",
      "2287\n",
      "2288\n",
      "2289\n",
      "2290\n",
      "2291\n",
      "2292\n",
      "2293\n",
      "2294\n",
      "2295\n",
      "2296\n",
      "2297\n",
      "2298\n",
      "2299\n",
      "2300\n",
      "2301\n",
      "2302\n",
      "2303\n",
      "2304\n",
      "2305\n",
      "2306\n",
      "2307\n",
      "2308\n",
      "2309\n",
      "2310\n",
      "2311\n",
      "2312\n",
      "2313\n",
      "2314\n",
      "2315\n",
      "2316\n",
      "2317\n",
      "2318\n",
      "2319\n",
      "2320\n",
      "2321\n",
      "2322\n",
      "2323\n",
      "2324\n",
      "2325\n",
      "2326\n",
      "2327\n",
      "2328\n",
      "2329\n",
      "2330\n",
      "2331\n",
      "2332\n",
      "2333\n",
      "2334\n",
      "2335\n",
      "2336\n",
      "2337\n",
      "2338\n",
      "2339\n",
      "2340\n",
      "2341\n",
      "2342\n",
      "2343\n",
      "2344\n",
      "2345\n",
      "2346\n",
      "2347\n",
      "2348\n",
      "2349\n",
      "2350\n",
      "2351\n",
      "2352\n",
      "2353\n",
      "2354\n",
      "2355\n",
      "2356\n",
      "2357\n",
      "2358\n",
      "2359\n",
      "2360\n",
      "2361\n",
      "2362\n",
      "2363\n",
      "2364\n",
      "2365\n",
      "2366\n",
      "2367\n",
      "2368\n",
      "2369\n",
      "2370\n",
      "2371\n",
      "2372\n",
      "2373\n",
      "2374\n",
      "2375\n",
      "2376\n",
      "2377\n",
      "2378\n",
      "2379\n",
      "2380\n",
      "2381\n",
      "2382\n",
      "2383\n",
      "2384\n",
      "2385\n",
      "2386\n",
      "2387\n",
      "2388\n",
      "2389\n",
      "2390\n",
      "2391\n",
      "2392\n",
      "2393\n",
      "2394\n",
      "2395\n",
      "2396\n",
      "2397\n",
      "2398\n",
      "2399\n",
      "2400\n",
      "2401\n",
      "2402\n",
      "2403\n",
      "2404\n",
      "2405\n",
      "2406\n",
      "2407\n",
      "2408\n",
      "2409\n",
      "2410\n",
      "2411\n",
      "2412\n",
      "2413\n",
      "2414\n",
      "2415\n",
      "2416\n",
      "2417\n",
      "2418\n",
      "2419\n",
      "2420\n",
      "2421\n",
      "2422\n",
      "2423\n",
      "2424\n",
      "2425\n",
      "2426\n",
      "2427\n",
      "2428\n",
      "2429\n",
      "2430\n",
      "2431\n",
      "2432\n",
      "2433\n",
      "2434\n",
      "2435\n",
      "2436\n",
      "2437\n",
      "2438\n",
      "2439\n",
      "2440\n",
      "2441\n",
      "2442\n",
      "2443\n",
      "2444\n",
      "2445\n",
      "2446\n",
      "2447\n",
      "2448\n",
      "2449\n",
      "2450\n",
      "2451\n",
      "2452\n",
      "2453\n",
      "2454\n",
      "2455\n",
      "2456\n",
      "2457\n",
      "2458\n",
      "2459\n",
      "2460\n",
      "2461\n",
      "2462\n",
      "2463\n",
      "2464\n",
      "2465\n",
      "2466\n",
      "2467\n",
      "2468\n",
      "2469\n",
      "2470\n",
      "2471\n",
      "2472\n",
      "2473\n",
      "2474\n",
      "2475\n",
      "2476\n",
      "2477\n",
      "2478\n",
      "2479\n",
      "2480\n",
      "2481\n",
      "2482\n",
      "2483\n",
      "2484\n",
      "2485\n",
      "2486\n",
      "2487\n",
      "2488\n",
      "2489\n",
      "2490\n",
      "2491\n",
      "2492\n",
      "2493\n",
      "2494\n",
      "2495\n",
      "2496\n",
      "2497\n",
      "2498\n",
      "2499\n",
      "2500\n",
      "2501\n",
      "2502\n",
      "2503\n",
      "2504\n",
      "2505\n",
      "2506\n",
      "2507\n",
      "2508\n",
      "2509\n",
      "2510\n",
      "2511\n",
      "2512\n",
      "2513\n",
      "2514\n",
      "2515\n",
      "2516\n",
      "2517\n",
      "2518\n",
      "2519\n",
      "2520\n",
      "2521\n",
      "2522\n",
      "2523\n",
      "2524\n",
      "2525\n",
      "2526\n",
      "2527\n",
      "2528\n",
      "2529\n",
      "2530\n",
      "2531\n",
      "2532\n",
      "2533\n",
      "2534\n",
      "2535\n",
      "2536\n",
      "2537\n",
      "2538\n",
      "2539\n",
      "2540\n",
      "2541\n",
      "2542\n",
      "2543\n",
      "2544\n",
      "2545\n",
      "2546\n",
      "2547\n",
      "2548\n",
      "2549\n",
      "2550\n",
      "2551\n",
      "2552\n",
      "2553\n",
      "2554\n",
      "2555\n",
      "2556\n",
      "2557\n",
      "2558\n",
      "2559\n",
      "2560\n",
      "2561\n",
      "2562\n",
      "2563\n",
      "2564\n",
      "2565\n",
      "2566\n",
      "2567\n",
      "2568\n",
      "2569\n",
      "2570\n",
      "2571\n",
      "2572\n",
      "2573\n",
      "2574\n",
      "2575\n",
      "2576\n",
      "2577\n",
      "2578\n",
      "2579\n",
      "2580\n",
      "2581\n",
      "2582\n",
      "2583\n",
      "2584\n",
      "2585\n",
      "2586\n",
      "2587\n",
      "2588\n",
      "2589\n",
      "2590\n",
      "2591\n",
      "2592\n",
      "2593\n",
      "2594\n",
      "2595\n",
      "2596\n",
      "2597\n",
      "2598\n",
      "2599\n",
      "2600\n",
      "2601\n",
      "2602\n",
      "2603\n",
      "2604\n",
      "2605\n",
      "2606\n",
      "2607\n",
      "2608\n",
      "2609\n",
      "2610\n",
      "2611\n",
      "2612\n",
      "2613\n",
      "2614\n",
      "2615\n",
      "2616\n",
      "2617\n",
      "2618\n",
      "2619\n",
      "2620\n",
      "2621\n",
      "2622\n",
      "2623\n",
      "2624\n",
      "2625\n",
      "2626\n",
      "2627\n",
      "2628\n",
      "2629\n",
      "2630\n",
      "2631\n",
      "2632\n",
      "2633\n",
      "2634\n",
      "2635\n",
      "2636\n",
      "2637\n",
      "2638\n",
      "2639\n",
      "2640\n",
      "2641\n",
      "2642\n",
      "2643\n",
      "2644\n",
      "2645\n",
      "2646\n",
      "2647\n",
      "2648\n",
      "2649\n",
      "2650\n",
      "2651\n",
      "2652\n",
      "2653\n",
      "2654\n",
      "2655\n",
      "2656\n",
      "2657\n",
      "2658\n",
      "2659\n",
      "2660\n",
      "2661\n",
      "2662\n",
      "2663\n",
      "2664\n",
      "2665\n",
      "2666\n",
      "2667\n",
      "2668\n",
      "2669\n",
      "2670\n",
      "2671\n",
      "2672\n",
      "2673\n",
      "2674\n",
      "2675\n",
      "2676\n",
      "2677\n",
      "2678\n",
      "2679\n",
      "2680\n",
      "2681\n",
      "2682\n",
      "2683\n",
      "2684\n",
      "2685\n",
      "2686\n",
      "2687\n",
      "2688\n",
      "2689\n",
      "2690\n",
      "2691\n",
      "2692\n",
      "2693\n",
      "2694\n",
      "2695\n",
      "2696\n",
      "2697\n",
      "2698\n",
      "2699\n",
      "2700\n",
      "2701\n",
      "2702\n",
      "2703\n",
      "2704\n",
      "2705\n",
      "2706\n",
      "2707\n",
      "2708\n",
      "2709\n",
      "2710\n",
      "2711\n",
      "2712\n",
      "2713\n",
      "2714\n",
      "2715\n",
      "2716\n",
      "2717\n",
      "2718\n",
      "2719\n",
      "2720\n",
      "2721\n",
      "2722\n",
      "2723\n",
      "2724\n",
      "2725\n",
      "2726\n",
      "2727\n",
      "2728\n",
      "2729\n",
      "2730\n",
      "2731\n",
      "2732\n",
      "2733\n",
      "2734\n",
      "2735\n",
      "2736\n",
      "2737\n",
      "2738\n",
      "2739\n",
      "2740\n",
      "2741\n",
      "2742\n",
      "2743\n",
      "2744\n",
      "2745\n",
      "2746\n",
      "2747\n",
      "2748\n",
      "2749\n",
      "2750\n",
      "2751\n",
      "2752\n",
      "2753\n",
      "2754\n",
      "2755\n",
      "2756\n",
      "2757\n",
      "2758\n",
      "2759\n",
      "2760\n",
      "2761\n",
      "2762\n",
      "2763\n",
      "2764\n",
      "2765\n",
      "2766\n",
      "2767\n",
      "2768\n",
      "2769\n",
      "2770\n",
      "2771\n",
      "2772\n",
      "2773\n",
      "2774\n",
      "2775\n",
      "2776\n",
      "2777\n",
      "2778\n",
      "2779\n",
      "2780\n",
      "2781\n",
      "2782\n",
      "2783\n",
      "2784\n",
      "2785\n",
      "2786\n",
      "2787\n",
      "2788\n",
      "2789\n",
      "2790\n",
      "2791\n",
      "2792\n",
      "2793\n",
      "2794\n",
      "2795\n",
      "2796\n",
      "2797\n",
      "2798\n",
      "2799\n",
      "2800\n",
      "2801\n",
      "2802\n",
      "2803\n",
      "2804\n",
      "2805\n",
      "2806\n",
      "2807\n",
      "2808\n",
      "2809\n",
      "2810\n",
      "2811\n",
      "2812\n",
      "2813\n",
      "2814\n",
      "2815\n",
      "2816\n",
      "2817\n",
      "2818\n",
      "2819\n",
      "2820\n",
      "2821\n",
      "2822\n",
      "2823\n",
      "2824\n",
      "2825\n",
      "2826\n",
      "2827\n",
      "2828\n",
      "2829\n",
      "2830\n",
      "2831\n",
      "2832\n",
      "2833\n",
      "2834\n",
      "2835\n",
      "2836\n",
      "2837\n",
      "2838\n",
      "2839\n",
      "2840\n",
      "2841\n",
      "2842\n",
      "2843\n",
      "2844\n",
      "2845\n",
      "2846\n",
      "2847\n",
      "2848\n",
      "2849\n",
      "2850\n",
      "2851\n",
      "2852\n",
      "2853\n",
      "2854\n",
      "2855\n",
      "2856\n",
      "2857\n",
      "2858\n",
      "2859\n",
      "2860\n",
      "2861\n",
      "2862\n",
      "2863\n",
      "2864\n",
      "2865\n",
      "2866\n",
      "2867\n",
      "2868\n",
      "2869\n",
      "2870\n",
      "2871\n",
      "2872\n",
      "2873\n",
      "2874\n",
      "2875\n",
      "2876\n",
      "2877\n",
      "2878\n",
      "2879\n",
      "2880\n",
      "2881\n",
      "2882\n",
      "2883\n",
      "2884\n",
      "2885\n",
      "2886\n",
      "2887\n",
      "2888\n",
      "2889\n",
      "2890\n",
      "2891\n",
      "2892\n",
      "2893\n",
      "2894\n",
      "2895\n",
      "2896\n",
      "2897\n",
      "2898\n",
      "2899\n",
      "2900\n",
      "2901\n",
      "2902\n",
      "2903\n",
      "2904\n",
      "2905\n",
      "2906\n",
      "2907\n",
      "2908\n",
      "2909\n",
      "2910\n",
      "2911\n",
      "2912\n",
      "2913\n",
      "2914\n",
      "2915\n",
      "2916\n",
      "2917\n",
      "2918\n",
      "2919\n",
      "2920\n",
      "2921\n",
      "2922\n",
      "2923\n",
      "2924\n",
      "2925\n",
      "2926\n",
      "2927\n",
      "2928\n",
      "2929\n",
      "2930\n",
      "2931\n",
      "2932\n",
      "2933\n",
      "2934\n",
      "2935\n",
      "2936\n",
      "2937\n",
      "2938\n",
      "2939\n",
      "2940\n",
      "2941\n",
      "2942\n",
      "2943\n",
      "2944\n",
      "2945\n",
      "2946\n",
      "2947\n",
      "2948\n",
      "2949\n",
      "2950\n",
      "2951\n",
      "2952\n",
      "2953\n",
      "2954\n",
      "2955\n",
      "2956\n",
      "2957\n",
      "2958\n",
      "2959\n",
      "2960\n",
      "2961\n",
      "2962\n",
      "2963\n",
      "2964\n",
      "2965\n",
      "2966\n",
      "2967\n",
      "2968\n",
      "2969\n",
      "2970\n",
      "2971\n",
      "2972\n",
      "2973\n",
      "2974\n",
      "2975\n",
      "2976\n",
      "2977\n",
      "2978\n",
      "2979\n",
      "2980\n",
      "2981\n",
      "2982\n",
      "2983\n",
      "2984\n",
      "2985\n",
      "2986\n",
      "2987\n",
      "2988\n",
      "2989\n",
      "2990\n",
      "2991\n",
      "2992\n",
      "2993\n",
      "2994\n",
      "2995\n",
      "2996\n",
      "2997\n",
      "2998\n",
      "2999\n",
      "3000\n",
      "3001\n",
      "3002\n",
      "3003\n",
      "3004\n",
      "3005\n",
      "3006\n",
      "3007\n",
      "3008\n",
      "3009\n",
      "3010\n",
      "3011\n",
      "3012\n",
      "3013\n",
      "3014\n",
      "3015\n",
      "3016\n",
      "3017\n",
      "3018\n",
      "3019\n",
      "3020\n",
      "3021\n",
      "3022\n",
      "3023\n",
      "3024\n",
      "3025\n",
      "3026\n",
      "3027\n",
      "3028\n",
      "3029\n",
      "3030\n",
      "3031\n",
      "3032\n",
      "3033\n",
      "3034\n",
      "3035\n",
      "3036\n",
      "3037\n",
      "3038\n",
      "3039\n",
      "3040\n",
      "3041\n",
      "3042\n",
      "3043\n",
      "3044\n",
      "3045\n",
      "3046\n",
      "3047\n",
      "3048\n",
      "3049\n",
      "3050\n",
      "3051\n",
      "3052\n",
      "3053\n",
      "3054\n",
      "3055\n",
      "3056\n",
      "3057\n",
      "3058\n",
      "3059\n",
      "3060\n",
      "3061\n",
      "3062\n",
      "3063\n",
      "3064\n",
      "3065\n",
      "3066\n",
      "3067\n",
      "3068\n",
      "3069\n",
      "3070\n",
      "3071\n",
      "3072\n",
      "3073\n",
      "3074\n",
      "3075\n",
      "3076\n",
      "3077\n",
      "3078\n",
      "3079\n",
      "3080\n",
      "3081\n",
      "3082\n",
      "3083\n",
      "3084\n",
      "3085\n",
      "3086\n",
      "3087\n",
      "3088\n",
      "3089\n",
      "3090\n",
      "3091\n",
      "3092\n",
      "3093\n",
      "3094\n",
      "3095\n",
      "3096\n",
      "3097\n",
      "3098\n",
      "3099\n",
      "3100\n",
      "3101\n",
      "3102\n",
      "3103\n",
      "3104\n",
      "3105\n",
      "3106\n",
      "3107\n",
      "3108\n",
      "3109\n",
      "3110\n",
      "3111\n",
      "3112\n",
      "3113\n",
      "3114\n",
      "3115\n",
      "3116\n",
      "3117\n",
      "3118\n",
      "3119\n",
      "3120\n",
      "3121\n",
      "3122\n",
      "3123\n",
      "3124\n",
      "3125\n",
      "3126\n",
      "3127\n",
      "3128\n",
      "3129\n",
      "3130\n",
      "3131\n",
      "3132\n",
      "3133\n",
      "3134\n",
      "3135\n",
      "3136\n",
      "3137\n",
      "3138\n",
      "3139\n",
      "3140\n",
      "3141\n",
      "3142\n",
      "3143\n",
      "3144\n",
      "3145\n",
      "3146\n",
      "3147\n",
      "3148\n",
      "3149\n",
      "3150\n",
      "3151\n",
      "3152\n",
      "3153\n",
      "3154\n",
      "3155\n",
      "3156\n",
      "3157\n",
      "3158\n",
      "3159\n",
      "3160\n",
      "3161\n",
      "3162\n",
      "3163\n",
      "3164\n",
      "3165\n",
      "3166\n",
      "3167\n",
      "3168\n",
      "3169\n",
      "3170\n",
      "3171\n",
      "3172\n",
      "3173\n",
      "3174\n",
      "3175\n",
      "3176\n",
      "3177\n",
      "3178\n",
      "3179\n",
      "3180\n",
      "3181\n",
      "3182\n",
      "3183\n",
      "3184\n",
      "3185\n",
      "3186\n",
      "3187\n",
      "3188\n",
      "3189\n",
      "3190\n",
      "3191\n",
      "3192\n",
      "3193\n",
      "3194\n",
      "3195\n",
      "3196\n",
      "3197\n",
      "3198\n",
      "3199\n",
      "3200\n",
      "3201\n",
      "3202\n",
      "3203\n",
      "3204\n",
      "3205\n",
      "3206\n",
      "3207\n",
      "3208\n",
      "3209\n",
      "3210\n",
      "3211\n",
      "3212\n",
      "3213\n",
      "3214\n",
      "3215\n",
      "3216\n",
      "3217\n",
      "3218\n",
      "3219\n",
      "3220\n",
      "3221\n",
      "3222\n",
      "3223\n",
      "3224\n",
      "3225\n",
      "3226\n",
      "3227\n",
      "3228\n",
      "3229\n",
      "3230\n",
      "3231\n",
      "3232\n",
      "3233\n",
      "3234\n",
      "3235\n",
      "3236\n",
      "3237\n",
      "3238\n",
      "3239\n",
      "3240\n",
      "3241\n",
      "3242\n",
      "3243\n",
      "3244\n",
      "3245\n",
      "3246\n",
      "3247\n",
      "3248\n",
      "3249\n",
      "3250\n",
      "3251\n",
      "3252\n",
      "3253\n",
      "3254\n",
      "3255\n",
      "3256\n",
      "3257\n",
      "3258\n",
      "3259\n",
      "3260\n",
      "3261\n",
      "3262\n",
      "3263\n",
      "3264\n",
      "3265\n",
      "3266\n",
      "3267\n",
      "3268\n",
      "3269\n",
      "3270\n",
      "3271\n",
      "3272\n",
      "3273\n",
      "3274\n",
      "3275\n",
      "3276\n",
      "3277\n",
      "3278\n",
      "3279\n",
      "3280\n",
      "3281\n",
      "3282\n",
      "3283\n",
      "3284\n",
      "3285\n",
      "3286\n",
      "3287\n",
      "3288\n",
      "3289\n",
      "3290\n",
      "3291\n",
      "3292\n",
      "3293\n",
      "3294\n",
      "3295\n",
      "3296\n",
      "3297\n",
      "3298\n",
      "3299\n",
      "3300\n",
      "3301\n",
      "3302\n",
      "3303\n",
      "3304\n",
      "3305\n",
      "3306\n",
      "3307\n",
      "3308\n",
      "3309\n",
      "3310\n",
      "3311\n",
      "3312\n",
      "3313\n",
      "3314\n",
      "3315\n",
      "3316\n",
      "3317\n",
      "3318\n",
      "3319\n",
      "3320\n",
      "3321\n",
      "3322\n",
      "3323\n",
      "3324\n",
      "3325\n",
      "3326\n",
      "3327\n",
      "3328\n",
      "3329\n",
      "3330\n",
      "3331\n",
      "3332\n",
      "3333\n",
      "3334\n",
      "3335\n",
      "3336\n",
      "3337\n",
      "3338\n",
      "3339\n",
      "3340\n",
      "3341\n",
      "3342\n",
      "3343\n",
      "3344\n",
      "3345\n",
      "3346\n",
      "3347\n",
      "3348\n",
      "3349\n",
      "3350\n",
      "3351\n",
      "3352\n",
      "3353\n",
      "3354\n",
      "3355\n",
      "3356\n",
      "3357\n",
      "3358\n",
      "3359\n",
      "3360\n",
      "3361\n",
      "3362\n",
      "3363\n",
      "3364\n",
      "3365\n",
      "3366\n",
      "3367\n",
      "3368\n",
      "3369\n",
      "3370\n",
      "3371\n",
      "3372\n",
      "3373\n",
      "3374\n",
      "3375\n",
      "3376\n",
      "3377\n",
      "3378\n",
      "3379\n",
      "3380\n",
      "3381\n",
      "3382\n",
      "3383\n",
      "3384\n",
      "3385\n",
      "3386\n",
      "3387\n",
      "3388\n",
      "3389\n",
      "3390\n",
      "3391\n",
      "3392\n",
      "3393\n",
      "3394\n",
      "3395\n",
      "3396\n",
      "3397\n",
      "3398\n",
      "3399\n",
      "3400\n",
      "3401\n",
      "3402\n",
      "3403\n",
      "3404\n",
      "3405\n",
      "3406\n",
      "3407\n",
      "3408\n",
      "3409\n",
      "3410\n",
      "3411\n",
      "3412\n",
      "3413\n",
      "3414\n",
      "3415\n",
      "3416\n",
      "3417\n",
      "3418\n",
      "3419\n",
      "3420\n",
      "3421\n",
      "3422\n",
      "3423\n",
      "3424\n",
      "3425\n",
      "3426\n",
      "3427\n",
      "3428\n",
      "3429\n",
      "3430\n",
      "3431\n",
      "3432\n",
      "3433\n",
      "3434\n",
      "3435\n",
      "3436\n",
      "3437\n",
      "3438\n",
      "3439\n",
      "3440\n",
      "3441\n",
      "3442\n",
      "3443\n",
      "3444\n",
      "3445\n",
      "3446\n",
      "3447\n",
      "3448\n",
      "3449\n",
      "3450\n",
      "3451\n",
      "3452\n",
      "3453\n",
      "3454\n",
      "3455\n",
      "3456\n",
      "3457\n",
      "3458\n",
      "3459\n",
      "3460\n",
      "3461\n",
      "3462\n",
      "3463\n",
      "3464\n",
      "3465\n",
      "3466\n",
      "3467\n",
      "3468\n",
      "3469\n",
      "3470\n",
      "3471\n",
      "3472\n",
      "3473\n",
      "3474\n",
      "3475\n",
      "3476\n",
      "3477\n",
      "3478\n",
      "3479\n",
      "3480\n",
      "3481\n",
      "3482\n",
      "3483\n",
      "3484\n",
      "3485\n",
      "3486\n",
      "3487\n",
      "3488\n",
      "3489\n",
      "3490\n",
      "3491\n",
      "3492\n",
      "3493\n",
      "3494\n",
      "3495\n",
      "3496\n",
      "3497\n",
      "3498\n",
      "3499\n",
      "3500\n",
      "3501\n",
      "3502\n",
      "3503\n",
      "3504\n",
      "3505\n",
      "3506\n",
      "3507\n",
      "3508\n",
      "3509\n",
      "3510\n",
      "3511\n",
      "3512\n",
      "3513\n",
      "3514\n",
      "3515\n",
      "3516\n",
      "3517\n",
      "3518\n",
      "3519\n",
      "3520\n",
      "3521\n",
      "3522\n",
      "3523\n",
      "3524\n",
      "3525\n",
      "3526\n",
      "3527\n",
      "3528\n",
      "3529\n",
      "3530\n",
      "3531\n",
      "3532\n",
      "3533\n",
      "3534\n",
      "3535\n",
      "3536\n",
      "3537\n",
      "3538\n",
      "3539\n",
      "3540\n",
      "3541\n",
      "3542\n",
      "3543\n",
      "3544\n",
      "3545\n",
      "3546\n",
      "3547\n",
      "3548\n",
      "3549\n",
      "3550\n",
      "3551\n",
      "3552\n",
      "3553\n",
      "3554\n",
      "3555\n",
      "3556\n",
      "3557\n",
      "3558\n",
      "3559\n",
      "3560\n",
      "3561\n",
      "3562\n",
      "3563\n",
      "3564\n",
      "3565\n",
      "3566\n",
      "3567\n",
      "3568\n",
      "3569\n",
      "3570\n",
      "3571\n",
      "3572\n",
      "3573\n",
      "3574\n",
      "3575\n",
      "3576\n",
      "3577\n",
      "3578\n",
      "3579\n",
      "3580\n",
      "3581\n",
      "3582\n",
      "3583\n",
      "3584\n",
      "3585\n",
      "3586\n",
      "3587\n",
      "3588\n",
      "3589\n",
      "3590\n",
      "3591\n",
      "3592\n",
      "3593\n",
      "3594\n",
      "3595\n",
      "3596\n",
      "3597\n",
      "3598\n",
      "3599\n",
      "3600\n",
      "3601\n",
      "3602\n",
      "3603\n",
      "3604\n",
      "3605\n",
      "3606\n",
      "3607\n",
      "3608\n",
      "3609\n",
      "3610\n",
      "3611\n",
      "3612\n",
      "3613\n",
      "3614\n",
      "3615\n",
      "3616\n",
      "3617\n",
      "3618\n",
      "3619\n",
      "3620\n",
      "3621\n",
      "3622\n",
      "3623\n",
      "3624\n",
      "3625\n",
      "3626\n",
      "3627\n",
      "3628\n",
      "3629\n",
      "3630\n",
      "3631\n",
      "3632\n",
      "3633\n",
      "3634\n",
      "3635\n",
      "3636\n",
      "3637\n",
      "3638\n",
      "3639\n",
      "3640\n",
      "3641\n",
      "3642\n",
      "3643\n",
      "3644\n",
      "3645\n",
      "3646\n",
      "3647\n",
      "3648\n",
      "3649\n",
      "3650\n",
      "3651\n",
      "3652\n",
      "3653\n",
      "3654\n",
      "3655\n",
      "3656\n",
      "3657\n",
      "3658\n",
      "3659\n",
      "3660\n",
      "3661\n",
      "3662\n",
      "3663\n",
      "3664\n",
      "3665\n",
      "3666\n",
      "3667\n",
      "3668\n",
      "3669\n",
      "3670\n",
      "3671\n",
      "3672\n",
      "3673\n",
      "3674\n",
      "3675\n",
      "3676\n",
      "3677\n",
      "3678\n",
      "3679\n",
      "3680\n",
      "3681\n",
      "3682\n",
      "3683\n",
      "3684\n",
      "3685\n",
      "3686\n",
      "3687\n",
      "3688\n",
      "3689\n",
      "3690\n",
      "3691\n",
      "3692\n",
      "3693\n",
      "3694\n",
      "3695\n",
      "3696\n",
      "3697\n",
      "3698\n",
      "3699\n",
      "3700\n",
      "3701\n",
      "3702\n",
      "3703\n",
      "3704\n",
      "3705\n",
      "3706\n",
      "3707\n",
      "3708\n",
      "3709\n",
      "3710\n",
      "3711\n",
      "3712\n",
      "3713\n",
      "3714\n",
      "3715\n",
      "3716\n",
      "3717\n",
      "3718\n",
      "3719\n",
      "3720\n",
      "3721\n",
      "3722\n",
      "3723\n",
      "3724\n",
      "3725\n",
      "3726\n",
      "3727\n",
      "3728\n",
      "3729\n",
      "3730\n",
      "3731\n",
      "3732\n",
      "3733\n",
      "3734\n",
      "3735\n",
      "3736\n",
      "3737\n",
      "3738\n",
      "3739\n",
      "3740\n",
      "3741\n",
      "3742\n",
      "3743\n",
      "3744\n",
      "3745\n",
      "3746\n",
      "3747\n",
      "3748\n",
      "3749\n",
      "3750\n",
      "3751\n",
      "3752\n",
      "3753\n",
      "3754\n",
      "3755\n",
      "3756\n",
      "3757\n",
      "3758\n",
      "3759\n",
      "3760\n",
      "3761\n",
      "3762\n",
      "3763\n",
      "3764\n",
      "3765\n",
      "3766\n",
      "3767\n",
      "3768\n",
      "3769\n",
      "3770\n",
      "3771\n",
      "3772\n",
      "3773\n",
      "3774\n",
      "3775\n",
      "3776\n",
      "3777\n",
      "3778\n",
      "3779\n",
      "3780\n",
      "3781\n",
      "3782\n",
      "3783\n",
      "3784\n",
      "3785\n",
      "3786\n",
      "3787\n",
      "3788\n",
      "3789\n",
      "3790\n",
      "3791\n",
      "3792\n",
      "3793\n",
      "3794\n",
      "3795\n",
      "3796\n",
      "3797\n",
      "3798\n",
      "3799\n",
      "3800\n",
      "3801\n",
      "3802\n",
      "3803\n",
      "3804\n",
      "3805\n",
      "3806\n",
      "3807\n",
      "3808\n",
      "3809\n",
      "3810\n",
      "3811\n",
      "3812\n",
      "3813\n",
      "3814\n",
      "3815\n",
      "3816\n",
      "3817\n",
      "3818\n",
      "3819\n",
      "3820\n",
      "3821\n",
      "3822\n",
      "3823\n",
      "3824\n",
      "3825\n",
      "3826\n",
      "3827\n",
      "3828\n",
      "3829\n",
      "3830\n",
      "3831\n",
      "3832\n",
      "3833\n",
      "3834\n",
      "3835\n",
      "3836\n",
      "3837\n",
      "3838\n",
      "3839\n",
      "3840\n",
      "3841\n",
      "3842\n",
      "3843\n",
      "3844\n",
      "3845\n",
      "3846\n",
      "3847\n",
      "3848\n",
      "3849\n",
      "3850\n",
      "3851\n",
      "3852\n",
      "3853\n",
      "3854\n",
      "3855\n",
      "3856\n",
      "3857\n",
      "3858\n",
      "3859\n",
      "3860\n",
      "3861\n",
      "3862\n",
      "3863\n",
      "3864\n",
      "3865\n",
      "3866\n",
      "3867\n",
      "3868\n",
      "3869\n",
      "3870\n",
      "3871\n",
      "3872\n",
      "3873\n",
      "3874\n",
      "3875\n",
      "3876\n",
      "3877\n",
      "3878\n",
      "3879\n",
      "3880\n",
      "3881\n",
      "3882\n",
      "3883\n",
      "3884\n",
      "3885\n",
      "3886\n",
      "3887\n",
      "3888\n",
      "3889\n",
      "3890\n",
      "3891\n",
      "3892\n",
      "3893\n",
      "3894\n",
      "3895\n",
      "3896\n",
      "3897\n",
      "3898\n",
      "3899\n",
      "3900\n",
      "3901\n",
      "3902\n",
      "3903\n",
      "3904\n",
      "3905\n",
      "3906\n",
      "3907\n",
      "3908\n",
      "3909\n",
      "3910\n",
      "3911\n",
      "3912\n",
      "3913\n",
      "3914\n",
      "3915\n",
      "3916\n",
      "3917\n",
      "3918\n",
      "3919\n",
      "3920\n",
      "3921\n",
      "3922\n",
      "3923\n",
      "3924\n",
      "3925\n",
      "3926\n",
      "3927\n",
      "3928\n",
      "3929\n",
      "3930\n",
      "3931\n",
      "3932\n",
      "3933\n",
      "3934\n",
      "3935\n",
      "3936\n",
      "3937\n",
      "3938\n",
      "3939\n",
      "3940\n",
      "3941\n",
      "3942\n",
      "3943\n",
      "3944\n",
      "3945\n",
      "3946\n",
      "3947\n",
      "3948\n",
      "3949\n",
      "3950\n",
      "3951\n",
      "3952\n",
      "3953\n",
      "3954\n",
      "3955\n",
      "3956\n",
      "3957\n",
      "3958\n",
      "3959\n",
      "3960\n",
      "3961\n",
      "3962\n",
      "3963\n",
      "3964\n",
      "3965\n",
      "3966\n",
      "3967\n",
      "3968\n",
      "3969\n",
      "3970\n",
      "3971\n",
      "3972\n",
      "3973\n",
      "3974\n",
      "3975\n",
      "3976\n",
      "3977\n",
      "3978\n",
      "3979\n",
      "3980\n",
      "3981\n",
      "3982\n",
      "3983\n",
      "3984\n",
      "3985\n",
      "3986\n",
      "3987\n",
      "3988\n",
      "3989\n",
      "3990\n",
      "3991\n",
      "3992\n",
      "3993\n",
      "3994\n",
      "3995\n",
      "3996\n",
      "3997\n",
      "3998\n",
      "3999\n",
      "4000\n",
      "4001\n",
      "4002\n",
      "4003\n",
      "4004\n",
      "4005\n",
      "4006\n",
      "4007\n",
      "4008\n",
      "4009\n",
      "4010\n",
      "4011\n",
      "4012\n",
      "4013\n",
      "4014\n",
      "4015\n",
      "4016\n",
      "4017\n",
      "4018\n",
      "4019\n",
      "4020\n",
      "4021\n",
      "4022\n",
      "4023\n",
      "4024\n",
      "4025\n",
      "4026\n",
      "4027\n",
      "4028\n",
      "4029\n",
      "4030\n",
      "4031\n",
      "4032\n",
      "4033\n",
      "4034\n",
      "4035\n",
      "4036\n",
      "4037\n",
      "4038\n",
      "4039\n",
      "4040\n",
      "4041\n",
      "4042\n",
      "4043\n",
      "4044\n",
      "4045\n",
      "4046\n",
      "4047\n",
      "4048\n",
      "4049\n",
      "4050\n",
      "4051\n",
      "4052\n",
      "4053\n",
      "4054\n",
      "4055\n",
      "4056\n",
      "4057\n",
      "4058\n",
      "4059\n",
      "4060\n",
      "4061\n",
      "4062\n",
      "4063\n",
      "4064\n",
      "4065\n",
      "4066\n",
      "4067\n",
      "4068\n",
      "4069\n",
      "4070\n",
      "4071\n",
      "4072\n",
      "4073\n",
      "4074\n",
      "4075\n",
      "4076\n",
      "4077\n",
      "4078\n",
      "4079\n",
      "4080\n",
      "4081\n",
      "4082\n",
      "4083\n",
      "4084\n",
      "4085\n",
      "4086\n",
      "4087\n",
      "4088\n",
      "4089\n",
      "4090\n",
      "4091\n",
      "4092\n",
      "4093\n",
      "4094\n",
      "4095\n",
      "4096\n",
      "4097\n",
      "4098\n",
      "4099\n",
      "4100\n",
      "4101\n",
      "4102\n",
      "4103\n",
      "4104\n",
      "4105\n",
      "4106\n",
      "4107\n",
      "4108\n",
      "4109\n",
      "4110\n",
      "4111\n",
      "4112\n",
      "4113\n",
      "4114\n",
      "4115\n",
      "4116\n",
      "4117\n",
      "4118\n",
      "4119\n",
      "4120\n",
      "4121\n",
      "4122\n",
      "4123\n",
      "4124\n",
      "4125\n",
      "4126\n",
      "4127\n",
      "4128\n",
      "4129\n",
      "4130\n",
      "4131\n",
      "4132\n",
      "4133\n",
      "4134\n",
      "4135\n",
      "4136\n",
      "4137\n",
      "4138\n",
      "4139\n",
      "4140\n",
      "4141\n",
      "4142\n",
      "4143\n",
      "4144\n",
      "4145\n",
      "4146\n",
      "4147\n",
      "4148\n",
      "4149\n",
      "4150\n",
      "4151\n",
      "4152\n",
      "4153\n",
      "4154\n",
      "4155\n",
      "4156\n",
      "4157\n",
      "4158\n",
      "4159\n",
      "4160\n",
      "4161\n",
      "4162\n",
      "4163\n",
      "4164\n",
      "4165\n",
      "4166\n",
      "4167\n",
      "4168\n",
      "4169\n",
      "4170\n",
      "4171\n",
      "4172\n",
      "4173\n",
      "4174\n",
      "4175\n",
      "4176\n",
      "4177\n",
      "4178\n",
      "4179\n",
      "4180\n",
      "4181\n",
      "4182\n",
      "4183\n",
      "4184\n",
      "4185\n",
      "4186\n",
      "4187\n",
      "4188\n",
      "4189\n",
      "4190\n",
      "4191\n",
      "4192\n",
      "4193\n",
      "4194\n",
      "4195\n",
      "4196\n",
      "4197\n",
      "4198\n",
      "4199\n",
      "4200\n",
      "4201\n",
      "4202\n",
      "4203\n",
      "4204\n",
      "4205\n",
      "4206\n",
      "4207\n",
      "4208\n",
      "4209\n",
      "4210\n",
      "4211\n",
      "4212\n",
      "4213\n",
      "4214\n",
      "4215\n",
      "4216\n",
      "4217\n",
      "4218\n",
      "4219\n",
      "4220\n",
      "4221\n",
      "4222\n",
      "4223\n",
      "4224\n",
      "4225\n",
      "4226\n",
      "4227\n",
      "4228\n",
      "4229\n",
      "4230\n",
      "4231\n",
      "4232\n",
      "4233\n",
      "4234\n",
      "4235\n",
      "4236\n",
      "4237\n",
      "4238\n",
      "4239\n",
      "4240\n",
      "4241\n",
      "4242\n",
      "4243\n",
      "4244\n",
      "4245\n",
      "4246\n",
      "4247\n",
      "4248\n",
      "4249\n",
      "4250\n",
      "4251\n",
      "4252\n",
      "4253\n",
      "4254\n",
      "4255\n",
      "4256\n",
      "4257\n",
      "4258\n",
      "4259\n",
      "4260\n",
      "4261\n",
      "4262\n",
      "4263\n",
      "4264\n",
      "4265\n",
      "4266\n",
      "4267\n",
      "4268\n",
      "4269\n",
      "4270\n",
      "4271\n",
      "4272\n",
      "4273\n",
      "4274\n",
      "4275\n",
      "4276\n",
      "4277\n",
      "4278\n",
      "4279\n",
      "4280\n",
      "4281\n",
      "4282\n",
      "4283\n",
      "4284\n",
      "4285\n",
      "4286\n",
      "4287\n",
      "4288\n",
      "4289\n",
      "4290\n",
      "4291\n",
      "4292\n",
      "4293\n",
      "4294\n",
      "4295\n",
      "4296\n",
      "4297\n",
      "4298\n",
      "4299\n",
      "4300\n",
      "4301\n",
      "4302\n",
      "4303\n",
      "4304\n",
      "4305\n",
      "4306\n",
      "4307\n",
      "4308\n",
      "4309\n",
      "4310\n",
      "4311\n",
      "4312\n",
      "4313\n",
      "4314\n",
      "4315\n",
      "4316\n",
      "4317\n",
      "4318\n",
      "4319\n",
      "4320\n",
      "4321\n",
      "4322\n",
      "4323\n",
      "4324\n",
      "4325\n",
      "4326\n",
      "4327\n",
      "4328\n",
      "4329\n",
      "4330\n",
      "4331\n",
      "4332\n",
      "4333\n",
      "4334\n",
      "4335\n",
      "4336\n",
      "4337\n",
      "4338\n",
      "4339\n",
      "4340\n",
      "4341\n",
      "4342\n",
      "4343\n",
      "4344\n",
      "4345\n",
      "4346\n",
      "4347\n",
      "4348\n",
      "4349\n",
      "4350\n",
      "4351\n",
      "4352\n",
      "4353\n",
      "4354\n",
      "4355\n",
      "4356\n",
      "4357\n",
      "4358\n",
      "4359\n",
      "4360\n",
      "4361\n",
      "4362\n",
      "4363\n",
      "4364\n",
      "4365\n",
      "4366\n",
      "4367\n",
      "4368\n",
      "4369\n",
      "4370\n",
      "4371\n",
      "4372\n",
      "4373\n",
      "4374\n",
      "4375\n",
      "4376\n",
      "4377\n",
      "4378\n",
      "4379\n",
      "4380\n",
      "4381\n",
      "4382\n",
      "4383\n",
      "4384\n",
      "4385\n",
      "4386\n",
      "4387\n",
      "4388\n",
      "4389\n",
      "4390\n",
      "4391\n",
      "4392\n",
      "4393\n",
      "4394\n",
      "4395\n",
      "4396\n",
      "4397\n",
      "4398\n",
      "4399\n",
      "4400\n",
      "4401\n",
      "4402\n",
      "4403\n",
      "4404\n",
      "4405\n",
      "4406\n",
      "4407\n",
      "4408\n",
      "4409\n",
      "4410\n",
      "4411\n",
      "4412\n",
      "4413\n",
      "4414\n",
      "4415\n",
      "4416\n",
      "4417\n",
      "4418\n",
      "4419\n",
      "4420\n",
      "4421\n",
      "4422\n",
      "4423\n",
      "4424\n",
      "4425\n",
      "4426\n",
      "4427\n",
      "4428\n",
      "4429\n",
      "4430\n",
      "4431\n",
      "4432\n",
      "4433\n",
      "4434\n",
      "4435\n",
      "4436\n",
      "4437\n",
      "4438\n",
      "4439\n",
      "4440\n",
      "4441\n",
      "4442\n",
      "4443\n",
      "4444\n",
      "4445\n",
      "4446\n",
      "4447\n",
      "4448\n",
      "4449\n",
      "4450\n",
      "4451\n",
      "4452\n",
      "4453\n",
      "4454\n",
      "4455\n",
      "4456\n",
      "4457\n",
      "4458\n",
      "4459\n",
      "4460\n",
      "4461\n",
      "4462\n",
      "4463\n",
      "4464\n",
      "4465\n",
      "4466\n",
      "4467\n",
      "4468\n",
      "4469\n",
      "4470\n",
      "4471\n",
      "4472\n",
      "4473\n",
      "4474\n",
      "4475\n",
      "4476\n",
      "4477\n",
      "4478\n",
      "4479\n",
      "4480\n",
      "4481\n",
      "4482\n",
      "4483\n",
      "4484\n",
      "4485\n",
      "4486\n",
      "4487\n",
      "4488\n",
      "4489\n",
      "4490\n",
      "4491\n",
      "4492\n",
      "4493\n",
      "4494\n",
      "4495\n",
      "4496\n",
      "4497\n",
      "4498\n",
      "4499\n",
      "4500\n",
      "4501\n",
      "4502\n",
      "4503\n",
      "4504\n",
      "4505\n",
      "4506\n",
      "4507\n",
      "4508\n",
      "4509\n",
      "4510\n",
      "4511\n",
      "4512\n",
      "4513\n",
      "4514\n",
      "4515\n",
      "4516\n",
      "4517\n",
      "4518\n",
      "4519\n",
      "4520\n",
      "4521\n",
      "4522\n",
      "4523\n",
      "4524\n",
      "4525\n",
      "4526\n",
      "4527\n",
      "4528\n",
      "4529\n",
      "4530\n",
      "4531\n",
      "4532\n",
      "4533\n",
      "4534\n",
      "4535\n",
      "4536\n",
      "4537\n",
      "4538\n",
      "4539\n",
      "4540\n",
      "4541\n",
      "4542\n",
      "4543\n",
      "4544\n",
      "4545\n",
      "4546\n",
      "4547\n",
      "4548\n",
      "4549\n",
      "4550\n",
      "4551\n",
      "4552\n",
      "4553\n",
      "4554\n",
      "4555\n",
      "4556\n",
      "4557\n",
      "4558\n",
      "4559\n",
      "4560\n",
      "4561\n",
      "4562\n",
      "4563\n",
      "4564\n",
      "4565\n",
      "4566\n",
      "4567\n",
      "4568\n",
      "4569\n",
      "4570\n",
      "4571\n",
      "4572\n",
      "4573\n",
      "4574\n",
      "4575\n",
      "4576\n",
      "4577\n",
      "4578\n",
      "4579\n",
      "4580\n",
      "4581\n",
      "4582\n",
      "4583\n",
      "4584\n",
      "4585\n",
      "4586\n",
      "4587\n",
      "4588\n",
      "4589\n",
      "4590\n",
      "4591\n",
      "4592\n",
      "4593\n",
      "4594\n",
      "4595\n",
      "4596\n",
      "4597\n",
      "4598\n",
      "4599\n",
      "4600\n",
      "4601\n",
      "4602\n",
      "4603\n",
      "4604\n",
      "4605\n",
      "4606\n",
      "4607\n",
      "4608\n",
      "4609\n",
      "4610\n",
      "4611\n",
      "4612\n",
      "4613\n",
      "4614\n",
      "4615\n",
      "4616\n",
      "4617\n",
      "4618\n",
      "4619\n",
      "4620\n",
      "4621\n",
      "4622\n",
      "4623\n",
      "4624\n",
      "4625\n",
      "4626\n",
      "4627\n",
      "4628\n",
      "4629\n",
      "4630\n",
      "4631\n",
      "4632\n",
      "4633\n",
      "4634\n",
      "4635\n",
      "4636\n",
      "4637\n",
      "4638\n",
      "4639\n",
      "4640\n",
      "4641\n",
      "4642\n",
      "4643\n",
      "4644\n",
      "4645\n",
      "4646\n",
      "4647\n",
      "4648\n",
      "4649\n",
      "4650\n",
      "4651\n",
      "4652\n",
      "4653\n",
      "4654\n",
      "4655\n",
      "4656\n",
      "4657\n",
      "4658\n",
      "4659\n",
      "4660\n",
      "4661\n",
      "4662\n",
      "4663\n",
      "4664\n",
      "4665\n",
      "4666\n",
      "4667\n",
      "4668\n",
      "4669\n",
      "4670\n",
      "4671\n",
      "4672\n",
      "4673\n",
      "4674\n",
      "4675\n",
      "4676\n",
      "4677\n",
      "4678\n",
      "4679\n",
      "4680\n",
      "4681\n",
      "4682\n",
      "4683\n",
      "4684\n",
      "4685\n",
      "4686\n",
      "4687\n",
      "4688\n",
      "4689\n",
      "4690\n",
      "4691\n",
      "4692\n",
      "4693\n",
      "4694\n",
      "4695\n",
      "4696\n",
      "4697\n",
      "4698\n",
      "4699\n",
      "4700\n",
      "4701\n",
      "4702\n",
      "4703\n",
      "4704\n",
      "4705\n",
      "4706\n",
      "4707\n",
      "4708\n",
      "4709\n",
      "4710\n",
      "4711\n",
      "4712\n",
      "4713\n",
      "4714\n",
      "4715\n",
      "4716\n",
      "4717\n",
      "4718\n",
      "4719\n",
      "4720\n",
      "4721\n",
      "4722\n",
      "4723\n",
      "4724\n",
      "4725\n",
      "4726\n",
      "4727\n",
      "4728\n",
      "4729\n",
      "4730\n",
      "4731\n",
      "4732\n",
      "4733\n",
      "4734\n",
      "4735\n",
      "4736\n",
      "4737\n",
      "4738\n",
      "4739\n",
      "4740\n",
      "4741\n",
      "4742\n",
      "4743\n",
      "4744\n",
      "4745\n",
      "4746\n",
      "4747\n",
      "4748\n",
      "4749\n",
      "4750\n",
      "4751\n",
      "4752\n",
      "4753\n",
      "4754\n",
      "4755\n",
      "4756\n",
      "4757\n",
      "4758\n",
      "4759\n",
      "4760\n",
      "4761\n",
      "4762\n",
      "4763\n",
      "4764\n",
      "4765\n",
      "4766\n",
      "4767\n",
      "4768\n",
      "4769\n",
      "4770\n",
      "4771\n",
      "4772\n",
      "4773\n",
      "4774\n",
      "4775\n",
      "4776\n",
      "4777\n",
      "4778\n",
      "4779\n",
      "4780\n",
      "4781\n",
      "4782\n",
      "4783\n",
      "4784\n",
      "4785\n",
      "4786\n",
      "4787\n",
      "4788\n",
      "4789\n",
      "4790\n",
      "4791\n",
      "4792\n",
      "4793\n",
      "4794\n",
      "4795\n",
      "4796\n",
      "4797\n",
      "4798\n",
      "4799\n",
      "4800\n",
      "4801\n",
      "4802\n",
      "4803\n",
      "4804\n",
      "4805\n",
      "4806\n",
      "4807\n",
      "4808\n",
      "4809\n",
      "4810\n",
      "4811\n",
      "4812\n",
      "4813\n",
      "4814\n",
      "4815\n",
      "4816\n",
      "4817\n",
      "4818\n",
      "4819\n",
      "4820\n",
      "4821\n",
      "4822\n",
      "4823\n",
      "4824\n",
      "4825\n",
      "4826\n",
      "4827\n",
      "4828\n",
      "4829\n",
      "4830\n",
      "4831\n",
      "4832\n",
      "4833\n",
      "4834\n",
      "4835\n",
      "4836\n",
      "4837\n",
      "4838\n",
      "4839\n",
      "4840\n",
      "4841\n",
      "4842\n",
      "4843\n",
      "4844\n",
      "4845\n",
      "4846\n",
      "4847\n",
      "4848\n",
      "4849\n",
      "4850\n",
      "4851\n",
      "4852\n",
      "4853\n",
      "4854\n",
      "4855\n",
      "4856\n",
      "4857\n",
      "4858\n",
      "4859\n",
      "4860\n",
      "4861\n",
      "4862\n",
      "4863\n",
      "4864\n",
      "4865\n",
      "4866\n",
      "4867\n",
      "4868\n",
      "4869\n",
      "4870\n",
      "4871\n",
      "4872\n",
      "4873\n",
      "4874\n",
      "4875\n",
      "4876\n",
      "4877\n",
      "4878\n",
      "4879\n",
      "4880\n",
      "4881\n",
      "4882\n",
      "4883\n",
      "4884\n",
      "4885\n",
      "4886\n",
      "4887\n",
      "4888\n",
      "4889\n",
      "4890\n",
      "4891\n",
      "4892\n",
      "4893\n",
      "4894\n",
      "4895\n",
      "4896\n",
      "4897\n",
      "4898\n",
      "4899\n",
      "4900\n",
      "4901\n",
      "4902\n",
      "4903\n",
      "4904\n",
      "4905\n",
      "4906\n",
      "4907\n",
      "4908\n",
      "4909\n",
      "4910\n",
      "4911\n",
      "4912\n",
      "4913\n",
      "4914\n",
      "4915\n",
      "4916\n",
      "4917\n",
      "4918\n",
      "4919\n",
      "4920\n",
      "4921\n",
      "4922\n",
      "4923\n",
      "4924\n",
      "4925\n",
      "4926\n",
      "4927\n",
      "4928\n",
      "4929\n",
      "4930\n",
      "4931\n",
      "4932\n",
      "4933\n",
      "4934\n",
      "4935\n",
      "4936\n",
      "4937\n",
      "4938\n",
      "4939\n",
      "4940\n",
      "4941\n",
      "4942\n",
      "4943\n",
      "4944\n",
      "4945\n",
      "4946\n",
      "4947\n",
      "4948\n",
      "4949\n",
      "4950\n",
      "4951\n",
      "4952\n",
      "4953\n",
      "4954\n",
      "4955\n",
      "4956\n",
      "4957\n",
      "4958\n",
      "4959\n",
      "4960\n",
      "4961\n",
      "4962\n",
      "4963\n",
      "4964\n",
      "4965\n",
      "4966\n",
      "4967\n",
      "4968\n",
      "4969\n",
      "4970\n",
      "4971\n",
      "4972\n",
      "4973\n",
      "4974\n",
      "4975\n",
      "4976\n",
      "4977\n",
      "4978\n",
      "4979\n",
      "4980\n",
      "4981\n",
      "4982\n",
      "4983\n",
      "4984\n",
      "4985\n",
      "4986\n",
      "4987\n",
      "4988\n",
      "4989\n",
      "4990\n",
      "4991\n",
      "4992\n",
      "4993\n",
      "4994\n",
      "4995\n",
      "4996\n",
      "4997\n",
      "4998\n",
      "4999\n",
      "5000\n",
      "5001\n",
      "5002\n",
      "5003\n",
      "5004\n",
      "5005\n",
      "5006\n",
      "5007\n",
      "5008\n",
      "5009\n",
      "5010\n",
      "5011\n",
      "5012\n",
      "5013\n",
      "5014\n",
      "5015\n",
      "5016\n",
      "5017\n",
      "5018\n",
      "5019\n",
      "5020\n",
      "5021\n",
      "5022\n",
      "5023\n",
      "5024\n",
      "5025\n",
      "5026\n",
      "5027\n",
      "5028\n",
      "5029\n",
      "5030\n",
      "5031\n",
      "5032\n",
      "5033\n",
      "5034\n",
      "5035\n",
      "5036\n",
      "5037\n",
      "5038\n",
      "5039\n",
      "5040\n",
      "5041\n",
      "5042\n",
      "5043\n",
      "5044\n",
      "5045\n",
      "5046\n",
      "5047\n",
      "5048\n",
      "5049\n",
      "5050\n",
      "5051\n",
      "5052\n",
      "5053\n",
      "5054\n",
      "5055\n",
      "5056\n",
      "5057\n",
      "5058\n",
      "5059\n",
      "5060\n",
      "5061\n",
      "5062\n",
      "5063\n",
      "5064\n",
      "5065\n",
      "5066\n",
      "5067\n",
      "5068\n",
      "5069\n",
      "5070\n",
      "5071\n",
      "5072\n",
      "5073\n",
      "5074\n",
      "5075\n",
      "5076\n",
      "5077\n",
      "5078\n",
      "5079\n",
      "5080\n",
      "5081\n",
      "5082\n",
      "5083\n",
      "5084\n",
      "5085\n",
      "5086\n",
      "5087\n",
      "5088\n",
      "5089\n",
      "5090\n",
      "5091\n",
      "5092\n",
      "5093\n",
      "5094\n",
      "5095\n",
      "5096\n",
      "5097\n",
      "5098\n",
      "5099\n",
      "5100\n",
      "5101\n",
      "5102\n",
      "5103\n",
      "5104\n",
      "5105\n",
      "5106\n",
      "5107\n",
      "5108\n",
      "5109\n",
      "5110\n",
      "5111\n",
      "5112\n",
      "5113\n",
      "5114\n",
      "5115\n",
      "5116\n",
      "5117\n",
      "5118\n",
      "5119\n",
      "5120\n",
      "5121\n",
      "5122\n",
      "5123\n",
      "5124\n",
      "5125\n",
      "5126\n",
      "5127\n",
      "5128\n",
      "5129\n",
      "5130\n",
      "5131\n",
      "5132\n",
      "5133\n",
      "5134\n",
      "5135\n",
      "5136\n",
      "5137\n",
      "5138\n",
      "5139\n",
      "5140\n",
      "5141\n",
      "5142\n",
      "5143\n",
      "5144\n",
      "5145\n",
      "5146\n",
      "5147\n",
      "5148\n",
      "5149\n",
      "5150\n",
      "5151\n",
      "5152\n",
      "5153\n",
      "5154\n",
      "5155\n",
      "5156\n",
      "5157\n",
      "5158\n",
      "5159\n",
      "5160\n",
      "5161\n",
      "5162\n",
      "5163\n",
      "5164\n",
      "5165\n",
      "5166\n",
      "5167\n",
      "5168\n",
      "5169\n",
      "5170\n",
      "5171\n",
      "5172\n",
      "5173\n",
      "5174\n",
      "5175\n",
      "5176\n",
      "5177\n",
      "5178\n",
      "5179\n",
      "5180\n",
      "5181\n",
      "5182\n",
      "5183\n",
      "5184\n",
      "5185\n",
      "5186\n",
      "5187\n",
      "5188\n",
      "5189\n",
      "5190\n",
      "5191\n",
      "5192\n",
      "5193\n",
      "5194\n",
      "5195\n",
      "5196\n",
      "5197\n",
      "5198\n",
      "5199\n",
      "5200\n",
      "5201\n",
      "5202\n",
      "5203\n",
      "5204\n",
      "5205\n",
      "5206\n",
      "5207\n",
      "5208\n",
      "5209\n",
      "5210\n",
      "5211\n",
      "5212\n",
      "5213\n",
      "5214\n",
      "5215\n",
      "5216\n",
      "5217\n",
      "5218\n",
      "5219\n",
      "5220\n",
      "5221\n",
      "5222\n",
      "5223\n",
      "5224\n",
      "5225\n",
      "5226\n",
      "5227\n",
      "5228\n",
      "5229\n",
      "5230\n",
      "5231\n",
      "5232\n",
      "5233\n",
      "5234\n",
      "5235\n",
      "5236\n",
      "5237\n",
      "5238\n",
      "5239\n",
      "5240\n",
      "5241\n",
      "5242\n",
      "5243\n",
      "5244\n",
      "5245\n",
      "5246\n",
      "5247\n",
      "5248\n",
      "5249\n",
      "5250\n",
      "5251\n",
      "5252\n",
      "5253\n",
      "5254\n",
      "5255\n",
      "5256\n",
      "5257\n",
      "5258\n",
      "5259\n",
      "5260\n",
      "5261\n",
      "5262\n",
      "5263\n",
      "5264\n",
      "5265\n",
      "5266\n",
      "5267\n",
      "5268\n",
      "5269\n",
      "5270\n",
      "5271\n",
      "5272\n",
      "5273\n",
      "5274\n",
      "5275\n",
      "5276\n",
      "5277\n",
      "5278\n",
      "5279\n",
      "5280\n",
      "5281\n",
      "5282\n",
      "5283\n",
      "5284\n",
      "5285\n",
      "5286\n",
      "5287\n",
      "5288\n",
      "5289\n",
      "5290\n",
      "5291\n",
      "5292\n",
      "5293\n",
      "5294\n",
      "5295\n",
      "5296\n",
      "5297\n",
      "5298\n",
      "5299\n",
      "5300\n",
      "5301\n",
      "5302\n",
      "5303\n",
      "5304\n",
      "5305\n",
      "5306\n",
      "5307\n",
      "5308\n",
      "5309\n",
      "5310\n",
      "5311\n",
      "5312\n",
      "5313\n",
      "5314\n",
      "5315\n",
      "5316\n",
      "5317\n",
      "5318\n",
      "5319\n",
      "5320\n",
      "5321\n",
      "5322\n",
      "5323\n",
      "5324\n",
      "5325\n",
      "5326\n",
      "5327\n",
      "5328\n",
      "5329\n",
      "5330\n",
      "5331\n",
      "5332\n",
      "5333\n",
      "5334\n",
      "5335\n",
      "5336\n",
      "5337\n",
      "5338\n",
      "5339\n",
      "5340\n",
      "5341\n",
      "5342\n",
      "5343\n",
      "5344\n",
      "5345\n",
      "5346\n",
      "5347\n",
      "5348\n",
      "5349\n",
      "5350\n",
      "5351\n",
      "5352\n",
      "5353\n",
      "5354\n",
      "5355\n",
      "5356\n",
      "5357\n",
      "5358\n",
      "5359\n",
      "5360\n",
      "5361\n",
      "5362\n",
      "5363\n",
      "5364\n",
      "5365\n",
      "5366\n",
      "5367\n",
      "5368\n",
      "5369\n",
      "5370\n",
      "5371\n",
      "5372\n",
      "5373\n",
      "5374\n",
      "5375\n",
      "5376\n",
      "5377\n",
      "5378\n",
      "5379\n",
      "5380\n",
      "5381\n",
      "5382\n",
      "5383\n",
      "5384\n",
      "5385\n",
      "5386\n",
      "5387\n",
      "5388\n",
      "5389\n",
      "5390\n",
      "5391\n",
      "5392\n",
      "5393\n",
      "5394\n",
      "5395\n",
      "5396\n",
      "5397\n",
      "5398\n",
      "5399\n",
      "5400\n",
      "5401\n",
      "5402\n",
      "5403\n",
      "5404\n",
      "5405\n",
      "5406\n",
      "5407\n",
      "5408\n",
      "5409\n",
      "5410\n",
      "5411\n",
      "5412\n",
      "5413\n",
      "5414\n",
      "5415\n",
      "5416\n",
      "5417\n",
      "5418\n",
      "5419\n",
      "5420\n",
      "5421\n",
      "5422\n",
      "5423\n",
      "5424\n",
      "5425\n",
      "5426\n",
      "5427\n",
      "5428\n",
      "5429\n",
      "5430\n",
      "5431\n",
      "5432\n",
      "5433\n",
      "5434\n",
      "5435\n",
      "5436\n",
      "5437\n",
      "5438\n",
      "5439\n",
      "5440\n",
      "5441\n",
      "5442\n",
      "5443\n",
      "5444\n",
      "5445\n",
      "5446\n",
      "5447\n",
      "5448\n",
      "5449\n",
      "5450\n",
      "5451\n",
      "5452\n",
      "5453\n",
      "5454\n",
      "5455\n",
      "5456\n",
      "5457\n",
      "5458\n",
      "5459\n",
      "5460\n",
      "5461\n",
      "5462\n",
      "5463\n",
      "5464\n",
      "5465\n",
      "5466\n",
      "5467\n",
      "5468\n",
      "5469\n",
      "5470\n",
      "5471\n",
      "5472\n",
      "5473\n",
      "5474\n",
      "5475\n",
      "5476\n",
      "5477\n",
      "5478\n",
      "5479\n",
      "5480\n",
      "5481\n",
      "5482\n",
      "5483\n",
      "5484\n",
      "5485\n",
      "5486\n",
      "5487\n",
      "5488\n",
      "5489\n",
      "5490\n",
      "5491\n",
      "5492\n",
      "5493\n",
      "5494\n",
      "5495\n",
      "5496\n",
      "5497\n",
      "5498\n",
      "5499\n",
      "5500\n",
      "5501\n",
      "5502\n",
      "5503\n",
      "5504\n",
      "5505\n",
      "5506\n",
      "5507\n",
      "5508\n",
      "5509\n",
      "5510\n",
      "5511\n",
      "5512\n",
      "5513\n",
      "5514\n",
      "5515\n",
      "5516\n",
      "5517\n",
      "5518\n",
      "5519\n",
      "5520\n",
      "5521\n",
      "5522\n",
      "5523\n",
      "5524\n",
      "5525\n",
      "5526\n",
      "5527\n",
      "5528\n",
      "5529\n",
      "5530\n",
      "5531\n",
      "5532\n",
      "5533\n",
      "5534\n",
      "5535\n",
      "5536\n",
      "5537\n",
      "5538\n",
      "5539\n",
      "5540\n",
      "5541\n",
      "5542\n",
      "5543\n",
      "5544\n",
      "5545\n",
      "5546\n",
      "5547\n",
      "5548\n",
      "5549\n",
      "5550\n",
      "5551\n",
      "5552\n",
      "5553\n",
      "5554\n",
      "5555\n",
      "5556\n",
      "5557\n",
      "5558\n",
      "5559\n",
      "5560\n",
      "5561\n",
      "5562\n",
      "5563\n",
      "5564\n",
      "5565\n",
      "5566\n",
      "5567\n",
      "5568\n",
      "5569\n",
      "5570\n",
      "5571\n",
      "5572\n",
      "5573\n",
      "5574\n",
      "5575\n",
      "5576\n",
      "5577\n",
      "5578\n",
      "5579\n",
      "5580\n",
      "5581\n",
      "5582\n",
      "5583\n",
      "5584\n",
      "5585\n",
      "5586\n",
      "5587\n",
      "5588\n",
      "5589\n",
      "5590\n",
      "5591\n",
      "5592\n",
      "5593\n",
      "5594\n",
      "5595\n",
      "5596\n",
      "5597\n",
      "5598\n",
      "5599\n",
      "5600\n",
      "5601\n",
      "5602\n",
      "5603\n",
      "5604\n",
      "5605\n",
      "5606\n",
      "5607\n",
      "5608\n",
      "5609\n",
      "5610\n",
      "5611\n",
      "5612\n",
      "5613\n",
      "5614\n",
      "5615\n",
      "5616\n",
      "5617\n",
      "5618\n",
      "5619\n",
      "5620\n",
      "5621\n",
      "5622\n",
      "5623\n",
      "5624\n",
      "5625\n",
      "5626\n",
      "5627\n",
      "5628\n",
      "5629\n",
      "5630\n",
      "5631\n",
      "5632\n",
      "5633\n",
      "5634\n",
      "5635\n",
      "5636\n",
      "5637\n",
      "5638\n",
      "5639\n",
      "5640\n",
      "5641\n",
      "5642\n",
      "5643\n",
      "5644\n",
      "5645\n",
      "5646\n",
      "5647\n",
      "5648\n",
      "5649\n",
      "5650\n",
      "5651\n",
      "5652\n",
      "5653\n",
      "5654\n",
      "5655\n",
      "5656\n",
      "5657\n",
      "5658\n",
      "5659\n",
      "5660\n",
      "5661\n",
      "5662\n",
      "5663\n",
      "5664\n",
      "5665\n",
      "5666\n",
      "5667\n",
      "5668\n",
      "5669\n",
      "5670\n",
      "5671\n",
      "5672\n",
      "5673\n",
      "5674\n",
      "5675\n",
      "5676\n",
      "5677\n",
      "5678\n",
      "5679\n",
      "5680\n",
      "5681\n",
      "5682\n",
      "5683\n",
      "5684\n",
      "5685\n",
      "5686\n",
      "5687\n",
      "5688\n",
      "5689\n",
      "5690\n",
      "5691\n",
      "5692\n",
      "5693\n",
      "5694\n",
      "5695\n",
      "5696\n",
      "5697\n",
      "5698\n",
      "5699\n",
      "5700\n",
      "5701\n",
      "5702\n",
      "5703\n",
      "5704\n",
      "5705\n",
      "5706\n",
      "5707\n",
      "5708\n",
      "5709\n",
      "5710\n",
      "5711\n",
      "5712\n",
      "5713\n",
      "5714\n",
      "5715\n",
      "5716\n",
      "5717\n",
      "5718\n",
      "5719\n",
      "5720\n",
      "5721\n",
      "5722\n",
      "5723\n",
      "5724\n",
      "5725\n",
      "5726\n",
      "5727\n",
      "5728\n",
      "5729\n",
      "5730\n",
      "5731\n",
      "5732\n",
      "5733\n",
      "5734\n",
      "5735\n",
      "5736\n",
      "5737\n",
      "5738\n",
      "5739\n",
      "5740\n",
      "5741\n",
      "5742\n",
      "5743\n",
      "5744\n",
      "5745\n",
      "5746\n",
      "5747\n",
      "5748\n",
      "5749\n",
      "5750\n",
      "5751\n",
      "5752\n",
      "5753\n",
      "5754\n",
      "5755\n",
      "5756\n",
      "5757\n",
      "5758\n",
      "5759\n",
      "5760\n",
      "5761\n",
      "5762\n",
      "5763\n",
      "5764\n",
      "5765\n",
      "5766\n",
      "5767\n",
      "5768\n",
      "5769\n",
      "5770\n",
      "5771\n",
      "5772\n",
      "5773\n",
      "5774\n",
      "5775\n",
      "5776\n",
      "5777\n",
      "5778\n",
      "5779\n",
      "5780\n",
      "5781\n",
      "5782\n",
      "5783\n",
      "5784\n",
      "5785\n",
      "5786\n",
      "5787\n",
      "5788\n",
      "5789\n",
      "5790\n",
      "5791\n",
      "5792\n",
      "5793\n",
      "5794\n",
      "5795\n",
      "5796\n",
      "5797\n",
      "5798\n",
      "5799\n",
      "5800\n",
      "5801\n",
      "5802\n",
      "5803\n",
      "5804\n",
      "5805\n",
      "5806\n",
      "5807\n",
      "5808\n",
      "5809\n",
      "5810\n",
      "5811\n",
      "5812\n",
      "5813\n",
      "5814\n",
      "5815\n",
      "5816\n",
      "5817\n",
      "5818\n",
      "5819\n",
      "5820\n",
      "5821\n",
      "5822\n",
      "5823\n",
      "5824\n",
      "5825\n",
      "5826\n",
      "5827\n",
      "5828\n",
      "5829\n",
      "5830\n",
      "5831\n",
      "5832\n",
      "5833\n",
      "5834\n",
      "5835\n",
      "5836\n",
      "5837\n",
      "5838\n",
      "5839\n",
      "5840\n",
      "5841\n",
      "5842\n",
      "5843\n",
      "5844\n",
      "5845\n",
      "5846\n",
      "5847\n",
      "5848\n",
      "5849\n",
      "5850\n",
      "5851\n",
      "5852\n",
      "5853\n",
      "5854\n",
      "5855\n",
      "5856\n",
      "5857\n",
      "5858\n",
      "5859\n",
      "5860\n",
      "5861\n",
      "5862\n",
      "5863\n",
      "5864\n",
      "5865\n",
      "5866\n",
      "5867\n",
      "5868\n",
      "5869\n",
      "5870\n",
      "5871\n",
      "5872\n",
      "5873\n",
      "5874\n",
      "5875\n",
      "5876\n",
      "5877\n",
      "5878\n",
      "5879\n",
      "5880\n",
      "5881\n",
      "5882\n",
      "5883\n",
      "5884\n",
      "5885\n",
      "5886\n",
      "5887\n",
      "5888\n",
      "5889\n",
      "5890\n",
      "5891\n",
      "5892\n",
      "5893\n",
      "5894\n",
      "5895\n",
      "5896\n",
      "5897\n",
      "5898\n",
      "5899\n",
      "5900\n",
      "5901\n",
      "5902\n",
      "5903\n",
      "5904\n",
      "5905\n",
      "5906\n",
      "5907\n",
      "5908\n",
      "5909\n",
      "5910\n",
      "5911\n",
      "5912\n",
      "5913\n",
      "5914\n",
      "5915\n",
      "5916\n",
      "5917\n",
      "5918\n",
      "5919\n",
      "5920\n",
      "5921\n",
      "5922\n",
      "5923\n",
      "5924\n",
      "5925\n",
      "5926\n",
      "5927\n",
      "5928\n",
      "5929\n",
      "5930\n",
      "5931\n",
      "5932\n",
      "5933\n",
      "5934\n",
      "5935\n",
      "5936\n",
      "5937\n",
      "5938\n",
      "5939\n",
      "5940\n",
      "5941\n",
      "5942\n",
      "5943\n",
      "5944\n",
      "5945\n",
      "5946\n",
      "5947\n",
      "5948\n",
      "5949\n",
      "5950\n",
      "5951\n",
      "5952\n",
      "5953\n",
      "5954\n",
      "5955\n",
      "5956\n",
      "5957\n",
      "5958\n",
      "5959\n",
      "5960\n",
      "5961\n",
      "5962\n",
      "5963\n",
      "5964\n",
      "5965\n",
      "5966\n",
      "5967\n",
      "5968\n",
      "5969\n",
      "5970\n",
      "5971\n",
      "5972\n",
      "5973\n",
      "5974\n",
      "5975\n",
      "5976\n",
      "5977\n",
      "5978\n",
      "5979\n",
      "5980\n",
      "5981\n",
      "5982\n",
      "5983\n",
      "5984\n",
      "5985\n",
      "5986\n",
      "5987\n",
      "5988\n",
      "5989\n",
      "5990\n",
      "5991\n",
      "5992\n",
      "5993\n",
      "5994\n",
      "5995\n",
      "5996\n",
      "5997\n",
      "5998\n",
      "5999\n",
      "6000\n",
      "6001\n",
      "6002\n",
      "6003\n",
      "6004\n",
      "6005\n",
      "6006\n",
      "6007\n",
      "6008\n",
      "6009\n",
      "6010\n",
      "6011\n",
      "6012\n",
      "6013\n",
      "6014\n",
      "6015\n",
      "6016\n",
      "6017\n",
      "6018\n",
      "6019\n",
      "6020\n",
      "6021\n",
      "6022\n",
      "6023\n",
      "6024\n",
      "6025\n",
      "6026\n",
      "6027\n",
      "6028\n",
      "6029\n",
      "6030\n",
      "6031\n",
      "6032\n",
      "6033\n",
      "6034\n",
      "6035\n",
      "6036\n",
      "6037\n",
      "6038\n",
      "6039\n",
      "6040\n",
      "6041\n",
      "6042\n",
      "6043\n",
      "6044\n",
      "6045\n",
      "6046\n",
      "6047\n",
      "6048\n",
      "6049\n",
      "6050\n",
      "6051\n",
      "6052\n",
      "6053\n",
      "6054\n",
      "6055\n",
      "6056\n",
      "6057\n",
      "6058\n",
      "6059\n",
      "6060\n",
      "6061\n",
      "6062\n",
      "6063\n",
      "6064\n",
      "6065\n",
      "6066\n",
      "6067\n",
      "6068\n",
      "6069\n",
      "6070\n",
      "6071\n",
      "6072\n",
      "6073\n",
      "6074\n",
      "6075\n",
      "6076\n",
      "6077\n",
      "6078\n",
      "6079\n",
      "6080\n",
      "6081\n",
      "6082\n",
      "6083\n",
      "6084\n",
      "6085\n",
      "6086\n",
      "6087\n",
      "6088\n",
      "6089\n",
      "6090\n",
      "6091\n",
      "6092\n",
      "6093\n",
      "6094\n",
      "6095\n",
      "6096\n",
      "6097\n",
      "6098\n",
      "6099\n",
      "6100\n",
      "6101\n",
      "6102\n",
      "6103\n",
      "6104\n",
      "6105\n",
      "6106\n",
      "6107\n",
      "6108\n",
      "6109\n",
      "6110\n",
      "6111\n",
      "6112\n",
      "6113\n",
      "6114\n",
      "6115\n",
      "6116\n",
      "6117\n",
      "6118\n",
      "6119\n",
      "6120\n",
      "6121\n",
      "6122\n",
      "6123\n",
      "6124\n",
      "6125\n",
      "6126\n",
      "6127\n",
      "6128\n",
      "6129\n",
      "6130\n",
      "6131\n",
      "6132\n",
      "6133\n",
      "6134\n",
      "6135\n",
      "6136\n",
      "6137\n",
      "6138\n",
      "6139\n",
      "6140\n",
      "6141\n",
      "6142\n",
      "6143\n",
      "6144\n",
      "6145\n",
      "6146\n",
      "6147\n",
      "6148\n",
      "6149\n",
      "6150\n",
      "6151\n",
      "6152\n",
      "6153\n",
      "6154\n",
      "6155\n",
      "6156\n",
      "6157\n",
      "6158\n",
      "6159\n",
      "6160\n",
      "6161\n",
      "6162\n",
      "6163\n",
      "6164\n",
      "6165\n",
      "6166\n",
      "6167\n",
      "6168\n",
      "6169\n",
      "6170\n",
      "6171\n",
      "6172\n",
      "6173\n",
      "6174\n",
      "6175\n",
      "6176\n",
      "6177\n",
      "6178\n",
      "6179\n",
      "6180\n",
      "6181\n",
      "6182\n",
      "6183\n",
      "6184\n",
      "6185\n",
      "6186\n",
      "6187\n",
      "6188\n",
      "6189\n",
      "6190\n",
      "6191\n",
      "6192\n",
      "6193\n",
      "6194\n",
      "6195\n",
      "6196\n",
      "6197\n",
      "6198\n",
      "6199\n",
      "6200\n",
      "6201\n",
      "6202\n",
      "6203\n",
      "6204\n",
      "6205\n",
      "6206\n",
      "6207\n",
      "6208\n",
      "6209\n",
      "6210\n",
      "6211\n",
      "6212\n",
      "6213\n",
      "6214\n",
      "6215\n",
      "6216\n",
      "6217\n",
      "6218\n",
      "6219\n",
      "6220\n",
      "6221\n",
      "6222\n",
      "6223\n",
      "6224\n",
      "6225\n",
      "6226\n",
      "6227\n",
      "6228\n",
      "6229\n",
      "6230\n",
      "6231\n",
      "6232\n",
      "6233\n",
      "6234\n",
      "6235\n",
      "6236\n",
      "6237\n",
      "6238\n",
      "6239\n",
      "6240\n",
      "6241\n",
      "6242\n",
      "6243\n",
      "6244\n",
      "6245\n",
      "6246\n",
      "6247\n",
      "6248\n",
      "6249\n",
      "6250\n",
      "6251\n",
      "6252\n",
      "6253\n",
      "6254\n",
      "6255\n",
      "6256\n",
      "6257\n",
      "6258\n",
      "6259\n",
      "6260\n",
      "6261\n",
      "6262\n",
      "6263\n",
      "6264\n",
      "6265\n",
      "6266\n",
      "6267\n",
      "6268\n",
      "6269\n",
      "6270\n",
      "6271\n",
      "6272\n",
      "6273\n",
      "6274\n",
      "6275\n",
      "6276\n",
      "6277\n",
      "6278\n",
      "6279\n",
      "6280\n",
      "6281\n",
      "6282\n",
      "6283\n",
      "6284\n",
      "6285\n",
      "6286\n",
      "6287\n",
      "6288\n",
      "6289\n",
      "6290\n",
      "6291\n",
      "6292\n",
      "6293\n",
      "6294\n",
      "6295\n",
      "6296\n",
      "6297\n",
      "6298\n",
      "6299\n",
      "6300\n",
      "6301\n",
      "6302\n",
      "6303\n",
      "6304\n",
      "6305\n",
      "6306\n",
      "6307\n",
      "6308\n",
      "6309\n",
      "6310\n",
      "6311\n",
      "6312\n",
      "6313\n",
      "6314\n",
      "6315\n",
      "6316\n",
      "6317\n",
      "6318\n",
      "6319\n",
      "6320\n",
      "6321\n",
      "6322\n",
      "6323\n",
      "6324\n",
      "6325\n",
      "6326\n",
      "6327\n",
      "6328\n",
      "6329\n",
      "6330\n",
      "6331\n",
      "6332\n",
      "6333\n",
      "6334\n",
      "6335\n",
      "6336\n",
      "6337\n",
      "6338\n",
      "6339\n",
      "6340\n",
      "6341\n",
      "6342\n",
      "6343\n",
      "6344\n",
      "6345\n",
      "6346\n",
      "6347\n",
      "6348\n",
      "6349\n",
      "6350\n",
      "6351\n",
      "6352\n",
      "6353\n",
      "6354\n",
      "6355\n",
      "6356\n",
      "6357\n",
      "6358\n",
      "6359\n",
      "6360\n",
      "6361\n",
      "6362\n",
      "6363\n",
      "6364\n",
      "6365\n",
      "6366\n",
      "6367\n",
      "6368\n",
      "6369\n",
      "6370\n",
      "6371\n",
      "6372\n",
      "6373\n",
      "6374\n",
      "6375\n",
      "6376\n",
      "6377\n",
      "6378\n",
      "6379\n",
      "6380\n",
      "6381\n",
      "6382\n",
      "6383\n",
      "6384\n",
      "6385\n",
      "6386\n",
      "6387\n",
      "6388\n",
      "6389\n",
      "6390\n",
      "6391\n",
      "6392\n",
      "6393\n",
      "6394\n",
      "6395\n",
      "6396\n",
      "6397\n",
      "6398\n",
      "6399\n",
      "6400\n",
      "6401\n",
      "6402\n",
      "6403\n",
      "6404\n",
      "6405\n",
      "6406\n",
      "6407\n",
      "6408\n",
      "6409\n",
      "6410\n",
      "6411\n",
      "6412\n",
      "6413\n",
      "6414\n",
      "6415\n",
      "6416\n",
      "6417\n",
      "6418\n",
      "6419\n",
      "6420\n",
      "6421\n",
      "6422\n",
      "6423\n",
      "6424\n",
      "6425\n",
      "6426\n",
      "6427\n",
      "6428\n",
      "6429\n",
      "6430\n",
      "6431\n",
      "6432\n",
      "6433\n",
      "6434\n",
      "6435\n",
      "6436\n",
      "6437\n",
      "6438\n",
      "6439\n",
      "6440\n",
      "6441\n",
      "6442\n",
      "6443\n",
      "6444\n",
      "6445\n",
      "6446\n",
      "6447\n",
      "6448\n",
      "6449\n",
      "6450\n",
      "6451\n",
      "6452\n",
      "6453\n",
      "6454\n",
      "6455\n",
      "6456\n",
      "6457\n",
      "6458\n",
      "6459\n",
      "6460\n",
      "6461\n",
      "6462\n",
      "6463\n",
      "6464\n",
      "6465\n",
      "6466\n",
      "6467\n",
      "6468\n",
      "6469\n",
      "6470\n",
      "6471\n",
      "6472\n",
      "6473\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "MAX_LEN=512\n",
    "df = pd.read_csv('/content/drive/My Drive/mycolab/ob/trainset1.csv')\n",
    "for num in range(0,df.shape[0]):\n",
    "  print(num)\n",
    "  if predicttext(df['text'].loc[num])<0.5:\n",
    "    df['sentiment'].loc[num]=0\n",
    "df.to_csv('/content/drive/My Drive/mycolab/ob/trainset1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-flood",
   "metadata": {
    "id": "TTjQNQjDyBcG"
   },
   "outputs": [],
   "source": [
    "texts=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-projection",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Vt8sBTQzkyMQ",
    "outputId": "6a2f249f-0ffb-4c41-c542-8f5e95a107b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-bdf63d94d231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0;36m510\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-534da235f929>\u001b[0m in \u001b[0;36mget_text\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/newspaper/article.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, input_html, title, recursion_counter)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_html\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_html_2XX_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED_RESPONSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/newspaper/network.py\u001b[0m in \u001b[0;36mget_html_2XX_only\u001b[0;34m(url, config, response)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     response = requests.get(\n\u001b[0;32m---> 63\u001b[0;31m         url=url, **get_request_kwargs(timeout, useragent, proxies, headers))\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_html_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             ssl_context=context)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_fingerprint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mca_certs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Python 2.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "MAX_LEN=512\n",
    "df = pd.read_csv('/content/drive/My Drive/mycolab/ob/testset.csv')\n",
    "count = 0\n",
    "y_true = np.zeros(df.shape[0])\n",
    "a=0.55\n",
    "b=1-a\n",
    "for num in range(0,df.shape[0]):\n",
    "  print(num)\n",
    "  text = get_text(df['url'].loc[num])\n",
    "  texts.append(text)\n",
    "  if len(text)<=510:\n",
    "    score = predicttext(text)\n",
    "  elif len(text)>510:\n",
    "    text1=text[0:510]\n",
    "    text2=text[510:]\n",
    "    score1 = predicttext(text1)\n",
    "    score2 = predicttext(text2)\n",
    "    score = a*score1+b*score2\n",
    "  if score>0.5:\n",
    "    y_true[num] = 1\n",
    "y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-valley",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrmhsGl71PeH",
    "outputId": "25fca9d5-8be7-4cc2-a33a-28605e9e6ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "MAX_LEN=512\n",
    "df = pd.read_csv('/content/drive/My Drive/mycolab/ob/testset.csv')\n",
    "count = 0\n",
    "y_true = np.zeros(df.shape[0])\n",
    "a=0.9\n",
    "b=1-a\n",
    "for num in range(0,df.shape[0]):\n",
    "  print(num)\n",
    "  text = texts[num]\n",
    "  textlist=text.split(\" \")\n",
    "  if len(textlist)<=510:\n",
    "    text = \" \".join(textlist)\n",
    "    score = predicttext(text)\n",
    "  elif len(textlist)>510:\n",
    "    textlist1=textlist[0:510]\n",
    "    textlist2=textlist[510:]\n",
    "    text1 = \" \".join(textlist1)\n",
    "    text2 = \" \".join(textlist2)\n",
    "    score1 = predicttext(text1)\n",
    "    score2 = predicttext(text2)\n",
    "    score = a*score1+b*score2\n",
    "  if score>0.9:\n",
    "    y_true[num] = 1\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-queue",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SCr40Y2Ix5i2",
    "outputId": "d1b7fc9f-fa96-4687-98cd-7a99537f0fd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1  = np.zeros(250)\n",
    "y_test2 = np.ones(250)\n",
    "y_test = np.concatenate((y_test1,y_test2),axis=0)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-publication",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKvqUFmblVLd",
    "outputId": "5e262344-0fa1-424a-e262-f3ceebe582f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[191, 59, 22, 228]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "accuracy = accuracy_score(y_test,y_true)\n",
    "print(accuracy)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_true).ravel()\n",
    "[tn, fp, fn, tp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-commission",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIPhCRay4yMG",
    "outputId": "9880be39-b1b9-4e57-f38b-ac677a797782"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-exemption",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ieXQqB4_je2",
    "outputId": "be32df44-b0d7-483b-a9be-17bc136ace56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-fellowship",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-O3GshtB0du",
    "outputId": "1ecfbfe6-6a7a-4dfa-fe19-63376b468261"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test1  = np.zeros(250)\n",
    "y_test2 = np.ones(250)\n",
    "y_test = np.concatenate((y_test1,y_test2),axis=0)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-paragraph",
   "metadata": {
    "id": "BfmDooDOCmgh"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "y_test1  = np.zeros(250)\n",
    "y_test2 = np.ones(250)\n",
    "y_test = np.concatenate((y_test1,y_test2),axis=0)\n",
    "y_test.shape\n",
    "accuracy = accuracy_score(y_test,y_true)\n",
    "print(accuracy)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_true).ravel()\n",
    "[tn, fp, fn, tp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-gender",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9bQJLCrCxUc",
    "outputId": "363a5718-f261-44ab-9b77-9c9f60fb1a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[184, 66, 17, 233]"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test,y_true)\n",
    "print(accuracy)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test,y_true).ravel()\n",
    "[tn, fp, fn, tp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-ability",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_oazbfR00U2",
    "outputId": "f3799daa-6abf-4400-d2ee-3d2e999ed349"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.686129   |     -      |     -     |   9.49   \n",
      "   1    |   40    |   0.641000   |     -      |     -     |   9.03   \n",
      "   1    |   60    |   0.577771   |     -      |     -     |   9.03   \n",
      "   1    |   80    |   0.490160   |     -      |     -     |   9.03   \n",
      "   1    |   100   |   0.396287   |     -      |     -     |   9.03   \n",
      "   1    |   120   |   0.336325   |     -      |     -     |   9.03   \n",
      "   1    |   140   |   0.257129   |     -      |     -     |   9.03   \n",
      "   1    |   160   |   0.233346   |     -      |     -     |   9.04   \n",
      "   1    |   180   |   0.188133   |     -      |     -     |   9.01   \n",
      "   1    |   200   |   0.196943   |     -      |     -     |   9.03   \n",
      "   1    |   220   |   0.167022   |     -      |     -     |   9.02   \n",
      "   1    |   240   |   0.103183   |     -      |     -     |   9.00   \n",
      "   1    |   260   |   0.105340   |     -      |     -     |   9.01   \n",
      "   1    |   280   |   0.097270   |     -      |     -     |   9.02   \n",
      "   1    |   300   |   0.107000   |     -      |     -     |   9.02   \n",
      "   1    |   320   |   0.126636   |     -      |     -     |   9.00   \n",
      "   1    |   340   |   0.149007   |     -      |     -     |   9.00   \n",
      "   1    |   360   |   0.084270   |     -      |     -     |   9.00   \n",
      "   1    |   380   |   0.077761   |     -      |     -     |   8.99   \n",
      "   1    |   400   |   0.085037   |     -      |     -     |   9.00   \n",
      "   1    |   420   |   0.056016   |     -      |     -     |   9.00   \n",
      "   1    |   440   |   0.068805   |     -      |     -     |   9.00   \n",
      "   1    |   460   |   0.035908   |     -      |     -     |   8.99   \n",
      "   1    |   480   |   0.154260   |     -      |     -     |   9.01   \n",
      "   1    |   500   |   0.112282   |     -      |     -     |   9.00   \n",
      "   1    |   520   |   0.080174   |     -      |     -     |   9.00   \n",
      "   1    |   540   |   0.065955   |     -      |     -     |   9.00   \n",
      "   1    |   560   |   0.084578   |     -      |     -     |   9.01   \n",
      "   1    |   580   |   0.066452   |     -      |     -     |   9.01   \n",
      "   1    |   600   |   0.083134   |     -      |     -     |   9.00   \n",
      "   1    |   620   |   0.036838   |     -      |     -     |   8.99   \n",
      "   1    |   640   |   0.094901   |     -      |     -     |   8.99   \n",
      "   1    |   660   |   0.109189   |     -      |     -     |   8.99   \n",
      "   1    |   680   |   0.092625   |     -      |     -     |   8.99   \n",
      "   1    |   700   |   0.104480   |     -      |     -     |   9.00   \n",
      "   1    |   720   |   0.070048   |     -      |     -     |   9.01   \n",
      "   1    |   740   |   0.083807   |     -      |     -     |   9.00   \n",
      "   1    |   760   |   0.058167   |     -      |     -     |   8.98   \n",
      "   1    |   780   |   0.050896   |     -      |     -     |   8.99   \n",
      "   1    |   800   |   0.123397   |     -      |     -     |   8.99   \n",
      "   1    |   820   |   0.095905   |     -      |     -     |   8.99   \n",
      "   1    |   840   |   0.087884   |     -      |     -     |   8.99   \n",
      "   1    |   860   |   0.051999   |     -      |     -     |   9.00   \n",
      "   1    |   880   |   0.079090   |     -      |     -     |   9.01   \n",
      "   1    |   900   |   0.086970   |     -      |     -     |   9.00   \n",
      "   1    |   920   |   0.077494   |     -      |     -     |   8.99   \n",
      "   1    |   940   |   0.074063   |     -      |     -     |   9.00   \n",
      "   1    |   960   |   0.087686   |     -      |     -     |   9.02   \n",
      "   1    |   980   |   0.059062   |     -      |     -     |   9.00   \n",
      "   1    |  1000   |   0.119913   |     -      |     -     |   8.99   \n",
      "   1    |  1020   |   0.156961   |     -      |     -     |   8.99   \n",
      "   1    |  1040   |   0.129275   |     -      |     -     |   8.99   \n",
      "   1    |  1060   |   0.054277   |     -      |     -     |   9.00   \n",
      "   1    |  1080   |   0.093241   |     -      |     -     |   9.02   \n",
      "   1    |  1100   |   0.097252   |     -      |     -     |   9.00   \n",
      "   1    |  1120   |   0.036684   |     -      |     -     |   8.99   \n",
      "   1    |  1140   |   0.166325   |     -      |     -     |   8.99   \n",
      "   1    |  1160   |   0.103216   |     -      |     -     |   9.02   \n",
      "   1    |  1180   |   0.039492   |     -      |     -     |   8.98   \n",
      "   1    |  1200   |   0.117364   |     -      |     -     |   8.99   \n",
      "   1    |  1220   |   0.098924   |     -      |     -     |   9.01   \n",
      "   1    |  1240   |   0.132012   |     -      |     -     |   9.00   \n",
      "   1    |  1260   |   0.121998   |     -      |     -     |   9.01   \n",
      "   1    |  1280   |   0.062576   |     -      |     -     |   9.00   \n",
      "   1    |  1300   |   0.076365   |     -      |     -     |   8.98   \n",
      "   1    |  1320   |   0.079481   |     -      |     -     |   8.99   \n",
      "   1    |  1340   |   0.117283   |     -      |     -     |   8.99   \n",
      "   1    |  1360   |   0.108345   |     -      |     -     |   9.00   \n",
      "   1    |  1380   |   0.150992   |     -      |     -     |   9.00   \n",
      "   1    |  1400   |   0.146988   |     -      |     -     |   9.00   \n",
      "   1    |  1420   |   0.082002   |     -      |     -     |   8.99   \n",
      "   1    |  1440   |   0.047878   |     -      |     -     |   8.99   \n",
      "   1    |  1460   |   0.070502   |     -      |     -     |   8.99   \n",
      "   1    |  1480   |   0.096318   |     -      |     -     |   8.99   \n",
      "   1    |  1500   |   0.099563   |     -      |     -     |   8.99   \n",
      "   1    |  1520   |   0.055405   |     -      |     -     |   9.01   \n",
      "   1    |  1540   |   0.085061   |     -      |     -     |   9.00   \n",
      "   1    |  1560   |   0.078003   |     -      |     -     |   9.00   \n",
      "   1    |  1580   |   0.104307   |     -      |     -     |   9.01   \n",
      "   1    |  1600   |   0.057704   |     -      |     -     |   8.99   \n",
      "   1    |  1620   |   0.077053   |     -      |     -     |   9.01   \n",
      "   1    |  1640   |   0.180917   |     -      |     -     |   8.99   \n",
      "   1    |  1660   |   0.021145   |     -      |     -     |   8.99   \n",
      "   1    |  1680   |   0.113594   |     -      |     -     |   8.99   \n",
      "   1    |  1700   |   0.093527   |     -      |     -     |   9.00   \n",
      "   1    |  1720   |   0.036186   |     -      |     -     |   8.99   \n",
      "   1    |  1740   |   0.061801   |     -      |     -     |   9.01   \n",
      "   1    |  1760   |   0.094858   |     -      |     -     |   9.00   \n",
      "   1    |  1780   |   0.151464   |     -      |     -     |   9.01   \n",
      "   1    |  1800   |   0.112417   |     -      |     -     |   8.99   \n",
      "   1    |  1820   |   0.108944   |     -      |     -     |   8.99   \n",
      "   1    |  1840   |   0.125321   |     -      |     -     |   8.99   \n",
      "   1    |  1860   |   0.010832   |     -      |     -     |   8.97   \n",
      "   1    |  1880   |   0.069501   |     -      |     -     |   9.01   \n",
      "   1    |  1900   |   0.110683   |     -      |     -     |   9.00   \n",
      "   1    |  1920   |   0.057505   |     -      |     -     |   9.01   \n",
      "   1    |  1940   |   0.010579   |     -      |     -     |   8.98   \n",
      "   1    |  1960   |   0.019926   |     -      |     -     |   8.99   \n",
      "   1    |  1980   |   0.080325   |     -      |     -     |   8.98   \n",
      "   1    |  2000   |   0.104676   |     -      |     -     |   8.99   \n",
      "   1    |  2020   |   0.092726   |     -      |     -     |   8.99   \n",
      "   1    |  2040   |   0.148856   |     -      |     -     |   9.00   \n",
      "   1    |  2060   |   0.073147   |     -      |     -     |   8.99   \n",
      "   1    |  2080   |   0.041594   |     -      |     -     |   8.99   \n",
      "   1    |  2100   |   0.100109   |     -      |     -     |   8.99   \n",
      "   1    |  2120   |   0.017792   |     -      |     -     |   8.99   \n",
      "   1    |  2140   |   0.075939   |     -      |     -     |   8.99   \n",
      "   1    |  2160   |   0.040423   |     -      |     -     |   8.99   \n",
      "   1    |  2180   |   0.174051   |     -      |     -     |   9.01   \n",
      "   1    |  2200   |   0.046515   |     -      |     -     |   9.00   \n",
      "   1    |  2220   |   0.078409   |     -      |     -     |   9.00   \n",
      "   1    |  2240   |   0.111115   |     -      |     -     |   9.00   \n",
      "   1    |  2260   |   0.161873   |     -      |     -     |   8.99   \n",
      "   1    |  2280   |   0.028734   |     -      |     -     |   8.98   \n",
      "   1    |  2300   |   0.129982   |     -      |     -     |   8.99   \n",
      "   1    |  2320   |   0.089350   |     -      |     -     |   9.00   \n",
      "   1    |  2340   |   0.064923   |     -      |     -     |   8.99   \n",
      "   1    |  2360   |   0.107486   |     -      |     -     |   9.00   \n",
      "   1    |  2380   |   0.053395   |     -      |     -     |   8.99   \n",
      "   1    |  2400   |   0.048886   |     -      |     -     |   9.00   \n",
      "   1    |  2420   |   0.017782   |     -      |     -     |   9.00   \n",
      "   1    |  2440   |   0.040274   |     -      |     -     |   8.99   \n",
      "   1    |  2460   |   0.047757   |     -      |     -     |   8.98   \n",
      "   1    |  2480   |   0.056061   |     -      |     -     |   8.99   \n",
      "   1    |  2500   |   0.040365   |     -      |     -     |   8.99   \n",
      "   1    |  2520   |   0.007386   |     -      |     -     |   8.99   \n",
      "   1    |  2540   |   0.004290   |     -      |     -     |   9.00   \n",
      "   1    |  2560   |   0.066331   |     -      |     -     |   9.00   \n",
      "   1    |  2580   |   0.062726   |     -      |     -     |   9.00   \n",
      "   1    |  2600   |   0.153661   |     -      |     -     |   8.97   \n",
      "   1    |  2620   |   0.058800   |     -      |     -     |   8.98   \n",
      "   1    |  2640   |   0.081527   |     -      |     -     |   8.98   \n",
      "   1    |  2660   |   0.116793   |     -      |     -     |   8.98   \n",
      "   1    |  2680   |   0.065369   |     -      |     -     |   9.00   \n",
      "   1    |  2700   |   0.066691   |     -      |     -     |   9.00   \n",
      "   1    |  2720   |   0.089852   |     -      |     -     |   8.99   \n",
      "   1    |  2740   |   0.086616   |     -      |     -     |   9.00   \n",
      "   1    |  2760   |   0.060017   |     -      |     -     |   8.98   \n",
      "   1    |  2780   |   0.101311   |     -      |     -     |   8.99   \n",
      "   1    |  2800   |   0.009200   |     -      |     -     |   8.97   \n",
      "   1    |  2820   |   0.003771   |     -      |     -     |   8.98   \n",
      "   1    |  2840   |   0.010566   |     -      |     -     |   9.00   \n",
      "   1    |  2860   |   0.038690   |     -      |     -     |   8.99   \n",
      "   1    |  2880   |   0.025413   |     -      |     -     |   8.98   \n",
      "   1    |  2900   |   0.106639   |     -      |     -     |   8.98   \n",
      "   1    |  2920   |   0.075490   |     -      |     -     |   8.99   \n",
      "   1    |  2940   |   0.095638   |     -      |     -     |   8.99   \n",
      "   1    |  2960   |   0.064777   |     -      |     -     |   8.98   \n",
      "   1    |  2980   |   0.045592   |     -      |     -     |   9.00   \n",
      "   1    |  3000   |   0.149397   |     -      |     -     |   9.00   \n",
      "   1    |  3020   |   0.003652   |     -      |     -     |   8.99   \n",
      "   1    |  3040   |   0.018569   |     -      |     -     |   8.98   \n",
      "   1    |  3060   |   0.041664   |     -      |     -     |   8.99   \n",
      "   1    |  3080   |   0.139510   |     -      |     -     |   9.00   \n",
      "   1    |  3100   |   0.070028   |     -      |     -     |   8.99   \n",
      "   1    |  3120   |   0.026808   |     -      |     -     |   8.98   \n",
      "   1    |  3140   |   0.054540   |     -      |     -     |   8.98   \n",
      "   1    |  3160   |   0.003252   |     -      |     -     |   8.99   \n",
      "   1    |  3180   |   0.118931   |     -      |     -     |   8.98   \n",
      "   1    |  3200   |   0.140445   |     -      |     -     |   8.99   \n",
      "   1    |  3220   |   0.088539   |     -      |     -     |   8.99   \n",
      "   1    |  3240   |   0.073244   |     -      |     -     |   8.99   \n",
      "   1    |  3260   |   0.163378   |     -      |     -     |   8.99   \n",
      "   1    |  3280   |   0.056844   |     -      |     -     |   8.98   \n",
      "   1    |  3300   |   0.138538   |     -      |     -     |   8.97   \n",
      "   1    |  3320   |   0.066643   |     -      |     -     |   8.98   \n",
      "   1    |  3340   |   0.028150   |     -      |     -     |   8.98   \n",
      "   1    |  3360   |   0.022387   |     -      |     -     |   8.99   \n",
      "   1    |  3369   |   0.196937   |     -      |     -     |   3.79   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.100753   |  0.040933  |   98.87   |  1570.70 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.022081   |     -      |     -     |   9.44   \n",
      "   2    |   40    |   0.088440   |     -      |     -     |   8.99   \n",
      "   2    |   60    |   0.020213   |     -      |     -     |   8.98   \n",
      "   2    |   80    |   0.004067   |     -      |     -     |   8.97   \n",
      "   2    |   100   |   0.047623   |     -      |     -     |   8.98   \n",
      "   2    |   120   |   0.064650   |     -      |     -     |   8.98   \n",
      "   2    |   140   |   0.186738   |     -      |     -     |   8.99   \n",
      "   2    |   160   |   0.055180   |     -      |     -     |   8.99   \n",
      "   2    |   180   |   0.042590   |     -      |     -     |   8.99   \n",
      "   2    |   200   |   0.043128   |     -      |     -     |   8.99   \n",
      "   2    |   220   |   0.070823   |     -      |     -     |   8.99   \n",
      "   2    |   240   |   0.012554   |     -      |     -     |   8.99   \n",
      "   2    |   260   |   0.013238   |     -      |     -     |   8.99   \n",
      "   2    |   280   |   0.020461   |     -      |     -     |   8.98   \n",
      "   2    |   300   |   0.034907   |     -      |     -     |   8.98   \n",
      "   2    |   320   |   0.048190   |     -      |     -     |   8.98   \n",
      "   2    |   340   |   0.078553   |     -      |     -     |   8.99   \n",
      "   2    |   360   |   0.053804   |     -      |     -     |   8.97   \n",
      "   2    |   380   |   0.001740   |     -      |     -     |   8.98   \n",
      "   2    |   400   |   0.001878   |     -      |     -     |   8.98   \n",
      "   2    |   420   |   0.001584   |     -      |     -     |   8.98   \n",
      "   2    |   440   |   0.031475   |     -      |     -     |   8.97   \n",
      "   2    |   460   |   0.071333   |     -      |     -     |   8.99   \n",
      "   2    |   480   |   0.060465   |     -      |     -     |   8.97   \n",
      "   2    |   500   |   0.014067   |     -      |     -     |   8.97   \n",
      "   2    |   520   |   0.043698   |     -      |     -     |   8.99   \n",
      "   2    |   540   |   0.078244   |     -      |     -     |   8.99   \n",
      "   2    |   560   |   0.044204   |     -      |     -     |   8.99   \n",
      "   2    |   580   |   0.110020   |     -      |     -     |   8.97   \n",
      "   2    |   600   |   0.032450   |     -      |     -     |   8.97   \n",
      "   2    |   620   |   0.052800   |     -      |     -     |   8.97   \n",
      "   2    |   640   |   0.042069   |     -      |     -     |   8.97   \n",
      "   2    |   660   |   0.075299   |     -      |     -     |   8.97   \n",
      "   2    |   680   |   0.001900   |     -      |     -     |   8.99   \n",
      "   2    |   700   |   0.070845   |     -      |     -     |   9.00   \n",
      "   2    |   720   |   0.073430   |     -      |     -     |   8.98   \n",
      "   2    |   740   |   0.002553   |     -      |     -     |   8.97   \n",
      "   2    |   760   |   0.070681   |     -      |     -     |   8.97   \n",
      "   2    |   780   |   0.110282   |     -      |     -     |   8.98   \n",
      "   2    |   800   |   0.012048   |     -      |     -     |   8.98   \n",
      "   2    |   820   |   0.071445   |     -      |     -     |   8.98   \n",
      "   2    |   840   |   0.080146   |     -      |     -     |   8.99   \n",
      "   2    |   860   |   0.079520   |     -      |     -     |   8.98   \n",
      "   2    |   880   |   0.016658   |     -      |     -     |   8.98   \n",
      "   2    |   900   |   0.027014   |     -      |     -     |   8.98   \n",
      "   2    |   920   |   0.070517   |     -      |     -     |   8.97   \n",
      "   2    |   940   |   0.037823   |     -      |     -     |   8.97   \n",
      "   2    |   960   |   0.062899   |     -      |     -     |   8.99   \n",
      "   2    |   980   |   0.043586   |     -      |     -     |   8.98   \n",
      "   2    |  1000   |   0.018689   |     -      |     -     |   8.98   \n",
      "   2    |  1020   |   0.049236   |     -      |     -     |   8.98   \n",
      "   2    |  1040   |   0.101318   |     -      |     -     |   8.99   \n",
      "   2    |  1060   |   0.038488   |     -      |     -     |   8.99   \n",
      "   2    |  1080   |   0.118870   |     -      |     -     |   8.98   \n",
      "   2    |  1100   |   0.001299   |     -      |     -     |   8.97   \n",
      "   2    |  1120   |   0.028386   |     -      |     -     |   8.98   \n",
      "   2    |  1140   |   0.067885   |     -      |     -     |   8.98   \n",
      "   2    |  1160   |   0.007039   |     -      |     -     |   8.97   \n",
      "   2    |  1180   |   0.110808   |     -      |     -     |   8.98   \n",
      "   2    |  1200   |   0.037166   |     -      |     -     |   8.99   \n",
      "   2    |  1220   |   0.044499   |     -      |     -     |   8.98   \n",
      "   2    |  1240   |   0.038313   |     -      |     -     |   8.96   \n",
      "   2    |  1260   |   0.003547   |     -      |     -     |   8.97   \n",
      "   2    |  1280   |   0.040681   |     -      |     -     |   8.98   \n",
      "   2    |  1300   |   0.001419   |     -      |     -     |   8.97   \n",
      "   2    |  1320   |   0.001141   |     -      |     -     |   8.97   \n",
      "   2    |  1340   |   0.036279   |     -      |     -     |   8.99   \n",
      "   2    |  1360   |   0.042159   |     -      |     -     |   8.99   \n",
      "   2    |  1380   |   0.128956   |     -      |     -     |   8.99   \n",
      "   2    |  1400   |   0.016994   |     -      |     -     |   8.97   \n",
      "   2    |  1420   |   0.072493   |     -      |     -     |   8.98   \n",
      "   2    |  1440   |   0.023894   |     -      |     -     |   8.98   \n",
      "   2    |  1460   |   0.074004   |     -      |     -     |   8.98   \n",
      "   2    |  1480   |   0.027639   |     -      |     -     |   8.99   \n",
      "   2    |  1500   |   0.041881   |     -      |     -     |   8.98   \n",
      "   2    |  1520   |   0.014651   |     -      |     -     |   8.98   \n",
      "   2    |  1540   |   0.006759   |     -      |     -     |   8.97   \n",
      "   2    |  1560   |   0.002044   |     -      |     -     |   8.97   \n",
      "   2    |  1580   |   0.016019   |     -      |     -     |   8.98   \n",
      "   2    |  1600   |   0.039388   |     -      |     -     |   8.98   \n",
      "   2    |  1620   |   0.036033   |     -      |     -     |   8.98   \n",
      "   2    |  1640   |   0.009030   |     -      |     -     |   8.99   \n",
      "   2    |  1660   |   0.079324   |     -      |     -     |   8.99   \n",
      "   2    |  1680   |   0.061355   |     -      |     -     |   8.98   \n",
      "   2    |  1700   |   0.080002   |     -      |     -     |   8.99   \n",
      "   2    |  1720   |   0.007194   |     -      |     -     |   8.99   \n",
      "   2    |  1740   |   0.012766   |     -      |     -     |   8.98   \n",
      "   2    |  1760   |   0.043522   |     -      |     -     |   8.97   \n",
      "   2    |  1780   |   0.017565   |     -      |     -     |   8.97   \n",
      "   2    |  1800   |   0.077309   |     -      |     -     |   8.98   \n",
      "   2    |  1820   |   0.000997   |     -      |     -     |   8.97   \n",
      "   2    |  1840   |   0.041416   |     -      |     -     |   8.98   \n",
      "   2    |  1860   |   0.041091   |     -      |     -     |   8.99   \n",
      "   2    |  1880   |   0.044547   |     -      |     -     |   8.99   \n",
      "   2    |  1900   |   0.016286   |     -      |     -     |   8.98   \n",
      "   2    |  1920   |   0.035477   |     -      |     -     |   8.98   \n",
      "   2    |  1940   |   0.003456   |     -      |     -     |   8.97   \n",
      "   2    |  1960   |   0.004585   |     -      |     -     |   8.98   \n",
      "   2    |  1980   |   0.066545   |     -      |     -     |   8.99   \n",
      "   2    |  2000   |   0.071266   |     -      |     -     |   8.99   \n",
      "   2    |  2020   |   0.071467   |     -      |     -     |   8.99   \n",
      "   2    |  2040   |   0.042113   |     -      |     -     |   8.98   \n",
      "   2    |  2060   |   0.144788   |     -      |     -     |   8.98   \n",
      "   2    |  2080   |   0.039048   |     -      |     -     |   8.98   \n",
      "   2    |  2100   |   0.006345   |     -      |     -     |   8.97   \n",
      "   2    |  2120   |   0.058757   |     -      |     -     |   8.97   \n",
      "   2    |  2140   |   0.129584   |     -      |     -     |   9.00   \n",
      "   2    |  2160   |   0.001105   |     -      |     -     |   8.98   \n",
      "   2    |  2180   |   0.000952   |     -      |     -     |   8.98   \n",
      "   2    |  2200   |   0.054559   |     -      |     -     |   8.98   \n",
      "   2    |  2220   |   0.032591   |     -      |     -     |   8.96   \n",
      "   2    |  2240   |   0.008231   |     -      |     -     |   8.97   \n",
      "   2    |  2260   |   0.028479   |     -      |     -     |   8.97   \n",
      "   2    |  2280   |   0.001673   |     -      |     -     |   8.97   \n",
      "   2    |  2300   |   0.001221   |     -      |     -     |   8.97   \n",
      "   2    |  2320   |   0.003420   |     -      |     -     |   8.98   \n",
      "   2    |  2340   |   0.042510   |     -      |     -     |   8.99   \n",
      "   2    |  2360   |   0.030555   |     -      |     -     |   8.96   \n",
      "   2    |  2380   |   0.050128   |     -      |     -     |   8.97   \n",
      "   2    |  2400   |   0.079932   |     -      |     -     |   8.97   \n",
      "   2    |  2420   |   0.019101   |     -      |     -     |   8.97   \n",
      "   2    |  2440   |   0.012436   |     -      |     -     |   8.98   \n",
      "   2    |  2460   |   0.053287   |     -      |     -     |   8.98   \n",
      "   2    |  2480   |   0.080079   |     -      |     -     |   8.97   \n",
      "   2    |  2500   |   0.004665   |     -      |     -     |   8.97   \n",
      "   2    |  2520   |   0.154171   |     -      |     -     |   8.98   \n",
      "   2    |  2540   |   0.037160   |     -      |     -     |   8.98   \n",
      "   2    |  2560   |   0.014387   |     -      |     -     |   8.97   \n",
      "   2    |  2580   |   0.058389   |     -      |     -     |   8.97   \n",
      "   2    |  2600   |   0.000798   |     -      |     -     |   8.96   \n",
      "   2    |  2620   |   0.048179   |     -      |     -     |   8.97   \n",
      "   2    |  2640   |   0.064778   |     -      |     -     |   8.97   \n",
      "   2    |  2660   |   0.047430   |     -      |     -     |   8.99   \n",
      "   2    |  2680   |   0.023205   |     -      |     -     |   8.97   \n",
      "   2    |  2700   |   0.127049   |     -      |     -     |   8.97   \n",
      "   2    |  2720   |   0.052111   |     -      |     -     |   8.96   \n",
      "   2    |  2740   |   0.045346   |     -      |     -     |   8.99   \n",
      "   2    |  2760   |   0.031148   |     -      |     -     |   8.96   \n",
      "   2    |  2780   |   0.043630   |     -      |     -     |   8.96   \n",
      "   2    |  2800   |   0.028330   |     -      |     -     |   8.98   \n",
      "   2    |  2820   |   0.027397   |     -      |     -     |   8.98   \n",
      "   2    |  2840   |   0.059705   |     -      |     -     |   8.98   \n",
      "   2    |  2860   |   0.019400   |     -      |     -     |   8.96   \n",
      "   2    |  2880   |   0.104536   |     -      |     -     |   8.97   \n",
      "   2    |  2900   |   0.015862   |     -      |     -     |   8.96   \n",
      "   2    |  2920   |   0.029099   |     -      |     -     |   8.96   \n",
      "   2    |  2940   |   0.032058   |     -      |     -     |   8.98   \n",
      "   2    |  2960   |   0.037696   |     -      |     -     |   8.97   \n",
      "   2    |  2980   |   0.036474   |     -      |     -     |   8.97   \n",
      "   2    |  3000   |   0.034428   |     -      |     -     |   8.97   \n",
      "   2    |  3020   |   0.001448   |     -      |     -     |   8.98   \n",
      "   2    |  3040   |   0.030964   |     -      |     -     |   8.96   \n",
      "   2    |  3060   |   0.060039   |     -      |     -     |   8.97   \n",
      "   2    |  3080   |   0.001815   |     -      |     -     |   8.97   \n",
      "   2    |  3100   |   0.053786   |     -      |     -     |   8.98   \n",
      "   2    |  3120   |   0.084586   |     -      |     -     |   8.98   \n",
      "   2    |  3140   |   0.065330   |     -      |     -     |   8.98   \n",
      "   2    |  3160   |   0.046016   |     -      |     -     |   8.98   \n",
      "   2    |  3180   |   0.009340   |     -      |     -     |   8.98   \n",
      "   2    |  3200   |   0.033412   |     -      |     -     |   8.98   \n",
      "   2    |  3220   |   0.032958   |     -      |     -     |   8.97   \n",
      "   2    |  3240   |   0.049294   |     -      |     -     |   8.96   \n",
      "   2    |  3260   |   0.042297   |     -      |     -     |   8.97   \n",
      "   2    |  3280   |   0.043687   |     -      |     -     |   8.97   \n",
      "   2    |  3300   |   0.002032   |     -      |     -     |   8.96   \n",
      "   2    |  3320   |   0.001834   |     -      |     -     |   8.98   \n",
      "   2    |  3340   |   0.015212   |     -      |     -     |   8.98   \n",
      "   2    |  3360   |   0.037025   |     -      |     -     |   8.97   \n",
      "   2    |  3369   |   0.001746   |     -      |     -     |   3.78   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.042637   |  0.047382  |   99.00   |  1567.51 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.095810   |     -      |     -     |   9.42   \n",
      "   3    |   40    |   0.003715   |     -      |     -     |   8.96   \n",
      "   3    |   60    |   0.000727   |     -      |     -     |   8.96   \n",
      "   3    |   80    |   0.000742   |     -      |     -     |   8.97   \n",
      "   3    |   100   |   0.012698   |     -      |     -     |   8.96   \n",
      "   3    |   120   |   0.041625   |     -      |     -     |   8.98   \n",
      "   3    |   140   |   0.000687   |     -      |     -     |   8.98   \n",
      "   3    |   160   |   0.000718   |     -      |     -     |   8.98   \n",
      "   3    |   180   |   0.005665   |     -      |     -     |   8.98   \n",
      "   3    |   200   |   0.000715   |     -      |     -     |   8.97   \n",
      "   3    |   220   |   0.018919   |     -      |     -     |   8.97   \n",
      "   3    |   240   |   0.024965   |     -      |     -     |   8.97   \n",
      "   3    |   260   |   0.000776   |     -      |     -     |   8.98   \n",
      "   3    |   280   |   0.035121   |     -      |     -     |   8.98   \n",
      "   3    |   300   |   0.018445   |     -      |     -     |   8.99   \n",
      "   3    |   320   |   0.011009   |     -      |     -     |   8.98   \n",
      "   3    |   340   |   0.021642   |     -      |     -     |   8.97   \n",
      "   3    |   360   |   0.000730   |     -      |     -     |   8.97   \n",
      "   3    |   380   |   0.000614   |     -      |     -     |   8.97   \n",
      "   3    |   400   |   0.023823   |     -      |     -     |   8.98   \n",
      "   3    |   420   |   0.037526   |     -      |     -     |   8.98   \n",
      "   3    |   440   |   0.037344   |     -      |     -     |   8.99   \n",
      "   3    |   460   |   0.005694   |     -      |     -     |   8.98   \n",
      "   3    |   480   |   0.000638   |     -      |     -     |   8.97   \n",
      "   3    |   500   |   0.011455   |     -      |     -     |   8.99   \n",
      "   3    |   520   |   0.030967   |     -      |     -     |   8.99   \n",
      "   3    |   540   |   0.007584   |     -      |     -     |   8.98   \n",
      "   3    |   560   |   0.019043   |     -      |     -     |   8.97   \n",
      "   3    |   580   |   0.015945   |     -      |     -     |   8.97   \n",
      "   3    |   600   |   0.068493   |     -      |     -     |   8.97   \n",
      "   3    |   620   |   0.012584   |     -      |     -     |   8.98   \n",
      "   3    |   640   |   0.000673   |     -      |     -     |   8.98   \n",
      "   3    |   660   |   0.001374   |     -      |     -     |   8.98   \n",
      "   3    |   680   |   0.039532   |     -      |     -     |   8.98   \n",
      "   3    |   700   |   0.041385   |     -      |     -     |   8.97   \n",
      "   3    |   720   |   0.075666   |     -      |     -     |   8.98   \n",
      "   3    |   740   |   0.030929   |     -      |     -     |   8.97   \n",
      "   3    |   760   |   0.003191   |     -      |     -     |   8.96   \n",
      "   3    |   780   |   0.000620   |     -      |     -     |   8.98   \n",
      "   3    |   800   |   0.001633   |     -      |     -     |   8.99   \n",
      "   3    |   820   |   0.039896   |     -      |     -     |   8.98   \n",
      "   3    |   840   |   0.024407   |     -      |     -     |   8.97   \n",
      "   3    |   860   |   0.000595   |     -      |     -     |   8.98   \n",
      "   3    |   880   |   0.000751   |     -      |     -     |   8.97   \n",
      "   3    |   900   |   0.000894   |     -      |     -     |   8.97   \n",
      "   3    |   920   |   0.000718   |     -      |     -     |   8.98   \n",
      "   3    |   940   |   0.001238   |     -      |     -     |   8.98   \n",
      "   3    |   960   |   0.000577   |     -      |     -     |   8.98   \n",
      "   3    |   980   |   0.013626   |     -      |     -     |   8.98   \n",
      "   3    |  1000   |   0.056195   |     -      |     -     |   8.98   \n",
      "   3    |  1020   |   0.025844   |     -      |     -     |   8.97   \n",
      "   3    |  1040   |   0.038627   |     -      |     -     |   8.97   \n",
      "   3    |  1060   |   0.001227   |     -      |     -     |   8.97   \n",
      "   3    |  1080   |   0.000537   |     -      |     -     |   8.98   \n",
      "   3    |  1100   |   0.028765   |     -      |     -     |   8.98   \n",
      "   3    |  1120   |   0.037368   |     -      |     -     |   8.99   \n",
      "   3    |  1140   |   0.000579   |     -      |     -     |   8.97   \n",
      "   3    |  1160   |   0.001422   |     -      |     -     |   8.99   \n",
      "   3    |  1180   |   0.062071   |     -      |     -     |   8.98   \n",
      "   3    |  1200   |   0.044171   |     -      |     -     |   8.97   \n",
      "   3    |  1220   |   0.000967   |     -      |     -     |   8.96   \n",
      "   3    |  1240   |   0.002675   |     -      |     -     |   8.97   \n",
      "   3    |  1260   |   0.009166   |     -      |     -     |   8.97   \n",
      "   3    |  1280   |   0.023421   |     -      |     -     |   8.97   \n",
      "   3    |  1300   |   0.000532   |     -      |     -     |   8.99   \n",
      "   3    |  1320   |   0.042415   |     -      |     -     |   8.98   \n",
      "   3    |  1340   |   0.000521   |     -      |     -     |   8.97   \n",
      "   3    |  1360   |   0.047242   |     -      |     -     |   8.97   \n",
      "   3    |  1380   |   0.000692   |     -      |     -     |   8.98   \n",
      "   3    |  1400   |   0.046875   |     -      |     -     |   8.98   \n",
      "   3    |  1420   |   0.000561   |     -      |     -     |   8.96   \n",
      "   3    |  1440   |   0.004813   |     -      |     -     |   8.98   \n",
      "   3    |  1460   |   0.000518   |     -      |     -     |   8.98   \n",
      "   3    |  1480   |   0.000513   |     -      |     -     |   8.98   \n",
      "   3    |  1500   |   0.044053   |     -      |     -     |   8.97   \n",
      "   3    |  1520   |   0.029812   |     -      |     -     |   8.97   \n",
      "   3    |  1540   |   0.054158   |     -      |     -     |   8.97   \n",
      "   3    |  1560   |   0.000566   |     -      |     -     |   8.97   \n",
      "   3    |  1580   |   0.001693   |     -      |     -     |   8.98   \n",
      "   3    |  1600   |   0.043129   |     -      |     -     |   8.98   \n",
      "   3    |  1620   |   0.034960   |     -      |     -     |   8.99   \n",
      "   3    |  1640   |   0.083119   |     -      |     -     |   8.99   \n",
      "   3    |  1660   |   0.072935   |     -      |     -     |   8.98   \n",
      "   3    |  1680   |   0.006342   |     -      |     -     |   8.98   \n",
      "   3    |  1700   |   0.000516   |     -      |     -     |   8.97   \n",
      "   3    |  1720   |   0.044620   |     -      |     -     |   8.97   \n",
      "   3    |  1740   |   0.017194   |     -      |     -     |   8.98   \n",
      "   3    |  1760   |   0.000532   |     -      |     -     |   8.98   \n",
      "   3    |  1780   |   0.076729   |     -      |     -     |   8.99   \n",
      "   3    |  1800   |   0.000511   |     -      |     -     |   8.97   \n",
      "   3    |  1820   |   0.048483   |     -      |     -     |   8.98   \n",
      "   3    |  1840   |   0.005337   |     -      |     -     |   8.96   \n",
      "   3    |  1860   |   0.000551   |     -      |     -     |   8.97   \n",
      "   3    |  1880   |   0.000589   |     -      |     -     |   8.98   \n",
      "   3    |  1900   |   0.001026   |     -      |     -     |   8.98   \n",
      "   3    |  1920   |   0.011642   |     -      |     -     |   8.99   \n",
      "   3    |  1940   |   0.002280   |     -      |     -     |   8.97   \n",
      "   3    |  1960   |   0.061905   |     -      |     -     |   8.98   \n",
      "   3    |  1980   |   0.003028   |     -      |     -     |   8.99   \n",
      "   3    |  2000   |   0.003132   |     -      |     -     |   8.99   \n",
      "   3    |  2020   |   0.011443   |     -      |     -     |   8.97   \n",
      "   3    |  2040   |   0.000682   |     -      |     -     |   8.97   \n",
      "   3    |  2060   |   0.000583   |     -      |     -     |   8.97   \n",
      "   3    |  2080   |   0.006057   |     -      |     -     |   8.97   \n",
      "   3    |  2100   |   0.000501   |     -      |     -     |   8.97   \n",
      "   3    |  2120   |   0.000767   |     -      |     -     |   8.98   \n",
      "   3    |  2140   |   0.071919   |     -      |     -     |   8.98   \n",
      "   3    |  2160   |   0.000545   |     -      |     -     |   8.97   \n",
      "   3    |  2180   |   0.000520   |     -      |     -     |   8.97   \n",
      "   3    |  2200   |   0.046681   |     -      |     -     |   8.97   \n",
      "   3    |  2220   |   0.000780   |     -      |     -     |   8.97   \n",
      "   3    |  2240   |   0.066996   |     -      |     -     |   8.98   \n",
      "   3    |  2260   |   0.001150   |     -      |     -     |   8.98   \n",
      "   3    |  2280   |   0.007871   |     -      |     -     |   8.98   \n",
      "   3    |  2300   |   0.000520   |     -      |     -     |   8.98   \n",
      "   3    |  2320   |   0.000644   |     -      |     -     |   8.97   \n",
      "   3    |  2340   |   0.000825   |     -      |     -     |   8.97   \n",
      "   3    |  2360   |   0.000591   |     -      |     -     |   8.96   \n",
      "   3    |  2380   |   0.000484   |     -      |     -     |   8.95   \n",
      "   3    |  2400   |   0.000497   |     -      |     -     |   8.96   \n",
      "   3    |  2420   |   0.051032   |     -      |     -     |   8.97   \n",
      "   3    |  2440   |   0.000494   |     -      |     -     |   8.97   \n",
      "   3    |  2460   |   0.015461   |     -      |     -     |   8.97   \n",
      "   3    |  2480   |   0.011130   |     -      |     -     |   8.97   \n",
      "   3    |  2500   |   0.000516   |     -      |     -     |   8.97   \n",
      "   3    |  2520   |   0.029542   |     -      |     -     |   8.97   \n",
      "   3    |  2540   |   0.000535   |     -      |     -     |   8.97   \n",
      "   3    |  2560   |   0.001283   |     -      |     -     |   8.98   \n",
      "   3    |  2580   |   0.000534   |     -      |     -     |   8.98   \n",
      "   3    |  2600   |   0.002238   |     -      |     -     |   8.97   \n",
      "   3    |  2620   |   0.021319   |     -      |     -     |   8.98   \n",
      "   3    |  2640   |   0.000681   |     -      |     -     |   8.98   \n",
      "   3    |  2660   |   0.046347   |     -      |     -     |   8.98   \n",
      "   3    |  2680   |   0.082248   |     -      |     -     |   8.97   \n",
      "   3    |  2700   |   0.000479   |     -      |     -     |   8.96   \n",
      "   3    |  2720   |   0.000521   |     -      |     -     |   8.96   \n",
      "   3    |  2740   |   0.000499   |     -      |     -     |   8.97   \n",
      "   3    |  2760   |   0.035752   |     -      |     -     |   8.97   \n",
      "   3    |  2780   |   0.000472   |     -      |     -     |   8.98   \n",
      "   3    |  2800   |   0.041263   |     -      |     -     |   8.98   \n",
      "   3    |  2820   |   0.000535   |     -      |     -     |   8.97   \n",
      "   3    |  2840   |   0.000605   |     -      |     -     |   8.97   \n",
      "   3    |  2860   |   0.018383   |     -      |     -     |   8.98   \n",
      "   3    |  2880   |   0.000557   |     -      |     -     |   8.96   \n",
      "   3    |  2900   |   0.050011   |     -      |     -     |   8.97   \n",
      "   3    |  2920   |   0.014899   |     -      |     -     |   8.98   \n",
      "   3    |  2940   |   0.077257   |     -      |     -     |   8.98   \n",
      "   3    |  2960   |   0.000469   |     -      |     -     |   8.97   \n",
      "   3    |  2980   |   0.047857   |     -      |     -     |   8.97   \n",
      "   3    |  3000   |   0.000622   |     -      |     -     |   8.97   \n",
      "   3    |  3020   |   0.000464   |     -      |     -     |   8.97   \n",
      "   3    |  3040   |   0.000471   |     -      |     -     |   8.96   \n",
      "   3    |  3060   |   0.000955   |     -      |     -     |   8.97   \n",
      "   3    |  3080   |   0.037653   |     -      |     -     |   8.98   \n",
      "   3    |  3100   |   0.023493   |     -      |     -     |   8.98   \n",
      "   3    |  3120   |   0.003236   |     -      |     -     |   8.97   \n",
      "   3    |  3140   |   0.000489   |     -      |     -     |   8.97   \n",
      "   3    |  3160   |   0.007854   |     -      |     -     |   8.97   \n",
      "   3    |  3180   |   0.002415   |     -      |     -     |   8.97   \n",
      "   3    |  3200   |   0.002129   |     -      |     -     |   8.97   \n",
      "   3    |  3220   |   0.000570   |     -      |     -     |   8.98   \n",
      "   3    |  3240   |   0.044061   |     -      |     -     |   8.98   \n",
      "   3    |  3260   |   0.038422   |     -      |     -     |   8.98   \n",
      "   3    |  3280   |   0.000477   |     -      |     -     |   8.97   \n",
      "   3    |  3300   |   0.000485   |     -      |     -     |   8.98   \n",
      "   3    |  3320   |   0.009399   |     -      |     -     |   8.98   \n",
      "   3    |  3340   |   0.000756   |     -      |     -     |   8.97   \n",
      "   3    |  3360   |   0.009907   |     -      |     -     |   8.97   \n",
      "   3    |  3369   |   0.000510   |     -      |     -     |   3.78   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.017404   |  0.033270  |   99.33   |  1567.10 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=3,lr=3e-6)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/bert_model1.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-importance",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xo5_IHryGtcB",
    "outputId": "5e9ff8f0-308e-465d-ebe2-ced2fe948d4a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.679226   |     -      |     -     |   9.51   \n",
      "   1    |   40    |   0.638924   |     -      |     -     |   9.03   \n",
      "   1    |   60    |   0.566960   |     -      |     -     |   9.04   \n",
      "   1    |   80    |   0.482859   |     -      |     -     |   9.03   \n",
      "   1    |   100   |   0.409099   |     -      |     -     |   9.03   \n",
      "   1    |   120   |   0.347565   |     -      |     -     |   9.04   \n",
      "   1    |   140   |   0.264103   |     -      |     -     |   9.04   \n",
      "   1    |   160   |   0.230039   |     -      |     -     |   9.04   \n",
      "   1    |   180   |   0.153100   |     -      |     -     |   9.04   \n",
      "   1    |   200   |   0.154950   |     -      |     -     |   9.03   \n",
      "   1    |   220   |   0.138506   |     -      |     -     |   9.02   \n",
      "   1    |   240   |   0.130855   |     -      |     -     |   9.03   \n",
      "   1    |   260   |   0.122065   |     -      |     -     |   9.01   \n",
      "   1    |   280   |   0.128454   |     -      |     -     |   9.02   \n",
      "   1    |   300   |   0.067512   |     -      |     -     |   9.01   \n",
      "   1    |   320   |   0.102609   |     -      |     -     |   9.01   \n",
      "   1    |   340   |   0.126236   |     -      |     -     |   9.01   \n",
      "   1    |   360   |   0.175617   |     -      |     -     |   9.03   \n",
      "   1    |   380   |   0.144301   |     -      |     -     |   9.02   \n",
      "   1    |   400   |   0.116660   |     -      |     -     |   9.01   \n",
      "   1    |   420   |   0.102255   |     -      |     -     |   9.01   \n",
      "   1    |   440   |   0.105204   |     -      |     -     |   9.00   \n",
      "   1    |   460   |   0.111175   |     -      |     -     |   9.01   \n",
      "   1    |   480   |   0.107015   |     -      |     -     |   9.01   \n",
      "   1    |   500   |   0.049428   |     -      |     -     |   9.02   \n",
      "   1    |   520   |   0.089330   |     -      |     -     |   9.02   \n",
      "   1    |   540   |   0.046301   |     -      |     -     |   9.01   \n",
      "   1    |   560   |   0.077263   |     -      |     -     |   9.01   \n",
      "   1    |   580   |   0.156114   |     -      |     -     |   9.01   \n",
      "   1    |   600   |   0.093031   |     -      |     -     |   9.01   \n",
      "   1    |   620   |   0.127321   |     -      |     -     |   9.01   \n",
      "   1    |   640   |   0.087458   |     -      |     -     |   9.02   \n",
      "   1    |   660   |   0.073481   |     -      |     -     |   9.02   \n",
      "   1    |   680   |   0.198494   |     -      |     -     |   9.03   \n",
      "   1    |   700   |   0.067639   |     -      |     -     |   9.01   \n",
      "   1    |   720   |   0.074347   |     -      |     -     |   8.99   \n",
      "   1    |   740   |   0.102269   |     -      |     -     |   9.00   \n",
      "   1    |   760   |   0.076857   |     -      |     -     |   9.00   \n",
      "   1    |   780   |   0.085658   |     -      |     -     |   9.01   \n",
      "   1    |   800   |   0.087986   |     -      |     -     |   9.02   \n",
      "   1    |   820   |   0.072416   |     -      |     -     |   9.02   \n",
      "   1    |   840   |   0.093055   |     -      |     -     |   9.01   \n",
      "   1    |   860   |   0.111593   |     -      |     -     |   9.01   \n",
      "   1    |   880   |   0.115716   |     -      |     -     |   9.01   \n",
      "   1    |   900   |   0.125801   |     -      |     -     |   9.00   \n",
      "   1    |   920   |   0.130812   |     -      |     -     |   9.01   \n",
      "   1    |   940   |   0.179021   |     -      |     -     |   9.02   \n",
      "   1    |   960   |   0.065480   |     -      |     -     |   9.01   \n",
      "   1    |   980   |   0.122045   |     -      |     -     |   9.01   \n",
      "   1    |  1000   |   0.112109   |     -      |     -     |   9.00   \n",
      "   1    |  1020   |   0.095486   |     -      |     -     |   9.02   \n",
      "   1    |  1040   |   0.042013   |     -      |     -     |   8.99   \n",
      "   1    |  1060   |   0.014019   |     -      |     -     |   8.99   \n",
      "   1    |  1080   |   0.139951   |     -      |     -     |   9.01   \n",
      "   1    |  1100   |   0.108127   |     -      |     -     |   9.01   \n",
      "   1    |  1120   |   0.072771   |     -      |     -     |   9.02   \n",
      "   1    |  1140   |   0.112974   |     -      |     -     |   9.02   \n",
      "   1    |  1160   |   0.057441   |     -      |     -     |   9.01   \n",
      "   1    |  1180   |   0.119054   |     -      |     -     |   9.02   \n",
      "   1    |  1200   |   0.038875   |     -      |     -     |   9.01   \n",
      "   1    |  1220   |   0.111205   |     -      |     -     |   9.00   \n",
      "   1    |  1240   |   0.086154   |     -      |     -     |   9.01   \n",
      "   1    |  1260   |   0.075073   |     -      |     -     |   9.00   \n",
      "   1    |  1280   |   0.051120   |     -      |     -     |   9.00   \n",
      "   1    |  1300   |   0.095739   |     -      |     -     |   9.01   \n",
      "   1    |  1320   |   0.057020   |     -      |     -     |   9.02   \n",
      "   1    |  1340   |   0.079383   |     -      |     -     |   9.01   \n",
      "   1    |  1360   |   0.013945   |     -      |     -     |   9.00   \n",
      "   1    |  1380   |   0.017711   |     -      |     -     |   8.99   \n",
      "   1    |  1400   |   0.083686   |     -      |     -     |   9.00   \n",
      "   1    |  1420   |   0.083379   |     -      |     -     |   9.01   \n",
      "   1    |  1440   |   0.166226   |     -      |     -     |   9.01   \n",
      "   1    |  1460   |   0.015002   |     -      |     -     |   9.01   \n",
      "   1    |  1480   |   0.076352   |     -      |     -     |   9.01   \n",
      "   1    |  1500   |   0.022466   |     -      |     -     |   9.00   \n",
      "   1    |  1520   |   0.180202   |     -      |     -     |   9.01   \n",
      "   1    |  1540   |   0.098881   |     -      |     -     |   9.02   \n",
      "   1    |  1560   |   0.058511   |     -      |     -     |   9.01   \n",
      "   1    |  1580   |   0.061766   |     -      |     -     |   9.02   \n",
      "   1    |  1600   |   0.137149   |     -      |     -     |   9.02   \n",
      "   1    |  1620   |   0.176462   |     -      |     -     |   9.02   \n",
      "   1    |  1640   |   0.044198   |     -      |     -     |   9.00   \n",
      "   1    |  1660   |   0.055721   |     -      |     -     |   8.99   \n",
      "   1    |  1680   |   0.095514   |     -      |     -     |   8.99   \n",
      "   1    |  1700   |   0.003812   |     -      |     -     |   8.98   \n",
      "   1    |  1720   |   0.073167   |     -      |     -     |   9.00   \n",
      "   1    |  1740   |   0.137191   |     -      |     -     |   9.02   \n",
      "   1    |  1760   |   0.075295   |     -      |     -     |   9.02   \n",
      "   1    |  1780   |   0.144238   |     -      |     -     |   9.02   \n",
      "   1    |  1800   |   0.125453   |     -      |     -     |   9.01   \n",
      "   1    |  1820   |   0.161053   |     -      |     -     |   9.02   \n",
      "   1    |  1840   |   0.078118   |     -      |     -     |   9.02   \n",
      "   1    |  1860   |   0.049209   |     -      |     -     |   9.00   \n",
      "   1    |  1880   |   0.035417   |     -      |     -     |   9.00   \n",
      "   1    |  1900   |   0.071491   |     -      |     -     |   8.99   \n",
      "   1    |  1920   |   0.090041   |     -      |     -     |   9.01   \n",
      "   1    |  1940   |   0.084290   |     -      |     -     |   9.00   \n",
      "   1    |  1960   |   0.060357   |     -      |     -     |   9.03   \n",
      "   1    |  1980   |   0.046283   |     -      |     -     |   9.01   \n",
      "   1    |  2000   |   0.108565   |     -      |     -     |   9.02   \n",
      "   1    |  2020   |   0.070649   |     -      |     -     |   9.00   \n",
      "   1    |  2040   |   0.025470   |     -      |     -     |   9.00   \n",
      "   1    |  2060   |   0.112169   |     -      |     -     |   9.01   \n",
      "   1    |  2080   |   0.037821   |     -      |     -     |   8.99   \n",
      "   1    |  2100   |   0.030526   |     -      |     -     |   9.00   \n",
      "   1    |  2120   |   0.032511   |     -      |     -     |   9.01   \n",
      "   1    |  2140   |   0.030508   |     -      |     -     |   9.01   \n",
      "   1    |  2160   |   0.030862   |     -      |     -     |   9.01   \n",
      "   1    |  2180   |   0.224919   |     -      |     -     |   9.01   \n",
      "   1    |  2200   |   0.046305   |     -      |     -     |   9.01   \n",
      "   1    |  2220   |   0.007376   |     -      |     -     |   9.01   \n",
      "   1    |  2240   |   0.047792   |     -      |     -     |   9.01   \n",
      "   1    |  2260   |   0.009184   |     -      |     -     |   9.02   \n",
      "   1    |  2280   |   0.092005   |     -      |     -     |   9.03   \n",
      "   1    |  2300   |   0.077610   |     -      |     -     |   9.02   \n",
      "   1    |  2320   |   0.069078   |     -      |     -     |   8.99   \n",
      "   1    |  2340   |   0.166189   |     -      |     -     |   9.00   \n",
      "   1    |  2360   |   0.112777   |     -      |     -     |   9.00   \n",
      "   1    |  2380   |   0.112655   |     -      |     -     |   9.00   \n",
      "   1    |  2400   |   0.028769   |     -      |     -     |   9.00   \n",
      "   1    |  2420   |   0.097410   |     -      |     -     |   9.01   \n",
      "   1    |  2440   |   0.115106   |     -      |     -     |   9.02   \n",
      "   1    |  2460   |   0.081519   |     -      |     -     |   9.00   \n",
      "   1    |  2480   |   0.134252   |     -      |     -     |   9.00   \n",
      "   1    |  2500   |   0.046839   |     -      |     -     |   8.99   \n",
      "   1    |  2520   |   0.103478   |     -      |     -     |   9.00   \n",
      "   1    |  2540   |   0.091380   |     -      |     -     |   9.00   \n",
      "   1    |  2560   |   0.079211   |     -      |     -     |   9.02   \n",
      "   1    |  2580   |   0.100083   |     -      |     -     |   9.02   \n",
      "   1    |  2600   |   0.052335   |     -      |     -     |   9.00   \n",
      "   1    |  2620   |   0.036567   |     -      |     -     |   9.01   \n",
      "   1    |  2640   |   0.029309   |     -      |     -     |   9.02   \n",
      "   1    |  2660   |   0.103990   |     -      |     -     |   9.02   \n",
      "   1    |  2680   |   0.065116   |     -      |     -     |   9.00   \n",
      "   1    |  2700   |   0.036665   |     -      |     -     |   9.00   \n",
      "   1    |  2720   |   0.073047   |     -      |     -     |   8.99   \n",
      "   1    |  2740   |   0.008577   |     -      |     -     |   8.99   \n",
      "   1    |  2760   |   0.058502   |     -      |     -     |   9.00   \n",
      "   1    |  2780   |   0.105966   |     -      |     -     |   9.02   \n",
      "   1    |  2800   |   0.003727   |     -      |     -     |   9.00   \n",
      "   1    |  2820   |   0.054365   |     -      |     -     |   9.01   \n",
      "   1    |  2840   |   0.125724   |     -      |     -     |   9.01   \n",
      "   1    |  2860   |   0.111925   |     -      |     -     |   9.01   \n",
      "   1    |  2880   |   0.095239   |     -      |     -     |   9.00   \n",
      "   1    |  2900   |   0.008144   |     -      |     -     |   9.00   \n",
      "   1    |  2920   |   0.081661   |     -      |     -     |   9.02   \n",
      "   1    |  2940   |   0.064156   |     -      |     -     |   9.01   \n",
      "   1    |  2960   |   0.045015   |     -      |     -     |   9.01   \n",
      "   1    |  2980   |   0.047312   |     -      |     -     |   9.00   \n",
      "   1    |  3000   |   0.081621   |     -      |     -     |   9.01   \n",
      "   1    |  3020   |   0.004051   |     -      |     -     |   8.99   \n",
      "   1    |  3040   |   0.091514   |     -      |     -     |   8.99   \n",
      "   1    |  3060   |   0.087527   |     -      |     -     |   9.01   \n",
      "   1    |  3080   |   0.044906   |     -      |     -     |   9.01   \n",
      "   1    |  3100   |   0.060078   |     -      |     -     |   9.00   \n",
      "   1    |  3120   |   0.020678   |     -      |     -     |   8.99   \n",
      "   1    |  3140   |   0.051278   |     -      |     -     |   8.99   \n",
      "   1    |  3160   |   0.065853   |     -      |     -     |   9.00   \n",
      "   1    |  3180   |   0.056075   |     -      |     -     |   9.00   \n",
      "   1    |  3200   |   0.029143   |     -      |     -     |   9.00   \n",
      "   1    |  3220   |   0.007827   |     -      |     -     |   9.01   \n",
      "   1    |  3240   |   0.033418   |     -      |     -     |   9.01   \n",
      "   1    |  3260   |   0.087044   |     -      |     -     |   9.00   \n",
      "   1    |  3280   |   0.056453   |     -      |     -     |   9.01   \n",
      "   1    |  3300   |   0.144435   |     -      |     -     |   9.03   \n",
      "   1    |  3320   |   0.053863   |     -      |     -     |   9.02   \n",
      "   1    |  3340   |   0.062245   |     -      |     -     |   8.99   \n",
      "   1    |  3360   |   0.067764   |     -      |     -     |   9.00   \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-f436263565e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Set seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/mycolab/ob/bert_model1.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-4d87651a4f76>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, epochs, evaluation)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# Perform a backward pass to calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m# Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=1,lr=3e-6)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/bert_model1.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-cyprus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 897,
     "referenced_widgets": [
      "c55c86f931dd4af0adb1924b402945a5",
      "ab936e29e1e14047a7e537d124728ad3",
      "66dd33ad7bd8431895ff758bd9f494c6",
      "726db895091e46c28ab2d304a848bc02",
      "50f6e727902d4122ae6caf45ae263269",
      "cd037319b5814950b8284096385d3683",
      "673729623cbd43f7bf0271e56d2e1db7",
      "1dcf378a7c36468fa1b2ff2b3d18d3cc"
     ]
    },
    "id": "BVDCPb58HSDG",
    "outputId": "7442cafc-22de-4681-ed5c-0bc3ef61daab"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c55c86f931dd4af0adb1924b402945a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.544578   |     -      |     -     |   9.50   \n",
      "   1    |   40    |   0.426634   |     -      |     -     |   9.02   \n",
      "   1    |   60    |   0.366638   |     -      |     -     |   9.02   \n",
      "   1    |   80    |   0.327667   |     -      |     -     |   9.02   \n",
      "   1    |   100   |   0.258088   |     -      |     -     |   9.12   \n",
      "   1    |   112   |   0.224266   |     -      |     -     |   5.21   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.369096   |  0.224830  |   82.69   |   52.74  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.189234   |     -      |     -     |   9.48   \n",
      "   2    |   40    |   0.147776   |     -      |     -     |   9.01   \n",
      "   2    |   60    |   0.116778   |     -      |     -     |   9.01   \n",
      "   2    |   80    |   0.097592   |     -      |     -     |   9.00   \n",
      "   2    |   100   |   0.090342   |     -      |     -     |   8.99   \n",
      "   2    |   112   |   0.082725   |     -      |     -     |   5.20   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.124038   |  0.061102  |  100.00   |   52.54  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   3    |   20    |   0.077929   |     -      |     -     |   9.45   \n",
      "   3    |   40    |   0.063970   |     -      |     -     |   9.00   \n",
      "   3    |   60    |   0.050802   |     -      |     -     |   8.99   \n",
      "   3    |   80    |   0.045585   |     -      |     -     |   8.98   \n",
      "   3    |   100   |   0.048685   |     -      |     -     |   8.98   \n",
      "   3    |   112   |   0.073731   |     -      |     -     |   5.19   \n",
      "----------------------------------------------------------------------\n",
      "   3    |    -    |   0.059311   |  0.041683  |  100.00   |   52.43  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2,3'\n",
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=3 ,lr=3e-6)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)\n",
    "\n",
    "with open('/content/drive/My Drive/mycolab/ob/bert_model2.pickle', 'wb') as fp:\n",
    "    pickle.dump(bert_classifier, fp)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Bert_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "002938520c414ee285395437d57f8a2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00b4ac654b194d9787bbff041e293a4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "00c3911fd2bb49f192e2f28d14f515b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "01182e29779940599e17c5caee682d39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d4acabbfa5b43ddaa2cc44cb24faae1",
      "placeholder": "​",
      "style": "IPY_MODEL_45b0cd3ac6e34bf0b9848a37f1992058",
      "value": " 440M/440M [00:07&lt;00:00, 61.3MB/s]"
     }
    },
    "0136b39bb5b64199be6df9af593d42a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01643d76f2b54f9b92b34152644e8d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ae80cace6bb4d4b98d00090ca8fccd8",
       "IPY_MODEL_37a5b813ad914bc8b5ba4bc585aac713"
      ],
      "layout": "IPY_MODEL_72d20c857c7a499ea8642da5b379cb1a"
     }
    },
    "04f4de2271fd47a1bfbef882924cb09d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a7b7cdc5ce7847fdbc81cbe57fb75efc",
       "IPY_MODEL_c38aa949f9ea40c1ba532b06f13ca401",
       "IPY_MODEL_3bfac18b3a744b0d95e8bd24e303d609"
      ],
      "layout": "IPY_MODEL_86f40d734baf4d2097d493c31afc4c6e"
     }
    },
    "060122daf3d5479d931e05f458a5092f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0dc8d41829af449a84c863790a9138e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0fb18f03eaf04078981fb88ffda9da37": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12e101c88ad144ed8641f600fa1cc44e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b817e62ba744c4ea1dbeaa264a960e6",
      "placeholder": "​",
      "style": "IPY_MODEL_5aee5418609e4123a19b445e52fff584",
      "value": " 570/570 [00:08&lt;00:00, 67.3B/s]"
     }
    },
    "136062b9e6f04ec9abedf0495c72a3a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f7bd845f37cb459ab037e84ce02f70c6",
       "IPY_MODEL_12e101c88ad144ed8641f600fa1cc44e"
      ],
      "layout": "IPY_MODEL_3cc433fa676c4deb8bfb15186f22cbd9"
     }
    },
    "1610044f589b4a9ebbd414bf93947ad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1c5030f346e34c789ab3e8dd6d992cab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1dcf378a7c36468fa1b2ff2b3d18d3cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ee466477722426b872be7b952d88a8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22281583fc674b018f767d46b4582130": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3275ef25a7a14c47817aff65a570a602": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "329f0886bba04fc98558a3d0e2dc8e71": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34cc322beda14d00b62f764b268c7b4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "361d85ba71bd443385856476c55d9ea7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37a5b813ad914bc8b5ba4bc585aac713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0136b39bb5b64199be6df9af593d42a9",
      "placeholder": "​",
      "style": "IPY_MODEL_f6223d67da1b4840b9733b648ebc55d9",
      "value": " 440M/440M [00:07&lt;00:00, 56.6MB/s]"
     }
    },
    "38b6a4e2ce384612a0eaa4e9f6f81a98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3977a980d05b4eac9a232d3a1bfa315b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b80fec785c34bbc833229e7efe6598b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3bfac18b3a744b0d95e8bd24e303d609": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_801d2d0f63e1494780413fcff2bbaa10",
      "placeholder": "​",
      "style": "IPY_MODEL_48203a3e1aa94e9c85b1cc5fbdebcbb6",
      "value": " 232k/232k [00:00&lt;00:00, 684kB/s]"
     }
    },
    "3cc433fa676c4deb8bfb15186f22cbd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e96078563f64baab04244454f476913": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3eba9ffef5944435ac316c8e6418e3df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c414b0c29fd4eebb00af32b6cb4a926",
       "IPY_MODEL_4be9ab392a8b45fc8517b1f0464ad65f"
      ],
      "layout": "IPY_MODEL_b2e42814904948728288aa00a86798fd"
     }
    },
    "45b0cd3ac6e34bf0b9848a37f1992058": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48203a3e1aa94e9c85b1cc5fbdebcbb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a41d8de7cd5420d86196bb70268191c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_eeb1beb6813a4d328b83f2b1a378799e",
       "IPY_MODEL_e7bffe0221e74f6194900b3bc170f467"
      ],
      "layout": "IPY_MODEL_95149713eaa14c108adf09e2215b45a5"
     }
    },
    "4a4464861b8b43d5a09047fb96a91db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b47dce8dbdc4c7ea56eef5e1e2f4d06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4be9ab392a8b45fc8517b1f0464ad65f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb561b4163bc41b08778fee1cdaef0e0",
      "placeholder": "​",
      "style": "IPY_MODEL_00b4ac654b194d9787bbff041e293a4f",
      "value": " 570/570 [00:00&lt;00:00, 1.01kB/s]"
     }
    },
    "4df09bd241f5466dabe0bb8c58ccf682": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e261293d240433a87440a4c0e1ec734": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e8fb7e4364a488d8b37300a8bb1c553": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89b742442f51489d8b72f267b82fd6cb",
      "placeholder": "​",
      "style": "IPY_MODEL_00c3911fd2bb49f192e2f28d14f515b2",
      "value": " 440M/440M [01:05&lt;00:00, 6.78MB/s]"
     }
    },
    "4f1c25a0a98543439dfc97674668e0dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e3ede741e3e34f819817bdcf2ef04699",
       "IPY_MODEL_01182e29779940599e17c5caee682d39"
      ],
      "layout": "IPY_MODEL_9616d8d1706943d19c82fd9cdd804cbd"
     }
    },
    "50f6e727902d4122ae6caf45ae263269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "516aa1b02a2245a69db142612d93ba7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5193976517824e749c400949e1b480bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "53faf34f78654183a26b0ca589d6d069": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5952245abb5044e0b94a1813f7af970a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cfb6507faa9f450f8c4f211ce43888bc",
       "IPY_MODEL_ae25edd05dd6492eb018bb55f02e2170"
      ],
      "layout": "IPY_MODEL_ace51fa2c9da4dd6a11dea90f0c61e62"
     }
    },
    "598b03ee0c5e414fb49f4a2a4e599b24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5aee5418609e4123a19b445e52fff584": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b817e62ba744c4ea1dbeaa264a960e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d4acabbfa5b43ddaa2cc44cb24faae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f1388ce45fd4eff923e31322a7fba1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bc1532258ee841e49d727dcc54d9238a",
       "IPY_MODEL_f78421c421bd44259fa12010ac4ff953",
       "IPY_MODEL_7104fbc5b5774014aa1dd84feab22638"
      ],
      "layout": "IPY_MODEL_53faf34f78654183a26b0ca589d6d069"
     }
    },
    "61062e55b02f41f88edf3407c18d270e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6308194b878246e2b65c2fbc0aac664f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bd765d26eda4afbb2eb9f73f007e2cf",
       "IPY_MODEL_dbe30fc67e794c9d818ca110e30ce1ac",
       "IPY_MODEL_f3902364d737418286bd1479a2df3db5"
      ],
      "layout": "IPY_MODEL_7584bde860ec488fa83307c9f765f83e"
     }
    },
    "63b67e6a9da748de8cc569c64a499671": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "646e396f91c344d0ae1a33d1d0bf049e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65d2b6a945254353abadf02edcb097b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66dd33ad7bd8431895ff758bd9f494c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd037319b5814950b8284096385d3683",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_50f6e727902d4122ae6caf45ae263269",
      "value": 440473133
     }
    },
    "673729623cbd43f7bf0271e56d2e1db7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ae80cace6bb4d4b98d00090ca8fccd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99eef8c0e40341ac95c3f9558c726b0a",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3275ef25a7a14c47817aff65a570a602",
      "value": 440473133
     }
    },
    "6bc9a7b3f20b4be58e7a3030c01dd1d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c063963d9644c6e8ccf7b978bda62c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6e2e45ec85364197af3654bd77b1be09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e51866900044b86b428cb93a78a23b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70b619307cb349b5986e307357451ddb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70d4273195ac43928313cc0dd73f067b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7104fbc5b5774014aa1dd84feab22638": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c56297ad0e364960a297360d6d338cec",
      "placeholder": "​",
      "style": "IPY_MODEL_d277198e13444a4bae71c326830a19af",
      "value": " 466k/466k [00:00&lt;00:00, 918kB/s]"
     }
    },
    "726db895091e46c28ab2d304a848bc02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1dcf378a7c36468fa1b2ff2b3d18d3cc",
      "placeholder": "​",
      "style": "IPY_MODEL_673729623cbd43f7bf0271e56d2e1db7",
      "value": " 440M/440M [00:56&lt;00:00, 7.85MB/s]"
     }
    },
    "72d20c857c7a499ea8642da5b379cb1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74d9e5c572e94919b3eba34bfc6d75dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7584bde860ec488fa83307c9f765f83e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b7ee9b2f8ad4de1968323e1899cd2a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c414b0c29fd4eebb00af32b6cb4a926": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4df09bd241f5466dabe0bb8c58ccf682",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2c1489e27ee4c75b01abd9999f1ffd6",
      "value": 570
     }
    },
    "7fd76663043c412f8fd92b9bebb60539": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "801d2d0f63e1494780413fcff2bbaa10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8085a8c47c5942698edf39a9786550bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_002938520c414ee285395437d57f8a2b",
      "placeholder": "​",
      "style": "IPY_MODEL_646e396f91c344d0ae1a33d1d0bf049e",
      "value": " 570/570 [00:04&lt;00:00, 117B/s]"
     }
    },
    "86f40d734baf4d2097d493c31afc4c6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89b742442f51489d8b72f267b82fd6cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bd765d26eda4afbb2eb9f73f007e2cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e261293d240433a87440a4c0e1ec734",
      "placeholder": "​",
      "style": "IPY_MODEL_598b03ee0c5e414fb49f4a2a4e599b24",
      "value": "Downloading: 100%"
     }
    },
    "8f7c3875657d4f0ea7c7d8abe9389b68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fd76663043c412f8fd92b9bebb60539",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f83ca4fade6040359a6da1d000dad5f7",
      "value": 570
     }
    },
    "91e38294851140e78b7328b4dac71e82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a04e456f444a40afb9d911b71ee67f30",
      "placeholder": "​",
      "style": "IPY_MODEL_fe387e812f8542bba4bd7eba212b69eb",
      "value": " 570/570 [00:33&lt;00:00, 17.2B/s]"
     }
    },
    "9462a3acbf234a85b651bcce3f669c62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63b67e6a9da748de8cc569c64a499671",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b51ae0d687bb48939aef24b812f4b62a",
      "value": 28
     }
    },
    "95149713eaa14c108adf09e2215b45a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9616d8d1706943d19c82fd9cdd804cbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98bc13262e684633b49df2b58d3887f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99eef8c0e40341ac95c3f9558c726b0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ea46bd09f3547b296981b6376ee33f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a04e456f444a40afb9d911b71ee67f30": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2afd530a5ea42e593e173f9301b0fdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a4ddd7b790664701845bf3816a5934a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b0302be9e7ae43b7a1ee1afc826afdb3",
       "IPY_MODEL_4e8fb7e4364a488d8b37300a8bb1c553"
      ],
      "layout": "IPY_MODEL_38b6a4e2ce384612a0eaa4e9f6f81a98"
     }
    },
    "a6b95151011a44b89017d928efe8b66c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7b7cdc5ce7847fdbc81cbe57fb75efc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dc8d41829af449a84c863790a9138e8",
      "placeholder": "​",
      "style": "IPY_MODEL_22281583fc674b018f767d46b4582130",
      "value": "Downloading: 100%"
     }
    },
    "ab936e29e1e14047a7e537d124728ad3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ace51fa2c9da4dd6a11dea90f0c61e62": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae25edd05dd6492eb018bb55f02e2170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74d9e5c572e94919b3eba34bfc6d75dd",
      "placeholder": "​",
      "style": "IPY_MODEL_aff6365cd709403d9feab4241b141df8",
      "value": " 570/570 [00:00&lt;00:00, 1.19kB/s]"
     }
    },
    "aff6365cd709403d9feab4241b141df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0302be9e7ae43b7a1ee1afc826afdb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b7ee9b2f8ad4de1968323e1899cd2a2",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6c063963d9644c6e8ccf7b978bda62c5",
      "value": 440473133
     }
    },
    "b2e42814904948728288aa00a86798fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b354656726b2476b9d569c07929fa4ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "b51ae0d687bb48939aef24b812f4b62a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "baedf19d86804066becd596c730047a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb65a6dbf9f0432f9264271e36488d22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70d4273195ac43928313cc0dd73f067b",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9ea46bd09f3547b296981b6376ee33f7",
      "value": 570
     }
    },
    "bc1532258ee841e49d727dcc54d9238a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ee466477722426b872be7b952d88a8a",
      "placeholder": "​",
      "style": "IPY_MODEL_516aa1b02a2245a69db142612d93ba7b",
      "value": "Downloading: 100%"
     }
    },
    "c12f62d86241469ea82d143da666d733": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c93175a16210440a931b856ff68dc51a",
       "IPY_MODEL_9462a3acbf234a85b651bcce3f669c62",
       "IPY_MODEL_d6fbae1e353f4b24a4c1bcf63122181d"
      ],
      "layout": "IPY_MODEL_4b47dce8dbdc4c7ea56eef5e1e2f4d06"
     }
    },
    "c38aa949f9ea40c1ba532b06f13ca401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1605c98d37b49999616920f61838101",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e51866900044b86b428cb93a78a23b8",
      "value": 231508
     }
    },
    "c467340bb5324356a4cd0a6d45ece484": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb65a6dbf9f0432f9264271e36488d22",
       "IPY_MODEL_8085a8c47c5942698edf39a9786550bb"
      ],
      "layout": "IPY_MODEL_db7968309df34569aade64de63d38da0"
     }
    },
    "c55c86f931dd4af0adb1924b402945a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_66dd33ad7bd8431895ff758bd9f494c6",
       "IPY_MODEL_726db895091e46c28ab2d304a848bc02"
      ],
      "layout": "IPY_MODEL_ab936e29e1e14047a7e537d124728ad3"
     }
    },
    "c56297ad0e364960a297360d6d338cec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c93175a16210440a931b856ff68dc51a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fb18f03eaf04078981fb88ffda9da37",
      "placeholder": "​",
      "style": "IPY_MODEL_34cc322beda14d00b62f764b268c7b4c",
      "value": "Downloading: 100%"
     }
    },
    "cc89e783b5554db6a51860c90f1ea38d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd037319b5814950b8284096385d3683": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cedb2ca08fa342ec94e7acb6ecf58ed2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f7c3875657d4f0ea7c7d8abe9389b68",
       "IPY_MODEL_91e38294851140e78b7328b4dac71e82"
      ],
      "layout": "IPY_MODEL_6bc9a7b3f20b4be58e7a3030c01dd1d8"
     }
    },
    "cfb6507faa9f450f8c4f211ce43888bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d17bd1c1dcdb4cd484d3e1bab9e8db1a",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5193976517824e749c400949e1b480bc",
      "value": 570
     }
    },
    "d1605c98d37b49999616920f61838101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d17bd1c1dcdb4cd484d3e1bab9e8db1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d277198e13444a4bae71c326830a19af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6fbae1e353f4b24a4c1bcf63122181d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_329f0886bba04fc98558a3d0e2dc8e71",
      "placeholder": "​",
      "style": "IPY_MODEL_d8b9149cec4e4931bb290b2b897920c8",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.06kB/s]"
     }
    },
    "d8b9149cec4e4931bb290b2b897920c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "db7968309df34569aade64de63d38da0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe30fc67e794c9d818ca110e30ce1ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e2e45ec85364197af3654bd77b1be09",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3b80fec785c34bbc833229e7efe6598b",
      "value": 570
     }
    },
    "e2c1489e27ee4c75b01abd9999f1ffd6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "e3ede741e3e34f819817bdcf2ef04699": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c5030f346e34c789ab3e8dd6d992cab",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e96078563f64baab04244454f476913",
      "value": 440473133
     }
    },
    "e7bffe0221e74f6194900b3bc170f467": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baedf19d86804066becd596c730047a8",
      "placeholder": "​",
      "style": "IPY_MODEL_060122daf3d5479d931e05f458a5092f",
      "value": " 440M/440M [00:09&lt;00:00, 45.0MB/s]"
     }
    },
    "eb561b4163bc41b08778fee1cdaef0e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eeb1beb6813a4d328b83f2b1a378799e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65d2b6a945254353abadf02edcb097b4",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b354656726b2476b9d569c07929fa4ad",
      "value": 440473133
     }
    },
    "f039da7554684ed9afe3c70370adc785": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3977a980d05b4eac9a232d3a1bfa315b",
      "placeholder": "​",
      "style": "IPY_MODEL_a2afd530a5ea42e593e173f9301b0fdb",
      "value": " 440M/440M [00:09&lt;00:00, 48.6MB/s]"
     }
    },
    "f3902364d737418286bd1479a2df3db5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6b95151011a44b89017d928efe8b66c",
      "placeholder": "​",
      "style": "IPY_MODEL_98bc13262e684633b49df2b58d3887f5",
      "value": " 570/570 [00:00&lt;00:00, 23.6kB/s]"
     }
    },
    "f6223d67da1b4840b9733b648ebc55d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f78421c421bd44259fa12010ac4ff953": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc89e783b5554db6a51860c90f1ea38d",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70b619307cb349b5986e307357451ddb",
      "value": 466062
     }
    },
    "f7bd845f37cb459ab037e84ce02f70c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a4464861b8b43d5a09047fb96a91db1",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1610044f589b4a9ebbd414bf93947ad3",
      "value": 570
     }
    },
    "f83ca4fade6040359a6da1d000dad5f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f99e44b4d9004190878b98da41f6a308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fddf144da84d41fb8c332ec0b2357bd8",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_61062e55b02f41f88edf3407c18d270e",
      "value": 440473133
     }
    },
    "fc3b4a35f0b54e6997085bbd3c73bd50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f99e44b4d9004190878b98da41f6a308",
       "IPY_MODEL_f039da7554684ed9afe3c70370adc785"
      ],
      "layout": "IPY_MODEL_361d85ba71bd443385856476c55d9ea7"
     }
    },
    "fddf144da84d41fb8c332ec0b2357bd8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe387e812f8542bba4bd7eba212b69eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
